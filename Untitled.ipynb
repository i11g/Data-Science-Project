{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcd0a39-557a-42c8-9705-6c867dbff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59efd1b7-5eb0-46d5-9730-fc3b199591e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_1=pd.read_csv(\"QS World University Rankings 2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e18aec3c-8f31-466b-8b39-c8d03e908794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year              int64\n",
       "rank_display     object\n",
       "university       object\n",
       "score           float64\n",
       "link             object\n",
       "country          object\n",
       "city             object\n",
       "region           object\n",
       "logo             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d885081c-3816-4a9e-980a-c13f323c7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year              0\n",
       "rank_display     20\n",
       "university        0\n",
       "score           515\n",
       "link              0\n",
       "country           0\n",
       "city             25\n",
       "region            0\n",
       "logo              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987f239-0568-4a83-adaa-6d2a2ee805ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rank = df_rank_uni[df_rank_uni.duplicated()]\n",
    "duplicate_gdp = df_gdp[df_gdp.duplicated()]\n",
    "\n",
    "print(\"\\nDuplicate rows in Ranking Dataset:\")\n",
    "print(duplicate_rank)\n",
    "\n",
    "print(\"\\n Duplicate rows in GDP Dataset:\")\n",
    "print(duplicate_gdp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5bbd1-bb16-4576-a5f9-e21841cbbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Rows in Ranking Dataset:\", df_rank.duplicated().sum())\n",
    "print(\"Duplicate Rows in GDP Dataset:\", df_gdp.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa381a97-a533-4286-97b5-1e331a8d844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_clean = df_rank_1.reset_index(drop=True).copy()\n",
    "missing_idx = np.where(df_rank_clean['score'].isna())[0]\n",
    "n = len(df_rank_clean)\n",
    "\n",
    "for i in missing_idx:\n",
    "    left  = df_rank_clean['score'].iloc[i-1] if i-1 >= 0 else np.nan\n",
    "    right = df_rank_clean['score'].iloc[i+1] if i+1 < n   else np.nan\n",
    "    df_rank_clean.at[i, 'score'] = np.nanmean([left, right])\n",
    "\n",
    "# In case both neighbors were NaN:\n",
    "df_rank_clean['score'] = df_rank_clean['score'].interpolate(limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ec2facd-2d84-4435-b02a-f2f4fa61a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_clean_2.to_csv(\"df_rank_clean_new_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c0254-b42e-47ff-9b0f-da825d2c6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_top10_all_years(df_clean):\n",
    "    # Calculate average score per country-year\n",
    "    avg_score = (\n",
    "        df_clean.groupby([\"country\", \"year\"], as_index=False)[\"score\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"score\": \"avg_score\"})\n",
    "    )\n",
    "\n",
    "    # Get top 10 per year\n",
    "    top10_per_year = (\n",
    "        avg_score.sort_values([\"year\", \"avg_score\"], ascending=[True, False])\n",
    "        .groupby(\"year\")\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    # Get unique years sorted\n",
    "    years = sorted(top10_per_year[\"year\"].unique())\n",
    "    n_years = len(years)\n",
    "\n",
    "    # Set up subplot grid\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_years, ncols=1, figsize=(10, 4 * n_years), constrained_layout=True\n",
    "    )\n",
    "\n",
    "    # In case there is only one year (avoid iterable error)\n",
    "    if n_years == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Plot each year's top 10\n",
    "    for ax, year in zip(axes, years):\n",
    "        data = top10_per_year[top10_per_year[\"year\"] == year]\n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=\"avg_score\",\n",
    "            y=\"country\",\n",
    "            palette=\"viridis\",\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"Top 10 Countries by Average Score ({year})\", fontsize=14)\n",
    "        ax.set_xlabel(\"Average Score\")\n",
    "        ax.set_ylabel(\"Country\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8453375-cd63-4879-a292-038fe5eb609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top10_all_years(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3689c26-a035-40b3-9dbd-f0a0ba000f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_university_dataset(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates by [year, country] and computes:\n",
    "      - total_score (sum of 'score')\n",
    "      - num_universities (unique 'university' count)\n",
    "    Then drops raw columns (if present) from the returned aggregated frame.\n",
    "    \"\"\"\n",
    "    # Aggregate\n",
    "    agg_df = (\n",
    "        df_clean.groupby([\"year\", \"country\"], as_index=False)\n",
    "                .agg(total_score=(\"score\", \"sum\"),\n",
    "                     num_universities=(\"university\", \"nunique\"))\n",
    "    )\n",
    "\n",
    "    # Drop the originals if you no longer need them (no-op here, but kept for clarity)\n",
    "    agg_df = agg_df.drop(columns=[\"university\", \"score\"], errors=\"ignore\")\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c43608-eb62-4147-88e5-9c46bb4a7e36",
   "metadata": {},
   "source": [
    "Forecasting GDP and ARIMAX Modeling\n",
    "Forecast GDP for 2023 using ARIMA\n",
    "\n",
    "GDP as exogenous variable. Target: 2023 university scores and/or number of universities\n",
    "\n",
    "Evaluate forecast vs actual 2023 data. Compare with ARIMA without GDP\n",
    "\n",
    "Document how you chose lag values and whether GDP improves forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0684e-af36-4200-b864-3b6f4b4a1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long = gdp_long[[\"country\", \"year\", \"gdp_per_capita\"]]\n",
    "edu_long = edu_long[[\"country\", \"year\", \"gov_exp_edu\"]]\n",
    "gov_eff_long = gov_eff_long [[\"country\", \"year\", \"gov_effectiveness\"]]\n",
    "rd_exp_long = rd_exp_long [[\"country\", \"year\", \"rd_exp_gdp\"]]\n",
    "merged_df_gov = merged_df_gov[[\"country\", \"year\", \"gov_effectiveness\", \"total_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061cebd-827e-4bc5-a131-8b5b87d0e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# --- 0) Minimal sanity cleaning for each source ---\n",
    "def prep(df, cols):\n",
    "    out = df[cols].copy()\n",
    "    # standardize column names and types\n",
    "    out[\"country\"] = out[\"country\"].astype(str).str.strip()\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    # drop exact duplicates\n",
    "    out = out.drop_duplicates(subset=[\"country\", \"year\"])\n",
    "    return out\n",
    "\n",
    "gdp_long      = prep(gdp_long,      [\"country\", \"year\", \"gdp_per_capita\"])\n",
    "edu_long      = prep(edu_long,      [\"country\", \"year\", \"gov_exp_edu\"])\n",
    "gov_eff_long  = prep(gov_eff_long,  [\"country\", \"year\", \"gov_effectiveness\"])\n",
    "rd_exp_long   = prep(rd_exp_long,   [\"country\", \"year\", \"rd_exp_gdp\"])\n",
    "merged_df_gov = prep(merged_df_gov, [\"country\", \"year\", \"gov_effectiveness\", \"total_score\"])\n",
    "\n",
    "# If merged_df_gov also has gov_effectiveness, keep the score from there\n",
    "# and prefer the indicator column from the dedicated gov_eff_long source.\n",
    "merged_df_gov = merged_df_gov.rename(columns={\"gov_effectiveness\": \"gov_effectiveness_from_score\"})\n",
    "\n",
    "# --- 1) Merge all indicators onto the score base (left join preserves your scoring rows) ---\n",
    "dfs_to_merge = [\n",
    "    merged_df_gov,\n",
    "    gdp_long,\n",
    "    edu_long,\n",
    "    gov_eff_long,   # authoritative gov_effectiveness\n",
    "    rd_exp_long\n",
    "]\n",
    "\n",
    "def left_merge(a, b):\n",
    "    # prevent accidental suffix collisions\n",
    "    cols_before = set(a.columns)\n",
    "    out = a.merge(b, on=[\"country\", \"year\"], how=\"left\", validate=\"one_to_one\")\n",
    "    # sanity check for duplicate indicators\n",
    "    dupes = [c for c in out.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "    if dupes:\n",
    "        raise ValueError(f\"Unexpected duplicate columns after merge: {dupes}\")\n",
    "    return out\n",
    "\n",
    "full_df = reduce(left_merge, dfs_to_merge)\n",
    "\n",
    "# Replace the possibly-missing 'gov_effectiveness' with the one from the score table if needed\n",
    "if \"gov_effectiveness\" in full_df.columns:\n",
    "    full_df[\"gov_effectiveness\"] = full_df[\"gov_effectiveness\"].fillna(full_df[\"gov_effectiveness_from_score\"])\n",
    "else:\n",
    "    full_df[\"gov_effectiveness\"] = full_df[\"gov_effectiveness_from_score\"]\n",
    "\n",
    "# Drop helper column\n",
    "full_df = full_df.drop(columns=[\"gov_effectiveness_from_score\"], errors=\"ignore\")\n",
    "\n",
    "# --- 2) Restrict to the modeling window 2017–2023 and ensure clean dtypes ---\n",
    "full_df = full_df[(full_df[\"year\"] >= 2017) & (full_df[\"year\"] <= 2023)].copy()\n",
    "full_df[\"year\"] = full_df[\"year\"].astype(int)\n",
    "\n",
    "# --- 3) If there are still multiple rows per (country,year), aggregate safely ---\n",
    "if full_df.duplicated(subset=[\"country\", \"year\"]).any():\n",
    "    agg_map = {\n",
    "        \"total_score\": \"sum\",             # country-year total from top 300\n",
    "        \"gdp_per_capita\": \"mean\",\n",
    "        \"gov_exp_edu\": \"mean\",\n",
    "        \"gov_effectiveness\": \"mean\",\n",
    "        \"rd_exp_gdp\": \"mean\"\n",
    "    }\n",
    "    full_df = (\n",
    "        full_df.groupby([\"country\", \"year\"], as_index=False)\n",
    "               .agg(agg_map)\n",
    "    )\n",
    "\n",
    "# --- 4) Quick completeness report ---\n",
    "needed_cols = [\"total_score\", \"gdp_per_capita\", \"gov_exp_edu\", \"gov_effectiveness\", \"rd_exp_gdp\"]\n",
    "missing_summary = (\n",
    "    full_df.assign(missing_any = full_df[needed_cols].isna().any(axis=1))\n",
    "           .groupby(\"year\")[\"missing_any\"].value_counts(dropna=False)\n",
    "           .unstack(fill_value=0)\n",
    "           .rename(columns={True:\"rows_with_NA\", False:\"rows_complete\"})\n",
    ")\n",
    "print(\"Completeness by year (rows):\\n\", missing_summary)\n",
    "\n",
    "# --- 5) Split train/test frames we’ll use for the three simple prediction approaches ---\n",
    "train_2017_2022 = full_df[(full_df[\"year\"] >= 2017) & (full_df[\"year\"] <= 2022)].dropna(subset=needed_cols).copy()\n",
    "test_2023        = full_df[(full_df[\"year\"] == 2023)].dropna(subset=needed_cols).copy()\n",
    "\n",
    "print(\"\\nTrain rows:\", len(train_2017_2022), \" | Test 2023 rows:\", len(test_2023))\n",
    "print(\"\\nColumns:\", list(full_df.columns))\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019622d1-f2cf-410a-ac9b-3ffae853dc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad3e32-0d72-4dba-af76-aef13930a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# 0) Merge target + GDP\n",
    "panel = (\n",
    "    merged_df_gov[[\"country\",\"year\",\"total_score\"]]\n",
    "    .merge(gdp_long[[\"country\",\"year\",\"gdp_per_capita\"]], on=[\"country\",\"year\"], how=\"left\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Hygiene\n",
    "panel[\"country\"] = panel[\"country\"].astype(str).str.strip()\n",
    "panel[\"year\"] = panel[\"year\"].astype(int)\n",
    "panel[\"gdp_per_capita\"] = pd.to_numeric(panel[\"gdp_per_capita\"], errors=\"coerce\")\n",
    "\n",
    "# Use log GDP (often more linear)\n",
    "panel[\"log_gdp\"] = np.log(panel[\"gdp_per_capita\"])\n",
    "\n",
    "# 1) Train on 2017–2022\n",
    "train = panel.query(\"2017 <= year <= 2022\").dropna(subset=[\"total_score\",\"log_gdp\"]).copy()\n",
    "\n",
    "# 2) Top-10 countries by avg total_score (2017–2022)\n",
    "top10 = (train.groupby(\"country\")[\"total_score\"].mean()\n",
    "               .nlargest(10).index.tolist())\n",
    "\n",
    "# Restrict training to those countries only (optional, keeps focus consistent)\n",
    "train_top10 = train[train[\"country\"].isin(top10)].copy()\n",
    "\n",
    "# 3) Fit fixed-effects OLS with a time trend\n",
    "# total_score ~ country dummies + year + log_gdp\n",
    "fe = smf.ols(\"total_score ~ C(country) + year + log_gdp\", data=train_top10)\\\n",
    "        .fit(cov_type=\"cluster\", cov_kwds={\"groups\": train_top10[\"country\"]})\n",
    "print(fe.summary())\n",
    "\n",
    "# 4) Build 2023 prediction frame for the same top-10 countries (pull 2023 GDP)\n",
    "future_2023 = (\n",
    "    gdp_long.query(\"year == 2023 and country in @top10\")\n",
    "            [[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "            .drop_duplicates()\n",
    "            .copy()\n",
    ")\n",
    "future_2023[\"log_gdp\"] = np.log(pd.to_numeric(future_2023[\"gdp_per_capita\"], errors=\"coerce\"))\n",
    "\n",
    "# In case any GDP is missing after coercion:\n",
    "future_2023 = future_2023.dropna(subset=[\"log_gdp\"])\n",
    "\n",
    "# 5) Predict 2023\n",
    "future_2023[\"forecast_2023\"] = fe.predict(future_2023)\n",
    "\n",
    "pred_lr = future_2023[[\"country\",\"forecast_2023\"]].sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c1870-06ea-4cd3-bfad-713337d380b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b262ede-df6b-4fbe-a314-fdb9854da1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# 0) Merge target + GDP\n",
    "panel = (\n",
    "    merged_df_gov[[\"country\",\"year\",\"total_score\"]]\n",
    "    .merge(gdp_long[[\"country\",\"year\",\"gdp_per_capita\"]], on=[\"country\",\"year\"], how=\"left\")\n",
    "    .dropna(subset=[\"total_score\",\"gdp_per_capita\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# 1) Transform GDP (often helps linearity)\n",
    "panel[\"log_gdp\"] = np.log(panel[\"gdp_per_capita\"])\n",
    "\n",
    "# 2) Train on 2017–2022\n",
    "train = panel.query(\"2017 <= year <= 2022\").copy()\n",
    "\n",
    "# 3) Fit Fixed-Effects OLS with time trend\n",
    "fe_model = smf.ols(\"total_score ~ C(country) + year + log_gdp\", data=train).fit(cov_type=\"cluster\", cov_kwds={\"groups\": train[\"country\"]})\n",
    "print(fe_model.summary())\n",
    "\n",
    "# 4) Prepare 2023 prediction rows for top-10 countries by 2017–2022 avg score\n",
    "top10 = (train.groupby(\"country\")[\"total_score\"].mean().nlargest(10).index.tolist())\n",
    "\n",
    "future_2023 = (\n",
    "    panel.query(\"year == 2023 and country in @top10\")[[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# If some GDP 2023 are missing, pull directly from gdp_long\n",
    "if future_2023.empty or future_2023[\"gdp_per_capita\"].isna().any():\n",
    "    fallback = gdp_long.query(\"year == 2023 and country in @top10\")[[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "    future_2023 = fallback.copy()\n",
    "\n",
    "future_2023[\"log_gdp\"] = np.log(future_2023[\"gdp_per_capita\"])\n",
    "\n",
    "# 5) Predict 2023\n",
    "future_2023[\"forecast_2023\"] = fe_model.predict(future_2023)\n",
    "\n",
    "pred_fe = future_2023[[\"country\",\"forecast_2023\"]].sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87894a-2f93-4557-86f1-68628869bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aca868-e07c-4cd6-96e5-9529407635ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def zscore_train(x):\n",
    "    mu = float(np.nanmean(x))\n",
    "    sd = float(np.nanstd(x, ddof=0))\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        sd = 1.0\n",
    "    return (x - mu) / sd, mu, sd\n",
    "\n",
    "def make_year_end_index(years):\n",
    "    # Convert int years to year-end timestamps with explicit freq\n",
    "    return pd.PeriodIndex(years.astype(str), freq=\"Y-DEC\").to_timestamp()\n",
    "\n",
    "pred_rows = []\n",
    "\n",
    "# Use your df_gdp_exog with the OUTER merge from earlier\n",
    "top10 = (df_gdp_exog.query(\"year <= 2022\")\n",
    "         .dropna(subset=[\"total_score\"])\n",
    "         .groupby(\"country\")[\"total_score\"].mean()\n",
    "         .nlargest(10).index.tolist())\n",
    "\n",
    "for c in top10:\n",
    "    cd = df_gdp_exog[df_gdp_exog[\"country\"] == c].copy()\n",
    "    train = cd[(cd[\"year\"] >= 2017) & (cd[\"year\"] <= 2022)].dropna(subset=[\"total_score\",\"gdp_per_capita\"])\n",
    "    fut  = cd[(cd[\"year\"] == 2023)][[\"gdp_per_capita\"]]\n",
    "\n",
    "    if len(train) < 4 or fut.empty or fut.isna().any().any():\n",
    "        pred_rows.append({\"country\": c, \"forecast_2023\": None, \"lo80\": None, \"hi80\": None, \"order\": \"\", \"note\": \"too little data or missing 2023 GDP\"})\n",
    "        continue\n",
    "\n",
    "    # Index with explicit yearly frequency\n",
    "    idx = make_year_end_index(train[\"year\"])\n",
    "    y  = pd.Series(train[\"total_score\"].to_numpy(float), index=idx, name=\"total_score\")\n",
    "    X  = pd.DataFrame({\"gdp_per_capita\": train[\"gdp_per_capita\"].to_numpy(float)}, index=idx)\n",
    "\n",
    "    # Standardize endog and exog (fit-time stats)\n",
    "    y_z, y_mu, y_sd = zscore_train(y.values)\n",
    "    X_z = X.copy()\n",
    "    X_z[\"gdp_per_capita\"], x_mu, x_sd = zscore_train(X[\"gdp_per_capita\"].values)\n",
    "\n",
    "    y_z = pd.Series(y_z, index=idx, name=\"y_z\")\n",
    "    X_z.index = idx\n",
    "\n",
    "    # 2023 exog (apply SAME scaling as training)\n",
    "    x2023_raw = float(fut[\"gdp_per_capita\"].iloc[0])\n",
    "    x2023_z   = (x2023_raw - x_mu) / (x_sd if x_sd != 0 else 1.0)\n",
    "    X_2023_z  = pd.DataFrame({\"gdp_per_capita\":[x2023_z]},\n",
    "                             index=make_year_end_index(pd.Series([2023], dtype=int)))\n",
    "\n",
    "    # Try a few robust orders for tiny samples\n",
    "    tried_orders = [(0,1,1), (1,0,0), (1,1,0), (0,1,0)]\n",
    "    fitted = None\n",
    "    last_err = \"\"\n",
    "    for order in tried_orders:\n",
    "        try:\n",
    "            model = SARIMAX(\n",
    "                y_z, exog=X_z, order=order, trend=\"c\",  # include constant\n",
    "                enforce_stationarity=False, enforce_invertibility=False\n",
    "            )\n",
    "            res = model.fit(disp=False, maxiter=1000, method=\"lbfgs\")\n",
    "            fitted = (order, res)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            continue\n",
    "\n",
    "    if fitted is None:\n",
    "        pred_rows.append({\"country\": c, \"forecast_2023\": None, \"lo80\": None, \"hi80\": None, \"order\": \"\", \"note\": f\"fit failed: {last_err[:80]}...\"})\n",
    "        continue\n",
    "\n",
    "    order_used, res = fitted\n",
    "\n",
    "    # One-step forecast on standardized scale\n",
    "    fc = res.get_forecast(steps=1, exog=X_2023_z)\n",
    "    mean_z = float(fc.predicted_mean.iloc[0])\n",
    "    ci_z   = fc.conf_int(alpha=0.2)  # 80% CI\n",
    "    lo_z   = float(ci_z.iloc[0, 0])\n",
    "    hi_z   = float(ci_z.iloc[0, 1])\n",
    "\n",
    "    # Unscale back to original total_score units\n",
    "    mean = mean_z * y_sd + y_mu\n",
    "    lo80 = lo_z   * y_sd + y_mu\n",
    "    hi80 = hi_z   * y_sd + y_mu\n",
    "\n",
    "    pred_rows.append({\n",
    "        \"country\": c,\n",
    "        \"forecast_2023\": mean,\n",
    "        \"lo80\": lo80,\n",
    "        \"hi80\": hi80,\n",
    "        \"order\": str(order_used),\n",
    "        \"note\": \"\"\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows).sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
