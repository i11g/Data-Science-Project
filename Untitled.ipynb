{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcd0a39-557a-42c8-9705-6c867dbff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26b5cb-756a-4eb4-8d55-f017492ed120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4d7599cb-7741-4f62-82b4-b67763270342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top10_per_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Loop through each year\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(top10_per_year[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m].unique()):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Get top 10 GDP and total_score countries for this year\u001b[39;00m\n\u001b[32m      4\u001b[39m     gdp_countries = \u001b[38;5;28mset\u001b[39m(top10_per_year[top10_per_year[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] == year][\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m     score_countries = \u001b[38;5;28mset\u001b[39m(top10_total_score[top10_total_score[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] == year][\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'top10_per_year' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through each year\n",
    "for year in sorted(top10_per_year[\"year\"].unique()):\n",
    "    # Get top 10 GDP and total_score countries for this year\n",
    "    gdp_countries = set(top10_per_year[top10_per_year[\"year\"] == year][\"country\"])\n",
    "    score_countries = set(top10_total_score[top10_total_score[\"year\"] == year][\"country\"])\n",
    "    \n",
    "    overlap = gdp_countries & score_countries\n",
    "    gdp_only = gdp_countries - score_countries\n",
    "    score_only = score_countries - gdp_countries\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for country in overlap:\n",
    "        plot_data.append((country, \"Overlap\"))\n",
    "    for country in gdp_only:\n",
    "        plot_data.append((country, \"GDP Only\"))\n",
    "    for country in score_only:\n",
    "        plot_data.append((country, \"Total_Score Only\"))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_plot = pd.DataFrame(plot_data, columns=[\"country\", \"category\"])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = {\"Overlap\": \"green\", \"GDP Only\": \"skyblue\", \"Total_Score Only\": \"orange\"}\n",
    "    plt.barh(df_plot[\"country\"], [1] * len(df_plot), color=df_plot[\"category\"].map(colors))\n",
    "    \n",
    "    plt.title(f\"Top 10 Overlap Between GDP and Total Score ({year})\")\n",
    "    plt.xlabel(\"Presence in Top 10\")\n",
    "    plt.ylabel(\"Country\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    legend_handles = [plt.Rectangle((0,0),1,1,color=colors[k]) for k in colors]\n",
    "    plt.legend(legend_handles, colors.keys(), title=\"Category\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a168ed2-c11f-41d1-b0f3-1f8c04bd4854",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "This project investigates the relationship between university rankings and economic indicators, specifically GDP, across countries from 2017 to 2022. By identifying the top-performing countries and analyzing their historical data, we apply various forecasting models to predict 2023 university scores. \n",
    "To accomplish this, we apply and compare a set of forecasting methods, including:\n",
    "\n",
    "- Linear Trend Extrapolation as a baseline; \n",
    "- exponential smoothing\n",
    "- ARIMA\n",
    "\n",
    "We then incorporate GDP data to explore economic influence, culminating in an ARIMAX model using GDP as an exogenous variable. Results are compared with actual 2023 data to evaluate prediction accuracy and economic influence.\n",
    "\n",
    "Each model offers a different perspective—some prioritize time dynamics, while others emphasize economic context and uncertainty. Forecasts are validated using actual 2024 QS rankings where available. The results highlight both the predictive power of national-level indicators and the strengths and weaknesses of each forecasting technique.\n",
    "\n",
    "This work provides a data-driven framework for understanding the evolution of educational performance across countries and suggests how policymakers and academic stakeholders might respond to long-term trends in global rankings. \n",
    "\n",
    "This study examines the interplay between economic development and the performance of elite higher education institutions. Utilizing QS World University Rankings data (2017–2024) for the top 300 universities in each year, coupled with GDP per capita statistics, we investigate the extent to which macroeconomic conditions shape institutional rankings. Restricting the analysis to the top 300 institutions ensures data completeness and reliability, mitigating biases from missing values prevalent in lower ranks. Employing a combination of statistical and time series forecasting approaches—including Linear Trend Extrapolation, Multiple Linear Regression, and ARIMA/ARIMAX—we project future ranking scores and evaluate the predictive power of economic indicators. The findings contribute to the understanding of how national economic performance influences the global standing of leading universities and offer implications for higher education policy and investment strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62d5e5-7c01-459f-83c0-aca29caa11ff",
   "metadata": {},
   "source": [
    "### Linear Trend Extrapolation\n",
    "To model simple growth or decline over time and provide a baseline forecast.\n",
    "\n",
    "Concept:\n",
    "Assumes a linear relationship between time $t$ and the response variable $y_t$ (e.g., score). \n",
    "$$\\hat{y}_t = \\beta_0 + \\beta_1 t$$\n",
    "Where $\\beta_0$ is the intercept, and $\\beta_1$ is the slope (change in score per year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738e9e0-bc8d-4117-a016-a7d4f0d94cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c7c04d31-3f84-4ee8-a2ba-749d5095217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            total_score   R-squared:                       0.092\n",
      "Model:                            OLS   Adj. R-squared:                  0.088\n",
      "Method:                 Least Squares   F-statistic:                     21.97\n",
      "Date:                Wed, 13 Aug 2025   Prob (F-statistic):           4.89e-06\n",
      "Time:                        07:05:04   Log-Likelihood:                -1732.4\n",
      "No. Observations:                 219   AIC:                             3469.\n",
      "Df Residuals:                     217   BIC:                             3476.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const            102.5591     78.625      1.304      0.193     -52.407     257.525\n",
      "gdp_per_capita     0.0081      0.002      4.687      0.000       0.005       0.012\n",
      "==============================================================================\n",
      "Omnibus:                      180.867   Durbin-Watson:                   1.181\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1871.691\n",
      "Skew:                           3.366   Prob(JB):                         0.00\n",
      "Kurtosis:                      15.641   Cond. No.                     7.97e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.97e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Merge your datasets (ensure GDP and total_score match in country-year)\n",
    "merged_df = total_score_df.merge(gdp_long, on=[\"country\", \"year\"], how=\"inner\")\n",
    "\n",
    "# Independent and dependent variables\n",
    "X = merged_df[\"gdp_per_capita\"]\n",
    "y = merged_df[\"total_score\"]\n",
    "\n",
    "# Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caf534-072b-4a96-8d2e-4430d2b114ee",
   "metadata": {},
   "source": [
    "### Correlation & Regression \n",
    "To explore whether economic performance influences university rankings.\n",
    "\n",
    "**Methods**\n",
    "Pearson correlation for linear relationships\n",
    "\n",
    "Spearman correlation for monotonic (not necessarily linear)\n",
    "\n",
    "Simple linear regression to model effect of GDP on score\n",
    "\n",
    "Simple Linear Regression Equation:\n",
    "$$\n",
    "\\hat{y}_t = \\beta_0 + \\beta_1 \\cdot \\text{GDP}_t + \\varepsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987f239-0568-4a83-adaa-6d2a2ee805ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rank = df_rank_uni[df_rank_uni.duplicated()]\n",
    "duplicate_gdp = df_gdp[df_gdp.duplicated()]\n",
    "\n",
    "print(\"\\nDuplicate rows in Ranking Dataset:\")\n",
    "print(duplicate_rank)\n",
    "\n",
    "print(\"\\n Duplicate rows in GDP Dataset:\")\n",
    "print(duplicate_gdp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5bbd1-bb16-4576-a5f9-e21841cbbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Rows in Ranking Dataset:\", df_rank.duplicated().sum())\n",
    "print(\"Duplicate Rows in GDP Dataset:\", df_gdp.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa381a97-a533-4286-97b5-1e331a8d844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_clean = df_rank_1.reset_index(drop=True).copy()\n",
    "missing_idx = np.where(df_rank_clean['score'].isna())[0]\n",
    "n = len(df_rank_clean)\n",
    "\n",
    "for i in missing_idx:\n",
    "    left  = df_rank_clean['score'].iloc[i-1] if i-1 >= 0 else np.nan\n",
    "    right = df_rank_clean['score'].iloc[i+1] if i+1 < n   else np.nan\n",
    "    df_rank_clean.at[i, 'score'] = np.nanmean([left, right])\n",
    "\n",
    "# In case both neighbors were NaN:\n",
    "df_rank_clean['score'] = df_rank_clean['score'].interpolate(limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ec2facd-2d84-4435-b02a-f2f4fa61a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_clean_2.to_csv(\"df_rank_clean_new_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c0254-b42e-47ff-9b0f-da825d2c6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_top10_all_years(df_clean):\n",
    "    # Calculate average score per country-year\n",
    "    avg_score = (\n",
    "        df_clean.groupby([\"country\", \"year\"], as_index=False)[\"score\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"score\": \"avg_score\"})\n",
    "    )\n",
    "\n",
    "    # Get top 10 per year\n",
    "    top10_per_year = (\n",
    "        avg_score.sort_values([\"year\", \"avg_score\"], ascending=[True, False])\n",
    "        .groupby(\"year\")\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    # Get unique years sorted\n",
    "    years = sorted(top10_per_year[\"year\"].unique())\n",
    "    n_years = len(years)\n",
    "\n",
    "    # Set up subplot grid\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_years, ncols=1, figsize=(10, 4 * n_years), constrained_layout=True\n",
    "    )\n",
    "\n",
    "    # In case there is only one year (avoid iterable error)\n",
    "    if n_years == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Plot each year's top 10\n",
    "    for ax, year in zip(axes, years):\n",
    "        data = top10_per_year[top10_per_year[\"year\"] == year]\n",
    "        sns.barplot(\n",
    "            data=data,\n",
    "            x=\"avg_score\",\n",
    "            y=\"country\",\n",
    "            palette=\"viridis\",\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"Top 10 Countries by Average Score ({year})\", fontsize=14)\n",
    "        ax.set_xlabel(\"Average Score\")\n",
    "        ax.set_ylabel(\"Country\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8453375-cd63-4879-a292-038fe5eb609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top10_all_years(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3689c26-a035-40b3-9dbd-f0a0ba000f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_university_dataset(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates by [year, country] and computes:\n",
    "      - total_score (sum of 'score')\n",
    "      - num_universities (unique 'university' count)\n",
    "    Then drops raw columns (if present) from the returned aggregated frame.\n",
    "    \"\"\"\n",
    "    # Aggregate\n",
    "    agg_df = (\n",
    "        df_clean.groupby([\"year\", \"country\"], as_index=False)\n",
    "                .agg(total_score=(\"score\", \"sum\"),\n",
    "                     num_universities=(\"university\", \"nunique\"))\n",
    "    )\n",
    "\n",
    "    # Drop the originals if you no longer need them (no-op here, but kept for clarity)\n",
    "    agg_df = agg_df.drop(columns=[\"university\", \"score\"], errors=\"ignore\")\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c43608-eb62-4147-88e5-9c46bb4a7e36",
   "metadata": {},
   "source": [
    "Forecasting GDP and ARIMAX Modeling\n",
    "Forecast GDP for 2023 using ARIMA\n",
    "\n",
    "GDP as exogenous variable. Target: 2023 university scores and/or number of universities\n",
    "\n",
    "Evaluate forecast vs actual 2023 data. Compare with ARIMA without GDP\n",
    "\n",
    "Document how you chose lag values and whether GDP improves forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0684e-af36-4200-b864-3b6f4b4a1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_long = gdp_long[[\"country\", \"year\", \"gdp_per_capita\"]]\n",
    "edu_long = edu_long[[\"country\", \"year\", \"gov_exp_edu\"]]\n",
    "gov_eff_long = gov_eff_long [[\"country\", \"year\", \"gov_effectiveness\"]]\n",
    "rd_exp_long = rd_exp_long [[\"country\", \"year\", \"rd_exp_gdp\"]]\n",
    "merged_df_gov = merged_df_gov[[\"country\", \"year\", \"gov_effectiveness\", \"total_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061cebd-827e-4bc5-a131-8b5b87d0e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# --- 0) Minimal sanity cleaning for each source ---\n",
    "def prep(df, cols):\n",
    "    out = df[cols].copy()\n",
    "    # standardize column names and types\n",
    "    out[\"country\"] = out[\"country\"].astype(str).str.strip()\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    # drop exact duplicates\n",
    "    out = out.drop_duplicates(subset=[\"country\", \"year\"])\n",
    "    return out\n",
    "\n",
    "gdp_long      = prep(gdp_long,      [\"country\", \"year\", \"gdp_per_capita\"])\n",
    "edu_long      = prep(edu_long,      [\"country\", \"year\", \"gov_exp_edu\"])\n",
    "gov_eff_long  = prep(gov_eff_long,  [\"country\", \"year\", \"gov_effectiveness\"])\n",
    "rd_exp_long   = prep(rd_exp_long,   [\"country\", \"year\", \"rd_exp_gdp\"])\n",
    "merged_df_gov = prep(merged_df_gov, [\"country\", \"year\", \"gov_effectiveness\", \"total_score\"])\n",
    "\n",
    "# If merged_df_gov also has gov_effectiveness, keep the score from there\n",
    "# and prefer the indicator column from the dedicated gov_eff_long source.\n",
    "merged_df_gov = merged_df_gov.rename(columns={\"gov_effectiveness\": \"gov_effectiveness_from_score\"})\n",
    "\n",
    "# --- 1) Merge all indicators onto the score base (left join preserves your scoring rows) ---\n",
    "dfs_to_merge = [\n",
    "    merged_df_gov,\n",
    "    gdp_long,\n",
    "    edu_long,\n",
    "    gov_eff_long,   # authoritative gov_effectiveness\n",
    "    rd_exp_long\n",
    "]\n",
    "\n",
    "def left_merge(a, b):\n",
    "    # prevent accidental suffix collisions\n",
    "    cols_before = set(a.columns)\n",
    "    out = a.merge(b, on=[\"country\", \"year\"], how=\"left\", validate=\"one_to_one\")\n",
    "    # sanity check for duplicate indicators\n",
    "    dupes = [c for c in out.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "    if dupes:\n",
    "        raise ValueError(f\"Unexpected duplicate columns after merge: {dupes}\")\n",
    "    return out\n",
    "\n",
    "full_df = reduce(left_merge, dfs_to_merge)\n",
    "\n",
    "# Replace the possibly-missing 'gov_effectiveness' with the one from the score table if needed\n",
    "if \"gov_effectiveness\" in full_df.columns:\n",
    "    full_df[\"gov_effectiveness\"] = full_df[\"gov_effectiveness\"].fillna(full_df[\"gov_effectiveness_from_score\"])\n",
    "else:\n",
    "    full_df[\"gov_effectiveness\"] = full_df[\"gov_effectiveness_from_score\"]\n",
    "\n",
    "# Drop helper column\n",
    "full_df = full_df.drop(columns=[\"gov_effectiveness_from_score\"], errors=\"ignore\")\n",
    "\n",
    "# --- 2) Restrict to the modeling window 2017–2023 and ensure clean dtypes ---\n",
    "full_df = full_df[(full_df[\"year\"] >= 2017) & (full_df[\"year\"] <= 2023)].copy()\n",
    "full_df[\"year\"] = full_df[\"year\"].astype(int)\n",
    "\n",
    "# --- 3) If there are still multiple rows per (country,year), aggregate safely ---\n",
    "if full_df.duplicated(subset=[\"country\", \"year\"]).any():\n",
    "    agg_map = {\n",
    "        \"total_score\": \"sum\",             # country-year total from top 300\n",
    "        \"gdp_per_capita\": \"mean\",\n",
    "        \"gov_exp_edu\": \"mean\",\n",
    "        \"gov_effectiveness\": \"mean\",\n",
    "        \"rd_exp_gdp\": \"mean\"\n",
    "    }\n",
    "    full_df = (\n",
    "        full_df.groupby([\"country\", \"year\"], as_index=False)\n",
    "               .agg(agg_map)\n",
    "    )\n",
    "\n",
    "# --- 4) Quick completeness report ---\n",
    "needed_cols = [\"total_score\", \"gdp_per_capita\", \"gov_exp_edu\", \"gov_effectiveness\", \"rd_exp_gdp\"]\n",
    "missing_summary = (\n",
    "    full_df.assign(missing_any = full_df[needed_cols].isna().any(axis=1))\n",
    "           .groupby(\"year\")[\"missing_any\"].value_counts(dropna=False)\n",
    "           .unstack(fill_value=0)\n",
    "           .rename(columns={True:\"rows_with_NA\", False:\"rows_complete\"})\n",
    ")\n",
    "print(\"Completeness by year (rows):\\n\", missing_summary)\n",
    "\n",
    "# --- 5) Split train/test frames we’ll use for the three simple prediction approaches ---\n",
    "train_2017_2022 = full_df[(full_df[\"year\"] >= 2017) & (full_df[\"year\"] <= 2022)].dropna(subset=needed_cols).copy()\n",
    "test_2023        = full_df[(full_df[\"year\"] == 2023)].dropna(subset=needed_cols).copy()\n",
    "\n",
    "print(\"\\nTrain rows:\", len(train_2017_2022), \" | Test 2023 rows:\", len(test_2023))\n",
    "print(\"\\nColumns:\", list(full_df.columns))\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad3e32-0d72-4dba-af76-aef13930a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# 0) Merge target + GDP\n",
    "panel = (\n",
    "    merged_df_gov[[\"country\",\"year\",\"total_score\"]]\n",
    "    .merge(gdp_long[[\"country\",\"year\",\"gdp_per_capita\"]], on=[\"country\",\"year\"], how=\"left\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Hygiene\n",
    "panel[\"country\"] = panel[\"country\"].astype(str).str.strip()\n",
    "panel[\"year\"] = panel[\"year\"].astype(int)\n",
    "panel[\"gdp_per_capita\"] = pd.to_numeric(panel[\"gdp_per_capita\"], errors=\"coerce\")\n",
    "\n",
    "# Use log GDP (often more linear)\n",
    "panel[\"log_gdp\"] = np.log(panel[\"gdp_per_capita\"])\n",
    "\n",
    "# 1) Train on 2017–2022\n",
    "train = panel.query(\"2017 <= year <= 2022\").dropna(subset=[\"total_score\",\"log_gdp\"]).copy()\n",
    "\n",
    "# 2) Top-10 countries by avg total_score (2017–2022)\n",
    "top10 = (train.groupby(\"country\")[\"total_score\"].mean()\n",
    "               .nlargest(10).index.tolist())\n",
    "\n",
    "# Restrict training to those countries only (optional, keeps focus consistent)\n",
    "train_top10 = train[train[\"country\"].isin(top10)].copy()\n",
    "\n",
    "# 3) Fit fixed-effects OLS with a time trend\n",
    "# total_score ~ country dummies + year + log_gdp\n",
    "fe = smf.ols(\"total_score ~ C(country) + year + log_gdp\", data=train_top10)\\\n",
    "        .fit(cov_type=\"cluster\", cov_kwds={\"groups\": train_top10[\"country\"]})\n",
    "print(fe.summary())\n",
    "\n",
    "# 4) Build 2023 prediction frame for the same top-10 countries (pull 2023 GDP)\n",
    "future_2023 = (\n",
    "    gdp_long.query(\"year == 2023 and country in @top10\")\n",
    "            [[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "            .drop_duplicates()\n",
    "            .copy()\n",
    ")\n",
    "future_2023[\"log_gdp\"] = np.log(pd.to_numeric(future_2023[\"gdp_per_capita\"], errors=\"coerce\"))\n",
    "\n",
    "# In case any GDP is missing after coercion:\n",
    "future_2023 = future_2023.dropna(subset=[\"log_gdp\"])\n",
    "\n",
    "# 5) Predict 2023\n",
    "future_2023[\"forecast_2023\"] = fe.predict(future_2023)\n",
    "\n",
    "pred_lr = future_2023[[\"country\",\"forecast_2023\"]].sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c1870-06ea-4cd3-bfad-713337d380b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b262ede-df6b-4fbe-a314-fdb9854da1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# 0) Merge target + GDP\n",
    "panel = (\n",
    "    merged_df_gov[[\"country\",\"year\",\"total_score\"]]\n",
    "    .merge(gdp_long[[\"country\",\"year\",\"gdp_per_capita\"]], on=[\"country\",\"year\"], how=\"left\")\n",
    "    .dropna(subset=[\"total_score\",\"gdp_per_capita\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# 1) Transform GDP (often helps linearity)\n",
    "panel[\"log_gdp\"] = np.log(panel[\"gdp_per_capita\"])\n",
    "\n",
    "# 2) Train on 2017–2022\n",
    "train = panel.query(\"2017 <= year <= 2022\").copy()\n",
    "\n",
    "# 3) Fit Fixed-Effects OLS with time trend\n",
    "fe_model = smf.ols(\"total_score ~ C(country) + year + log_gdp\", data=train).fit(cov_type=\"cluster\", cov_kwds={\"groups\": train[\"country\"]})\n",
    "print(fe_model.summary())\n",
    "\n",
    "# 4) Prepare 2023 prediction rows for top-10 countries by 2017–2022 avg score\n",
    "top10 = (train.groupby(\"country\")[\"total_score\"].mean().nlargest(10).index.tolist())\n",
    "\n",
    "future_2023 = (\n",
    "    panel.query(\"year == 2023 and country in @top10\")[[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# If some GDP 2023 are missing, pull directly from gdp_long\n",
    "if future_2023.empty or future_2023[\"gdp_per_capita\"].isna().any():\n",
    "    fallback = gdp_long.query(\"year == 2023 and country in @top10\")[[\"country\",\"year\",\"gdp_per_capita\"]]\n",
    "    future_2023 = fallback.copy()\n",
    "\n",
    "future_2023[\"log_gdp\"] = np.log(future_2023[\"gdp_per_capita\"])\n",
    "\n",
    "# 5) Predict 2023\n",
    "future_2023[\"forecast_2023\"] = fe_model.predict(future_2023)\n",
    "\n",
    "pred_fe = future_2023[[\"country\",\"forecast_2023\"]].sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87894a-2f93-4557-86f1-68628869bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aca868-e07c-4cd6-96e5-9529407635ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def zscore_train(x):\n",
    "    mu = float(np.nanmean(x))\n",
    "    sd = float(np.nanstd(x, ddof=0))\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        sd = 1.0\n",
    "    return (x - mu) / sd, mu, sd\n",
    "\n",
    "def make_year_end_index(years):\n",
    "    # Convert int years to year-end timestamps with explicit freq\n",
    "    return pd.PeriodIndex(years.astype(str), freq=\"Y-DEC\").to_timestamp()\n",
    "\n",
    "pred_rows = []\n",
    "\n",
    "# Use your df_gdp_exog with the OUTER merge from earlier\n",
    "top10 = (df_gdp_exog.query(\"year <= 2022\")\n",
    "         .dropna(subset=[\"total_score\"])\n",
    "         .groupby(\"country\")[\"total_score\"].mean()\n",
    "         .nlargest(10).index.tolist())\n",
    "\n",
    "for c in top10:\n",
    "    cd = df_gdp_exog[df_gdp_exog[\"country\"] == c].copy()\n",
    "    train = cd[(cd[\"year\"] >= 2017) & (cd[\"year\"] <= 2022)].dropna(subset=[\"total_score\",\"gdp_per_capita\"])\n",
    "    fut  = cd[(cd[\"year\"] == 2023)][[\"gdp_per_capita\"]]\n",
    "\n",
    "    if len(train) < 4 or fut.empty or fut.isna().any().any():\n",
    "        pred_rows.append({\"country\": c, \"forecast_2023\": None, \"lo80\": None, \"hi80\": None, \"order\": \"\", \"note\": \"too little data or missing 2023 GDP\"})\n",
    "        continue\n",
    "\n",
    "    # Index with explicit yearly frequency\n",
    "    idx = make_year_end_index(train[\"year\"])\n",
    "    y  = pd.Series(train[\"total_score\"].to_numpy(float), index=idx, name=\"total_score\")\n",
    "    X  = pd.DataFrame({\"gdp_per_capita\": train[\"gdp_per_capita\"].to_numpy(float)}, index=idx)\n",
    "\n",
    "    # Standardize endog and exog (fit-time stats)\n",
    "    y_z, y_mu, y_sd = zscore_train(y.values)\n",
    "    X_z = X.copy()\n",
    "    X_z[\"gdp_per_capita\"], x_mu, x_sd = zscore_train(X[\"gdp_per_capita\"].values)\n",
    "\n",
    "    y_z = pd.Series(y_z, index=idx, name=\"y_z\")\n",
    "    X_z.index = idx\n",
    "\n",
    "    # 2023 exog (apply SAME scaling as training)\n",
    "    x2023_raw = float(fut[\"gdp_per_capita\"].iloc[0])\n",
    "    x2023_z   = (x2023_raw - x_mu) / (x_sd if x_sd != 0 else 1.0)\n",
    "    X_2023_z  = pd.DataFrame({\"gdp_per_capita\":[x2023_z]},\n",
    "                             index=make_year_end_index(pd.Series([2023], dtype=int)))\n",
    "\n",
    "    # Try a few robust orders for tiny samples\n",
    "    tried_orders = [(0,1,1), (1,0,0), (1,1,0), (0,1,0)]\n",
    "    fitted = None\n",
    "    last_err = \"\"\n",
    "    for order in tried_orders:\n",
    "        try:\n",
    "            model = SARIMAX(\n",
    "                y_z, exog=X_z, order=order, trend=\"c\",  # include constant\n",
    "                enforce_stationarity=False, enforce_invertibility=False\n",
    "            )\n",
    "            res = model.fit(disp=False, maxiter=1000, method=\"lbfgs\")\n",
    "            fitted = (order, res)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            continue\n",
    "\n",
    "    if fitted is None:\n",
    "        pred_rows.append({\"country\": c, \"forecast_2023\": None, \"lo80\": None, \"hi80\": None, \"order\": \"\", \"note\": f\"fit failed: {last_err[:80]}...\"})\n",
    "        continue\n",
    "\n",
    "    order_used, res = fitted\n",
    "\n",
    "    # One-step forecast on standardized scale\n",
    "    fc = res.get_forecast(steps=1, exog=X_2023_z)\n",
    "    mean_z = float(fc.predicted_mean.iloc[0])\n",
    "    ci_z   = fc.conf_int(alpha=0.2)  # 80% CI\n",
    "    lo_z   = float(ci_z.iloc[0, 0])\n",
    "    hi_z   = float(ci_z.iloc[0, 1])\n",
    "\n",
    "    # Unscale back to original total_score units\n",
    "    mean = mean_z * y_sd + y_mu\n",
    "    lo80 = lo_z   * y_sd + y_mu\n",
    "    hi80 = hi_z   * y_sd + y_mu\n",
    "\n",
    "    pred_rows.append({\n",
    "        \"country\": c,\n",
    "        \"forecast_2023\": mean,\n",
    "        \"lo80\": lo80,\n",
    "        \"hi80\": hi80,\n",
    "        \"order\": str(order_used),\n",
    "        \"note\": \"\"\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows).sort_values(\"country\").reset_index(drop=True)\n",
    "print(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
