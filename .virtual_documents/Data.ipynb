import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA















































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


# Display basic information
df_rank.info()


df_rank.describe().T


df_rank.head()


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()

print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank} years")


print("Missing Values in Ranking Dataset:")
print(df_rank.isnull().sum())


# Convert rank_display to numeric 
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 universities for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# check missing by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


df_top300.to_csv("top300_universities.csv", index=False)


df_top300.shape





# Keep only the columns needed 
df_clean = df_top300[["university", "year", "score", "country"]].copy()


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

# Replace common variations
country_replacements = {
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",
    "UK": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean["country"] = df_clean["country"].replace(country_replacements)


df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


# Drop the missing values
df_clean = df_clean.dropna(subset=["score"])


# Convert to int and float
df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


df_clean.info()


def tidy_university_dataset(df_clean: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregates by [year, country] and computes:
      - total_score (sum of 'score')
      - num_universities (unique 'university' count)
    Then drops raw columns (if present) from the returned aggregated frame.
    """
    # Aggregate
    agg_df = (
        df_clean.groupby(["year", "country"], as_index=False)
                .agg(total_score=("score", "sum"),
                     num_universities=("university", "nunique"))
    )

    # Drop the originals if you no longer need them (no-op here, but kept for clarity)
    agg_df = agg_df.drop(columns=["university", "score"], errors="ignore")

    return agg_df


def plot_totals_and_counts_per_country_per_year(
    agg_data: pd.DataFrame,
    top_n: int = 10,
    top_by: str = "total_score",   # or "num_universities"
    figsize_per_row: tuple = (10, 2.8)
):
    """
    Makes two figures stacked by years:
      1) Total score per country
      2) Number of universities per country
    Uses the same top-N selection per year based on `top_by`.
    """
    years = sorted(agg_data["year"].unique())
    n_years = len(years)

    def top_per_year(df_year, by_col):
        return df_year.sort_values(by_col, ascending=False).head(top_n)

    # -------- Figure 1: Total Score --------
    fig1, axes1 = plt.subplots(
        n_years, 1,
        figsize=(figsize_per_row[0], max(figsize_per_row[1] * n_years, 2.5)),
        squeeze=False
    )
    for i, yr in enumerate(years):
        sub = agg_data[agg_data["year"] == yr].copy()
        top_set = top_per_year(sub, top_by)["country"]
        sub = sub[sub["country"].isin(top_set)].sort_values("total_score", ascending=False)

        ax = axes1[i, 0]
        ax.bar(sub["country"], sub["total_score"])
        ax.set_title(f"Total Score by Country — {yr}")
        ax.set_ylabel("Total Score")

        # Rotate + align tick labels (no 'ha' inside tick_params!)
        ax.tick_params(axis='x', labelrotation=45)
        plt.setp(ax.get_xticklabels(), ha='right')

    fig1.tight_layout()

    # -------- Figure 2: University Count --------
    fig2, axes2 = plt.subplots(
        n_years, 1,
        figsize=(figsize_per_row[0], max(figsize_per_row[1] * n_years, 2.5)),
        squeeze=False
    )
    for i, yr in enumerate(years):
        sub = agg_data[agg_data["year"] == yr].copy()
        top_set = top_per_year(sub, top_by)["country"]
        sub = sub[sub["country"].isin(top_set)].sort_values("num_universities", ascending=False)

        ax = axes2[i, 0]
        ax.bar(sub["country"], sub["num_universities"])
        ax.set_title(f"University Count by Country — {yr}")
        ax.set_ylabel("# Universities")

        ax.tick_params(axis='x', labelrotation=45)
        plt.setp(ax.get_xticklabels(), ha='right')

    fig2.tight_layout()
    return fig1, fig2


agg_data = tidy_university_dataset(df_clean)
fig1, fig2 = plot_totals_and_counts_per_country_per_year(
    agg_data, top_n=10, top_by="total_score"
)
plt.show()


agg_data.info()








# Find top 10 countries by total_score
top10_countries = (
    agg_data.groupby("country")["total_score"]
    .mean()
    .nlargest(10)
    .index
)
# Filter dataset
top10_df = agg_data[agg_data["country"].isin(top10_countries)].copy()
top10_df = top10_df.sort_values(["country", "year"])


# Forecasting by moving average and linear forecast

# ---- 1) Time-index helpers ----
def yearly_series(group, value_col="total_score"):
    """Regular annual PeriodIndex series with freq=Y-DEC. Interpolates gaps."""
    g = group.sort_values("year")
    s = g.set_index(pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC"))[value_col]
    s = s.asfreq("Y-DEC")
    if s.isna().any():
        s = s.interpolate(limit_direction="both")
    return s
    
def future_index(s, steps):
    """Future yearly PeriodIndex aligned to s.frequency."""
    return pd.period_range(s.index[-1] + 1, periods=steps, freq=s.index.freq)

# ---- 2) Moving Average forecast ----
def moving_average_forecast(df, window=3, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        if len(s) == 0:
            # empty guard
            idx = pd.period_range(2000, periods=steps, freq="Y-DEC")
            results[c] = pd.Series([np.nan]*steps, index=idx)
            continue
        w = max(1, min(window, len(s)))
        ma_last = s.rolling(w).mean().iloc[-1]
        idx = future_index(s, steps)
        results[c] = pd.Series([ma_last]*steps, index=idx)
    return results

# ---- 3) Linear Trend Extrapolation (no sklearn; pure NumPy) ----
def linear_trend_forecast(df, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        g = g.sort_values("year")
        years = g["year"].astype(int).values
        y = g["total_score"].values

        if len(y) < 2:
            # fallback to last observed value if too short
            last = y[-1] if len(y) else np.nan
            last_year = years[-1] if len(years) else 2023
            fut_years = np.arange(last_year+1, last_year+1+steps)
            idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
            results[c] = pd.Series([last]*steps, index=idx)
            continue

        # center years for numerical stability
        base = years.min()
        x = (years - base).astype(float)
        # fit y = a*x + b
        a, b = np.polyfit(x, y, 1)

        last_year = years[-1]
        fut_years = np.arange(last_year+1, last_year+1+steps)
        x_future = (fut_years - base).astype(float)
        preds = a * x_future + b
        idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
        results[c] = pd.Series(preds, index=idx)
    return results

# ---- 4) Run both & build comparison table ----
STEPS = 2  # forecast next 2 years beyond each country's last observed year
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)

rows = []
for c in top10_countries:
    # Use linear trend index for naming (both methods share same future years count)
    idx = lin_preds[c].index
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]  = ma_preds[c].iloc[i]  if len(ma_preds[c])  > i else np.nan
        row[f"LIN_{y}"] = lin_preds[c].iloc[i] if len(lin_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)


comparison_df


# Forecasting using ARIMA

def arima_forecast(df, steps=2, order=(1,1,0)):
    """
    ARIMA forecast for total_score per country.
    order=(p,d,q):
        p - AR order
        d - differencing order
        q - MA order
    """
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)  # from your helper — gives annual PeriodIndex, interpolated

        if s.isna().all() or len(s) < (order[0] + order[2] + 1):
            # fallback: repeat last value
            last_val = s.iloc[-1] if len(s) else np.nan
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)
            continue

        try:
            # Fit ARIMA model
            model = ARIMA(s, order=order)
            fitted = model.fit()

            # Forecast
            forecast = fitted.forecast(steps=steps)
            idx = future_index(s, steps)
            results[c] = pd.Series(forecast.values, index=idx)
        except Exception as e:
            # Fallback on error
            last_val = s.iloc[-1]
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)
    return results


STEPS = 2
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)
arima_preds = arima_forecast(top10_df, steps=STEPS, order=(1,1,0))


rows = []
for c in top10_countries:
    idx = lin_preds[c].index  # consistent years
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]    = ma_preds[c].iloc[i]    if len(ma_preds[c]) > i else np.nan
        row[f"LIN_{y}"]   = lin_preds[c].iloc[i]   if len(lin_preds[c]) > i else np.nan
        row[f"ARIMA_{y}"] = arima_preds[c].iloc[i] if len(arima_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)


comparison_df








df_rank_2023 = pd.read_csv("data/2023 QS World University Rankings.csv")


df_rank_2023


df_rank_2024 = pd.read_csv("data/2024 QS World University Rankings 1.1 (For qs.com).csv")


df_rank_2024


df_rank_2023.shape





# Investigate columns from both data
columns1 = df_rank.columns # columns from data 2017-2022 
columns2 = df_rank_2023.columns # columns from data 2023 

print("columns from data 2017-2022:", columns1)
print("columns from data 2023 :", columns2)





# copy selected columns from Data 2023
df_temp2023 = df_rank_2023[['institution','Rank','score scaled','location']].copy()
# add year column
df_temp2023['year'] = 2023
# Check result
df_temp2023.info()


df_temp2023.columns = ['university','rank_display','score','country', 'year']
# Check result
df_temp2023.info()


# Convert rank_display to numeric 
df_temp2023['rank_display'] = pd.to_numeric(df_temp2023['rank_display'], errors='coerce')

# Filter top 300 for each year
df_top300_2023 = df_temp2023[df_temp2023['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300_2023['score'].isna().sum()
total_count = len(df_top300_2023)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")


# Start with a copy to avoid accidental index mismatch
df_clean_2023 = df_top300_2023.copy()

# Strip whitespace and title-case country names
df_clean_2023["country"] = df_clean_2023["country"].str.strip().str.title()

# Replace common variations
country_replacements = {
    "Usa": "United States",
    "U.s.a.": "United States",
    "United States Of America": "United States",
    "Uk": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean_2023["country"] = df_clean_2023["country"].replace(country_replacements)


def tidy_university_dataset(df_clean_2023: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregates by [year, country] and computes:
      - total_score (sum of 'score' across universities)
      - num_universities (unique 'university' count)
    Returns one row per (year, country).
    """
    agg_df = (
        df_clean_2023.groupby(["year", "country"], as_index=False)
                .agg(total_score=("score", "sum"),
                     num_universities=("university", "nunique"))
    )
    return agg_df

# Build aggregated dataset
agg_df_yr_country = tidy_university_dataset(df_clean_2023)

# Actuals for 2023 (one row per country)
actuals_2023 = (
    agg_df_yr_country.loc[agg_df_yr_country["year"] == 2023, ["country", "total_score"]]
    .rename(columns={"total_score": "actual_2023"})
)

# Keep only top-10 countries you forecasted for
actuals_2023 = actuals_2023[actuals_2023["country"].isin(top10_countries)].reset_index(drop=True)


actuals_2023


# One row per (year, country)
assert agg_df_yr_country.duplicated(["year", "country"]).sum() == 0

# All top10 countries appear in actuals (unless missing in 2023)
print(set(top10_countries) - set(actuals_2023["country"]))


comparison_df["country"] = comparison_df["country"].astype(str).str.strip()
actuals_2023["country"]  = actuals_2023["country"].astype(str).str.strip()


if "actual_2023" in comparison_df.columns:
    comparison_df = comparison_df.drop(columns=["actual_2023"])

comparison_df = comparison_df.merge(actuals_2023, on="country", how="left")


comparison_df


# Drop redundant columns
comparison_df = comparison_df.drop(columns=["actual_2023_x", "actual_2023_y"], errors="ignore")


comparison_df


for model in ["MA", "LIN", "ARIMA"]:
    comparison_df[f"{model}_abs_error_2023"] = (comparison_df[f"{model}_2023"] - comparison_df["actual_2023"]).abs()
    comparison_df[f"{model}_pct_error_2023"] = (
        (comparison_df[f"{model}_2023"] - comparison_df["actual_2023"]).abs() / comparison_df["actual_2023"] * 100
    )


comparison_df


avg_errors = comparison_df[
    [col for col in comparison_df.columns if "pct_error_2023" in col]
].mean().sort_values()

print(avg_errors)


# Visulas 











df_gdp = pd.read_csv("data/gdp.csv")


df_gdp.info()


df_gdp.head(5)





# Keep only GDP per capita (current US$)
gdp_filtered = df_gdp[df_gdp["Series Name"] == "GDP per capita (current US$)"].copy()

# Drop unneeded columns
gdp_filtered = gdp_filtered.drop(columns=["Country Code", "Series Code", "Series Name"], errors="ignore")
gdp_filtered = gdp_filtered.loc[:, ~gdp_filtered.columns.str.contains("^Unnamed")]

# Reshape from wide to long format
gdp_long = gdp_filtered.melt(id_vars=["Country Name"], var_name="year", value_name="gdp_per_capita")

# Clean year strings like "2017 [YR2017]" -> "2017"
gdp_long["year"] = gdp_long["year"].str.extract(r"(\d{4})")

# Continue with steps
gdp_long = gdp_long.rename(columns={"Country Name": "country"})
gdp_long["year"] = pd.to_numeric(gdp_long["year"], errors="coerce")
gdp_long = gdp_long.dropna(subset=["year", "gdp_per_capita"])
gdp_long["year"] = gdp_long["year"].astype(int)

# Filter for 2017–2024
gdp_long = gdp_long[gdp_long["year"].between(2017, 2024)]

gdp_long.head(10)








# Merge GDP with ranking stats
merged_df = pd.merge(gdp_long, agg_data, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df.head(10)


merged_df.info()


# Compute correlation matrix
correlation = merged_df[["gdp_per_capita", "total_score", "num_universities"]].corr()

# Display correlation table
print(correlation)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Clean numeric conversion
def to_numeric(series):
    return pd.to_numeric(series, errors="coerce")

# Ensure numeric
merged_df['total_score'] = to_numeric(merged_df['total_score'])
merged_df['gdp_per_capita'] = to_numeric(merged_df['gdp_per_capita'])

# Drop missing rows
corr_df = merged_df.dropna(subset=['total_score', 'gdp_per_capita'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gdp_per_capita'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gdp_per_capita'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gdp_per_capita'])

print("=== Correlation with GDP per Capita ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")



# Load dataset
edu_df = pd.read_csv("data/gov_exp_on_edu.csv")


edu_df.info()


edu_df.head(5)


# Step 2: Keep only education expenditure rows
edu_filtered = edu_df[
    edu_df["Series Name"] == "Government expenditure on education, total (% of GDP)"
].copy()

# Step 3: Drop unneeded columns
edu_filtered = edu_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
edu_filtered = edu_filtered.loc[:, ~edu_filtered.columns.str.contains("^Unnamed")]

# Step 4: Reshape from wide to long format
edu_long = edu_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_exp_edu"
)

# Extract just the year number from "2017 [YR2017]"
edu_long["year"] = edu_long["year"].str.extract(r"(\d{4})")

# Rename country column
edu_long = edu_long.rename(columns={"Country Name": "country"})

# Convert to numeric types
edu_long["year"] = pd.to_numeric(edu_long["year"], errors="coerce")
edu_long["gov_exp_edu"] = pd.to_numeric(edu_long["gov_exp_edu"], errors="coerce")

# Drop missing values
edu_long = edu_long.dropna(subset=["year", "gov_exp_edu"])

# Convert year to int
edu_long["year"] = edu_long["year"].astype(int)

# Filter only 2017–2024
edu_long = edu_long[edu_long["year"].between(2017, 2023)]

print(edu_long.dtypes)
print(edu_long.head(10))


# Merge GDP with ranking stats
merged_df_edu = pd.merge(edu_long, agg_data, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_edu.head(10)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Ensure numeric
merged_df_edu['total_score'] = pd.to_numeric(merged_df_edu['total_score'], errors='coerce')
merged_df_edu['gov_exp_edu'] = pd.to_numeric(merged_df_edu['gov_exp_edu'], errors='coerce')

# Drop missing rows
corr_df = merged_df_edu.dropna(subset=['total_score', 'gov_exp_edu'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_exp_edu'])

print("=== Correlation with Government Expenditure on Education ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Compute correlation matrix
correlation_edu = merged_df_edu[["gov_exp_edu", "total_score", "num_universities"]].corr()

# Display correlation table
print(correlation_edu)


# Load dataset
gov_df = pd.read_csv("data/Gov_effect_score .csv")


gov_df.head(5)


# Step 1: Keep only Government Effectiveness rows
gov_eff_filtered = gov_df[
    gov_df["Series Name"] == "Government Effectiveness: Percentile Rank"
].copy()

# Step 2: Drop unneeded columns
gov_eff_filtered = gov_eff_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
gov_eff_filtered = gov_eff_filtered.loc[:, ~gov_eff_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
gov_eff_long = gov_eff_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_effectiveness"
)

# Extract year number from "2017 [YR2017]"
gov_eff_long["year"] = gov_eff_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
gov_eff_long = gov_eff_long.rename(columns={"Country Name": "country"})

# Convert to numeric
gov_eff_long["year"] = pd.to_numeric(gov_eff_long["year"], errors="coerce")
gov_eff_long["gov_effectiveness"] = pd.to_numeric(gov_eff_long["gov_effectiveness"], errors="coerce")

# Drop missing values
gov_eff_long = gov_eff_long.dropna(subset=["year", "gov_effectiveness"])

# Ensure year is integer
gov_eff_long["year"] = gov_eff_long["year"].astype(int)

# Filter for years 2017–2024
gov_eff_long = gov_eff_long[gov_eff_long["year"].between(2017, 2023)]

print(gov_eff_long.dtypes)
print(gov_eff_long.head(10))


# Merge GDP with ranking stats
merged_df_gov = pd.merge(gov_eff_long, agg_data, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_gov.head(5)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Ensure numeric
merged_df_gov['total_score'] = pd.to_numeric(merged_df_gov['total_score'], errors='coerce')
merged_df_gov['gov_effectiveness'] = pd.to_numeric(merged_df_gov['gov_effectiveness'], errors='coerce')

# Drop missing rows
corr_df = merged_df_gov.dropna(subset=['total_score', 'gov_effectiveness'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_effectiveness'])

print("=== Correlation with Government Effectiveness ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Example: if your current year columns are like '2017 [YR2017]', convert to numeric
def clean_year(df):
    df["year"] = pd.to_numeric(df["year"], errors="coerce")
    return df

gdp_long = clean_year(gdp_long)
edu_long = clean_year(edu_long)
merged_df_gov = clean_year(merged_df_gov)


# Load dataset
res_df = pd.read_csv("data/res_dev.csv")  


res_df.head(5)


# Step 1: Keep only R&D expenditure rows
rd_exp_filtered = res_df[
    res_df["Series Name"] == "Research and development expenditure (% of GDP)"
].copy()

# Step 2: Drop unneeded columns
rd_exp_filtered = rd_exp_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
rd_exp_filtered = rd_exp_filtered.loc[:, ~rd_exp_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
rd_exp_long = rd_exp_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="rd_exp_gdp"
)

# Extract year number from "2017 [YR2017]"
rd_exp_long["year"] = rd_exp_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
rd_exp_long = rd_exp_long.rename(columns={"Country Name": "country"})

# Convert to numeric
rd_exp_long["year"] = pd.to_numeric(rd_exp_long["year"], errors="coerce")
rd_exp_long["rd_exp_gdp"] = pd.to_numeric(rd_exp_long["rd_exp_gdp"], errors="coerce")

# Drop missing values
rd_exp_long = rd_exp_long.dropna(subset=["year", "rd_exp_gdp"])

# Ensure year is integer
rd_exp_long["year"] = rd_exp_long["year"].astype(int)

# Filter for years 2017–2023
rd_exp_long = rd_exp_long[rd_exp_long["year"].between(2017, 2023)]

print(rd_exp_long.dtypes)
print(rd_exp_long.head(10))


from scipy.stats import pearsonr, spearmanr, kendalltau
import pandas as pd

# Assuming:
# rd_exp_long → cleaned dataset with ['country', 'year', 'rd_exp_gdp']
# merged_df   → main dataset with ['country', 'year', 'total_score']

# --- Merge R&D data into main dataset ---
merged_rd = pd.merge(
    merged_df,
    rd_exp_long,
    on=["country", "year"],
    how="inner"
)

# Ensure numeric types
merged_rd['total_score'] = pd.to_numeric(merged_rd['total_score'], errors='coerce')
merged_rd['rd_exp_gdp'] = pd.to_numeric(merged_rd['rd_exp_gdp'], errors='coerce')

# Drop NaNs
corr_df = merged_rd.dropna(subset=['total_score', 'rd_exp_gdp'])

# --- Pearson ---
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Spearman ---
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Kendall ---
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Print ---
print("=== Correlation with R&D Expenditure (% of GDP) ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")












gdp_long = gdp_long[["country", "year", "gdp_per_capita"]]
edu_long = edu_long[["country", "year", "gov_exp_edu"]]
gov_eff_long = gov_eff_long [["country", "year", "gov_effectiveness"]]
rd_exp_long = rd_exp_long [["country", "year", "rd_exp_gdp"]]
merged_df_gov = merged_df_gov[["country", "year", "gov_effectiveness", "total_score"]]


import pandas as pd
from functools import reduce

# --- 0) Minimal sanity cleaning for each source ---
def prep(df, cols):
    out = df[cols].copy()
    # standardize column names and types
    out["country"] = out["country"].astype(str).str.strip()
    out["year"] = pd.to_numeric(out["year"], errors="coerce").astype("Int64")
    # drop exact duplicates
    out = out.drop_duplicates(subset=["country", "year"])
    return out

gdp_long      = prep(gdp_long,      ["country", "year", "gdp_per_capita"])
edu_long      = prep(edu_long,      ["country", "year", "gov_exp_edu"])
gov_eff_long  = prep(gov_eff_long,  ["country", "year", "gov_effectiveness"])
rd_exp_long   = prep(rd_exp_long,   ["country", "year", "rd_exp_gdp"])
merged_df_gov = prep(merged_df_gov, ["country", "year", "gov_effectiveness", "total_score"])

# If merged_df_gov also has gov_effectiveness, keep the score from there
# and prefer the indicator column from the dedicated gov_eff_long source.
merged_df_gov = merged_df_gov.rename(columns={"gov_effectiveness": "gov_effectiveness_from_score"})

# --- 1) Merge all indicators onto the score base (left join preserves your scoring rows) ---
dfs_to_merge = [
    merged_df_gov,
    gdp_long,
    edu_long,
    gov_eff_long,   # authoritative gov_effectiveness
    rd_exp_long
]

def left_merge(a, b):
    # prevent accidental suffix collisions
    cols_before = set(a.columns)
    out = a.merge(b, on=["country", "year"], how="left", validate="one_to_one")
    # sanity check for duplicate indicators
    dupes = [c for c in out.columns if c.endswith("_x") or c.endswith("_y")]
    if dupes:
        raise ValueError(f"Unexpected duplicate columns after merge: {dupes}")
    return out

full_df = reduce(left_merge, dfs_to_merge)

# Replace the possibly-missing 'gov_effectiveness' with the one from the score table if needed
if "gov_effectiveness" in full_df.columns:
    full_df["gov_effectiveness"] = full_df["gov_effectiveness"].fillna(full_df["gov_effectiveness_from_score"])
else:
    full_df["gov_effectiveness"] = full_df["gov_effectiveness_from_score"]

# Drop helper column
full_df = full_df.drop(columns=["gov_effectiveness_from_score"], errors="ignore")

# --- 2) Restrict to the modeling window 2017–2023 and ensure clean dtypes ---
full_df = full_df[(full_df["year"] >= 2017) & (full_df["year"] <= 2023)].copy()
full_df["year"] = full_df["year"].astype(int)

# --- 3) If there are still multiple rows per (country,year), aggregate safely ---
if full_df.duplicated(subset=["country", "year"]).any():
    agg_map = {
        "total_score": "sum",             # country-year total from top 300
        "gdp_per_capita": "mean",
        "gov_exp_edu": "mean",
        "gov_effectiveness": "mean",
        "rd_exp_gdp": "mean"
    }
    full_df = (
        full_df.groupby(["country", "year"], as_index=False)
               .agg(agg_map)
    )

# --- 4) Quick completeness report ---
needed_cols = ["total_score", "gdp_per_capita", "gov_exp_edu", "gov_effectiveness", "rd_exp_gdp"]
missing_summary = (
    full_df.assign(missing_any = full_df[needed_cols].isna().any(axis=1))
           .groupby("year")["missing_any"].value_counts(dropna=False)
           .unstack(fill_value=0)
           .rename(columns={True:"rows_with_NA", False:"rows_complete"})
)
print("Completeness by year (rows):\n", missing_summary)

# --- 5) Split train/test frames we’ll use for the three simple prediction approaches ---
train_2017_2022 = full_df[(full_df["year"] >= 2017) & (full_df["year"] <= 2022)].dropna(subset=needed_cols).copy()
test_2023        = full_df[(full_df["year"] == 2023)].dropna(subset=needed_cols).copy()

print("\nTrain rows:", len(train_2017_2022), " | Test 2023 rows:", len(test_2023))
print("\nColumns:", list(full_df.columns))
full_df.head()


predictor_cols = ["gdp_per_capita", "gov_exp_edu", "gov_effectiveness", "rd_exp_gdp"]

# --- Training table: up to 2022 with complete predictors and score ---
train_df = full_df[
    (full_df["year"] >= 2017) & 
    (full_df["year"] <= 2022)
].dropna(subset=["total_score"] + predictor_cols)

# --- Prediction table: only 2023 with predictors ---
pred_df = full_df[
    (full_df["year"] == 2023)
].drop(columns=["total_score"]).dropna(subset=predictor_cols)

print("Train shape:", train_df.shape)
print("Predictions shape:", pred_df.shape)

print("\nTraining table sample:")
print(train_df.head())

print("\nPrediction table sample:")
print(pred_df.head())


print("GDP last year:", gdp_long["year"].max())
print("Gov Exp Edu last year:", edu_long["year"].max())
print("Gov Effectiveness last year:", merged_df_gov["year"].max())


from functools import reduce
import pandas as pd

# --- Step 1: Prepare each predictors dataset ---
gdp_long      = gdp_long[["country", "year", "gdp_per_capita"]]
edu_long      = edu_long[["country", "year", "gov_exp_edu"]]
gov_eff_long  = gov_eff_long[["country", "year", "gov_effectiveness"]]
rd_exp_long   = rd_exp_long[["country", "year", "rd_exp_gdp"]]

# Target table: total_score
score_df = merged_df_gov[["country", "year", "total_score"]]

# --- Step 2: Merge predictors into one table ---
predictor_tables = [gdp_long, edu_long, gov_eff_long, rd_exp_long]
predictors_df = reduce(lambda left, right: pd.merge(left, right, on=["country", "year"], how="outer"), predictor_tables)

# --- Step 3: Build training table (2017–2022 with score) ---
train_df = pd.merge(
    predictors_df,
    score_df,
    on=["country", "year"],
    how="inner"   # only keep rows that have score
)
train_df = train_df[(train_df["year"] >= 2017) & (train_df["year"] <= 2022)]
train_df = train_df.dropna(subset=["total_score", "gdp_per_capita", "gov_exp_edu", "gov_effectiveness", "rd_exp_gdp"])

# --- Step 4: Build prediction table (2023 predictors only) ---
pred_df = predictors_df[(predictors_df["year"] == 2023)]
pred_df = pred_df.dropna(subset=["gdp_per_capita", "gov_exp_edu", "gov_effectiveness", "rd_exp_gdp"])

# --- Step 5: Check ---
print("Training table shape:", train_df.shape)
print("Prediction table shape:", pred_df.shape)
print("\nTraining table sample:")
print(train_df.head(10))
print("\nPrediction table sample:")
print(pred_df.head(10))


import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# --- 0) helpers: numeric coercion ---
def coerce_numeric(df, cols):
    for c in cols:
        if c in df.columns:
            df[c] = (
                df[c].astype(str)
                      .str.replace(r"[^\d\.\-eE]", "", regex=True)
            )
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

X_cols = ["gdp_per_capita", "gov_exp_edu", "gov_effectiveness", "rd_exp_gdp"]
req_train = ["total_score"] + X_cols

train_df = coerce_numeric(train_df.copy(), req_train).dropna(subset=req_train).reset_index(drop=True)
pred_df  = coerce_numeric(pred_df.copy(),  X_cols).dropna(subset=X_cols).reset_index(drop=True)

# --- 1) Top-10 countries by avg total_score (2017–2022) ---
top10_countries = (
    train_df.groupby("country")["total_score"]
            .mean()
            .sort_values(ascending=False)
            .head(10)
            .index
            .tolist()
)
print("Top 10 by avg total_score (2017–2022):", top10_countries)

# --- 2) Build a 2023 predictors table strictly for Top-10 countries ---
# Take any existing 2023 predictors for Top-10
pred_top10_2023 = pred_df[(pred_df["year"] == 2023) & (pred_df["country"].isin(top10_countries))].copy()

# Which Top-10 are missing 2023 predictors?
missing_top10 = sorted(set(top10_countries) - set(pred_top10_2023["country"].unique()))
print("Missing 2023 predictors for:", missing_top10)

# If some are missing, carry-forward their 2022 predictors
if missing_top10:
    # We need a predictors source for 2022; use train_df (it contains 2017–2022 with predictors+score)
    preds_2022 = (
        train_df[(train_df["year"] == 2022) & (train_df["country"].isin(missing_top10))]
        [["country"] + X_cols]
        .copy()
    )
    if not preds_2022.empty:
        preds_2022["year"] = 2023
        cf_2023 = preds_2022  # carry-forward
        pred_top10_2023 = pd.concat([pred_top10_2023, cf_2023], ignore_index=True)

# Final sanity: keep only Top-10 & 2023, drop duplicates
pred_top10_2023 = (
    pred_top10_2023[pred_top10_2023["country"].isin(top10_countries)]
    .copy()
)
pred_top10_2023["year"] = 2023
pred_top10_2023 = pred_top10_2023.drop_duplicates(subset=["country", "year"])

# Check we have samples now
if pred_top10_2023.empty:
    raise ValueError("Still no 2023 predictor rows for Top-10. Consider Option A (restrict Top-10 to countries present in 2023 predictors).")

# --- 3) Fit models on Top-10 training rows only ---
train_top10 = train_df[train_df["country"].isin(top10_countries)].reset_index(drop=True)

# Single-variable (R&D)
single_model = LinearRegression().fit(train_top10[["rd_exp_gdp"]], train_top10["total_score"])
pred_single = single_model.predict(pred_top10_2023[["rd_exp_gdp"]])

# Multiple (all predictors)
multi_model = LinearRegression().fit(train_top10[X_cols], train_top10["total_score"])
pred_multi = multi_model.predict(pred_top10_2023[X_cols])

# Correlation-weighted baseline (z-scored)
pearson = {
    "gdp_per_capita": 0.3032,
    "gov_exp_edu": 0.0973,
    "gov_effectiveness": 0.2974,
    "rd_exp_gdp": 0.3381
}
w = np.array([pearson[c] for c in X_cols], dtype=float)
w = w / w.sum()

mu = train_top10[X_cols].mean()
sd = train_top10[X_cols].std().replace(0, 1e-12)
Z_pred = (pred_top10_2023[X_cols] - mu) / sd
z = Z_pred.values.dot(w)

score_mu = train_top10["total_score"].mean()
score_sd = train_top10["total_score"].std() if train_top10["total_score"].std() > 0 else 1.0
w_norm = np.sqrt((w**2).sum())
pred_weighted = score_mu + (score_sd * (z / (w_norm if w_norm > 0 else 1.0)))

# --- 4) Output table ---
predictions_2023_top10 = pred_top10_2023[["country", "year"]].copy()
predictions_2023_top10["pred_single_RD"] = pred_single
predictions_2023_top10["pred_multi_all"] = pred_multi
predictions_2023_top10["pred_corr_weighted"] = pred_weighted

# Sort by multi (or any) for readability
predictions_2023_top10 = predictions_2023_top10.sort_values("pred_multi_all", ascending=False).reset_index(drop=True)
print(predictions_2023_top10)


from sklearn.linear_model import LinearRegression
import pandas as pd

# --- 1) Pick Top-10 countries by avg total_score (2017–2022) ---
top10_countries = (
    train_df.groupby("country")["total_score"]
            .mean()
            .sort_values(ascending=False)
            .head(10)
            .index
            .tolist()
)
print("Top 10 countries by avg total_score:", top10_countries)

# --- 2) Get 2023 GDP for Top-10, carry-forward 2022 GDP if missing ---
pred_top10_2023 = pred_df[(pred_df["year"] == 2023) & (pred_df["country"].isin(top10_countries))][["country", "year", "gdp_per_capita"]].copy()

missing_top10 = sorted(set(top10_countries) - set(pred_top10_2023["country"].unique()))
if missing_top10:
    gdp_2022 = (
        train_df[(train_df["year"] == 2022) & (train_df["country"].isin(missing_top10))]
        [["country", "gdp_per_capita"]]
        .copy()
    )
    if not gdp_2022.empty:
        gdp_2022["year"] = 2023
        pred_top10_2023 = pd.concat([pred_top10_2023, gdp_2022], ignore_index=True)

# --- 3) Filter training data for Top-10 only ---
train_top10 = train_df[train_df["country"].isin(top10_countries)].copy()

# --- 4) Fit linear regression using only GDP ---
X_train_gdp = train_top10[["gdp_per_capita"]]
y_train = train_top10["total_score"]

model_gdp = LinearRegression()
model_gdp.fit(X_train_gdp, y_train)

# --- 5) Predict 2023 total_score ---
pred_top10_2023["pred_total_score_gdp"] = model_gdp.predict(pred_top10_2023[["gdp_per_capita"]])

# --- 6) Sort predictions ---
pred_top10_2023 = pred_top10_2023.sort_values("pred_total_score_gdp", ascending=False).reset_index(drop=True)

# Show results
print("\nPredictions for 2023 (GDP only):")
print(pred_top10_2023)



import numpy as np
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX

def zscore_train(x):
    mu = float(np.nanmean(x))
    sd = float(np.nanstd(x, ddof=0))
    if sd == 0 or np.isnan(sd):
        sd = 1.0
    return (x - mu) / sd, mu, sd

def make_year_end_index(years):
    # Convert int years to year-end timestamps with explicit freq
    return pd.PeriodIndex(years.astype(str), freq="Y-DEC").to_timestamp()

pred_rows = []

# Use your df_gdp_exog with the OUTER merge from earlier
top10 = (df_gdp_exog.query("year <= 2022")
         .dropna(subset=["total_score"])
         .groupby("country")["total_score"].mean()
         .nlargest(10).index.tolist())

for c in top10:
    cd = df_gdp_exog[df_gdp_exog["country"] == c].copy()
    train = cd[(cd["year"] >= 2017) & (cd["year"] <= 2022)].dropna(subset=["total_score","gdp_per_capita"])
    fut  = cd[(cd["year"] == 2023)][["gdp_per_capita"]]

    if len(train) < 4 or fut.empty or fut.isna().any().any():
        pred_rows.append({"country": c, "forecast_2023": None, "lo80": None, "hi80": None, "order": "", "note": "too little data or missing 2023 GDP"})
        continue

    # Index with explicit yearly frequency
    idx = make_year_end_index(train["year"])
    y  = pd.Series(train["total_score"].to_numpy(float), index=idx, name="total_score")
    X  = pd.DataFrame({"gdp_per_capita": train["gdp_per_capita"].to_numpy(float)}, index=idx)

    # Standardize endog and exog (fit-time stats)
    y_z, y_mu, y_sd = zscore_train(y.values)
    X_z = X.copy()
    X_z["gdp_per_capita"], x_mu, x_sd = zscore_train(X["gdp_per_capita"].values)

    y_z = pd.Series(y_z, index=idx, name="y_z")
    X_z.index = idx

    # 2023 exog (apply SAME scaling as training)
    x2023_raw = float(fut["gdp_per_capita"].iloc[0])
    x2023_z   = (x2023_raw - x_mu) / (x_sd if x_sd != 0 else 1.0)
    X_2023_z  = pd.DataFrame({"gdp_per_capita":[x2023_z]},
                             index=make_year_end_index(pd.Series([2023], dtype=int)))

    # Try a few robust orders for tiny samples
    tried_orders = [(0,1,1), (1,0,0), (1,1,0), (0,1,0)]
    fitted = None
    last_err = ""
    for order in tried_orders:
        try:
            model = SARIMAX(
                y_z, exog=X_z, order=order, trend="c",  # include constant
                enforce_stationarity=False, enforce_invertibility=False
            )
            res = model.fit(disp=False, maxiter=1000, method="lbfgs")
            fitted = (order, res)
            break
        except Exception as e:
            last_err = str(e)
            continue

    if fitted is None:
        pred_rows.append({"country": c, "forecast_2023": None, "lo80": None, "hi80": None, "order": "", "note": f"fit failed: {last_err[:80]}..."})
        continue

    order_used, res = fitted

    # One-step forecast on standardized scale
    fc = res.get_forecast(steps=1, exog=X_2023_z)
    mean_z = float(fc.predicted_mean.iloc[0])
    ci_z   = fc.conf_int(alpha=0.2)  # 80% CI
    lo_z   = float(ci_z.iloc[0, 0])
    hi_z   = float(ci_z.iloc[0, 1])

    # Unscale back to original total_score units
    mean = mean_z * y_sd + y_mu
    lo80 = lo_z   * y_sd + y_mu
    hi80 = hi_z   * y_sd + y_mu

    pred_rows.append({
        "country": c,
        "forecast_2023": mean,
        "lo80": lo80,
        "hi80": hi80,
        "order": str(order_used),
        "note": ""
    })

pred_df = pd.DataFrame(pred_rows).sort_values("country").reset_index(drop=True)
print(pred_df)


import numpy as np
import pandas as pd
import statsmodels.formula.api as smf

# 0) Merge target + GDP
panel = (
    merged_df_gov[["country","year","total_score"]]
    .merge(gdp_long[["country","year","gdp_per_capita"]], on=["country","year"], how="left")
    .dropna(subset=["total_score","gdp_per_capita"])
    .copy()
)

# 1) Transform GDP (often helps linearity)
panel["log_gdp"] = np.log(panel["gdp_per_capita"])

# 2) Train on 2017–2022
train = panel.query("2017 <= year <= 2022").copy()

# 3) Fit Fixed-Effects OLS with time trend
fe_model = smf.ols("total_score ~ C(country) + year + log_gdp", data=train).fit(cov_type="cluster", cov_kwds={"groups": train["country"]})
print(fe_model.summary())

# 4) Prepare 2023 prediction rows for top-10 countries by 2017–2022 avg score
top10 = (train.groupby("country")["total_score"].mean().nlargest(10).index.tolist())

future_2023 = (
    panel.query("year == 2023 and country in @top10")[["country","year","gdp_per_capita"]]
    .drop_duplicates()
    .copy()
)

# If some GDP 2023 are missing, pull directly from gdp_long
if future_2023.empty or future_2023["gdp_per_capita"].isna().any():
    fallback = gdp_long.query("year == 2023 and country in @top10")[["country","year","gdp_per_capita"]]
    future_2023 = fallback.copy()

future_2023["log_gdp"] = np.log(future_2023["gdp_per_capita"])

# 5) Predict 2023
future_2023["forecast_2023"] = fe_model.predict(future_2023)

pred_fe = future_2023[["country","forecast_2023"]].sort_values("country").reset_index(drop=True)
print(pred_fe)


import numpy as np
import pandas as pd
import statsmodels.formula.api as smf

# 0) Merge target + GDP
panel = (
    merged_df_gov[["country","year","total_score"]]
    .merge(gdp_long[["country","year","gdp_per_capita"]], on=["country","year"], how="left")
    .copy()
)

# Hygiene
panel["country"] = panel["country"].astype(str).str.strip()
panel["year"] = panel["year"].astype(int)
panel["gdp_per_capita"] = pd.to_numeric(panel["gdp_per_capita"], errors="coerce")

# Use log GDP (often more linear)
panel["log_gdp"] = np.log(panel["gdp_per_capita"])

# 1) Train on 2017–2022
train = panel.query("2017 <= year <= 2022").dropna(subset=["total_score","log_gdp"]).copy()

# 2) Top-10 countries by avg total_score (2017–2022)
top10 = (train.groupby("country")["total_score"].mean()
               .nlargest(10).index.tolist())

# Restrict training to those countries only (optional, keeps focus consistent)
train_top10 = train[train["country"].isin(top10)].copy()

# 3) Fit fixed-effects OLS with a time trend
# total_score ~ country dummies + year + log_gdp
fe = smf.ols("total_score ~ C(country) + year + log_gdp", data=train_top10)\
        .fit(cov_type="cluster", cov_kwds={"groups": train_top10["country"]})
print(fe.summary())

# 4) Build 2023 prediction frame for the same top-10 countries (pull 2023 GDP)
future_2023 = (
    gdp_long.query("year == 2023 and country in @top10")
            [["country","year","gdp_per_capita"]]
            .drop_duplicates()
            .copy()
)
future_2023["log_gdp"] = np.log(pd.to_numeric(future_2023["gdp_per_capita"], errors="coerce"))

# In case any GDP is missing after coercion:
future_2023 = future_2023.dropna(subset=["log_gdp"])

# 5) Predict 2023
future_2023["forecast_2023"] = fe.predict(future_2023)

pred_lr = future_2023[["country","forecast_2023"]].sort_values("country").reset_index(drop=True)
print(pred_lr)


# Merge GDP
df = pd.merge(merged_df_gov, gdp_long, on=["country", "year"], how="left")

# Merge Education spending
df = pd.merge(df, edu_long, on=["country", "year"], how="left")


df.head(10)





















