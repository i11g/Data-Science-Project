import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA



































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


# Display basic information
df_rank.info()


df_rank.describe().T


df_rank.head()


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()
print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank} years")


print("Missing Values in Ranking Dataset:")
print(df_rank.isnull().sum())


# Convert rank_display to numeric 
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 universities for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# check missing by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


df_top300.to_csv("top300_universities.csv", index=False)


df_top300.shape





# Keep only the columns needed 
df_clean = df_top300[["university", "year", "score", "country"]].copy()


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

# Replace common variations
country_replacements = {
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",
    "UK": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean["country"] = df_clean["country"].replace(country_replacements)


df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


# Drop the missing values
df_clean = df_clean.dropna(subset=["score"])


# Convert to int and float
df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


df_clean.info()


# Step 1: total_score = sum of all university scores per country-year
total_score_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(total_score=("score", "sum"))
)

print(total_score_df.head())


import matplotlib.pyplot as plt

# Boxplot for total_score to visualize outliers
plt.figure(figsize=(8, 4))
plt.boxplot(total_score_df["total_score"], vert=False, patch_artist=True, boxprops=dict(facecolor="lightblue"))

plt.title("Boxplot of Total Score (All Years)")
plt.xlabel("Total Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.show()


# Step 2: num_universities = count of unique universities per country-year
num_univ_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(num_universities=("university", "nunique"))
)

print(num_univ_df.head())


# Boxplot for num_universities
plt.figure(figsize=(8, 4))
plt.boxplot(num_univ_df["num_universities"], vert=False, patch_artist=True, boxprops=dict(facecolor="lightgreen"))
plt.title("Boxplot of Number of Universities (All Years)")
plt.xlabel("Number of Universities")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()


# Merge total_score and num_universities
agg_df = total_score_df.merge(num_univ_df, on=["year", "country"], how="left")
print(agg_df.head())


# avg_score = total_score / num_universities
agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

print(agg_df.head())


# Boxplot for avg_score
plt.figure(figsize=(8, 4))
plt.boxplot(agg_df["avg_score"], vert=False, patch_artist=True, boxprops=dict(facecolor="lightcoral"))
plt.title("Boxplot of Average Score (All Years)")
plt.xlabel("Average Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()


# avg_score = total_score / num_universities
agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

print(agg_df.head())





# ---------- 1) Build the aggregated dataset from RAW df ----------
def build_agg_from_raw(df_clean: pd.DataFrame) -> pd.DataFrame:
    """
    Expects columns: ['university','year','score','country'].
    Returns agg_df with: ['year','country','total_score','num_universities','avg_score'].
    """
    need = {"university", "year", "score", "country"}
    missing = need - set(df_clean.columns)
    if missing:
        raise KeyError(f"Raw DF missing columns: {sorted(missing)}")

    df = df_clean.dropna(subset=["country", "year", "score", "university"]).copy()
    df["year"] = df["year"].astype(int)

    total_score_df = (
        df.groupby(["year", "country"], as_index=False)
          .agg(total_score=("score", "sum"))
    )
    num_univ_df = (
        df.groupby(["year", "country"], as_index=False)
          .agg(num_universities=("university", "nunique"))
    )

    agg_df = total_score_df.merge(num_univ_df, on=["year", "country"], how="left")
    agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]
    return agg_df

agg_df.to_csv("agg_university_metrics.csv", index=False)

# ---------- 2) Helper to get top-K per year ----------
def top_k_per_year(df_clean: pd.DataFrame, value_col: str, k: int = 10) -> pd.DataFrame:
    need = {"year", "country", value_col}
    if not need.issubset(df.columns):
        raise KeyError(f"DataFrame must contain: {sorted(need)}")
    return (
        df.sort_values(["year", value_col], ascending=[True, False], kind="mergesort")
          .groupby("year", group_keys=False)
          .head(k)
          .reset_index(drop=True)
    )

# ---------- 3) Plotters (one per metric) ----------
def plot_top10_total_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("total_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["total_score"])
        for i, v in enumerate(d["total_score"]):
            plt.text(v, i, f"{v:.1f}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Total Score ({y})")
        plt.xlabel("Total Score")
        plt.tight_layout()
        plt.show()


def plot_top10_avg_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("avg_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["avg_score"])
        # label with both avg and number of universities
        for i, (v, n) in enumerate(zip(d["avg_score"], d["num_universities"])):
            plt.text(v, i, f"{v:.1f}  |  {n} univ", va="center", ha="left")
        plt.title(f"Top {k} Countries by Average Score ({y})")
        plt.xlabel("Average Score per University")
        plt.tight_layout()
        plt.show()


def plot_top10_num_universities_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("num_universities", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["num_universities"])
        for i, v in enumerate(d["num_universities"]):
            plt.text(v, i, f"{v}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Number of Universities ({y})")
        plt.xlabel("Number of Universities")
        plt.tight_layout()
        plt.show()


# ---------- 4) Example usage ----------
# raw df name assumed to be `df` with cols: university, year, score, country
# agg_df = build_agg_from_raw(df)

# Tables (if you need them):
# top10_total = top_k_per_year(agg_df[["year","country","total_score"]], "total_score", k=10)
# top10_avg   = top_k_per_year(agg_df[["year","country","avg_score","num_universities"]], "avg_score", k=10)
# top10_count = top_k_per_year(agg_df[["year","country","num_universities"]], "num_universities", k=10)

# Plots:
# plot_top10_total_score_per_year(agg_df, k=10)
# plot_top10_avg_score_per_year(agg_df, k=10)
# plot_top10_num_universities_per_year(agg_df, k=10)


plot_top10_total_score_per_year(agg_df, k=10)
plot_top10_avg_score_per_year(agg_df, k=10)
plot_top10_num_universities_per_year(agg_df, k=10)














df_gdp = pd.read_csv("data/gdp.csv")


df_gdp.info()


df_gdp.head(5)


df_gdp = df_gdp.replace(r"^\s*\.\.\s*$", np.nan, regex=True)


df_gdp.isna().sum()


df_gdp.isna().mean() * 100


df_gdp[df_gdp.isna().any(axis=1)]


# First, replace ".." with NaN if you haven't already
import numpy as np
df_gdp = df_gdp.replace(r"^\s*\.\.\s*$", np.nan, regex=True)

# Convert numeric year columns to float
year_cols = [col for col in df_gdp.columns if "[YR" in col]
df_gdp[year_cols] = df_gdp[year_cols].astype(float)

# Interpolate row-wise across years
df_gdp[year_cols] = df_gdp[year_cols].interpolate(axis=1, limit_direction="both")


df_gdp.isna().sum()


df_gdp[df_gdp.isna().any(axis=1)]


# Keep only GDP per capita (current US$)
gdp_filtered = df_gdp[df_gdp["Series Name"] == "GDP per capita (current US$)"].copy()

# Drop unneeded columns
gdp_filtered = gdp_filtered.drop(columns=["Country Code", "Series Code", "Series Name"], errors="ignore")
gdp_filtered = gdp_filtered.loc[:, ~gdp_filtered.columns.str.contains("^Unnamed")]

# Reshape from wide to long format
gdp_long = gdp_filtered.melt(id_vars=["Country Name"], var_name="year", value_name="gdp_per_capita")

# Clean year strings like "2017 [YR2017]" -> "2017"
gdp_long["year"] = gdp_long["year"].str.extract(r"(\d{4})")

# Continue with steps
gdp_long = gdp_long.rename(columns={"Country Name": "country"})
gdp_long["year"] = pd.to_numeric(gdp_long["year"], errors="coerce")
gdp_long = gdp_long.dropna(subset=["year", "gdp_per_capita"])
gdp_long["year"] = gdp_long["year"].astype(int)

# Filter for 2017–2024
gdp_long = gdp_long[gdp_long["year"].between(2017, 2024)]

gdp_long.head(10)





top10_per_year = (
    gdp_long
    .sort_values(["year", "gdp_per_capita"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year)


# Show the top 10 GDP countries per year as a table
print("Top 10 GDP Countries per Year:")
print(top10_per_year)

# If you want to save to CSV
top10_per_year.to_csv("top10_gdp_per_year.csv", index=False)

import matplotlib.pyplot as plt

# Loop through each year and make a separate plot
for year, group in top10_per_year.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gdp_per_capita", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gdp_per_capita"], color="skyblue")
    plt.xlabel("GDP per Capita (US$)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by GDP per Capita in {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()


# Assuming your total_score data is in df_total_score with columns: country, year, total_score
top10_total_score = (
    total_score_df
    .sort_values(["year", "total_score"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True))


# Add an indicator column so we can identify source
top10_per_year["source"] = "GDP"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_per_year, top10_total_score], ignore_index=True)


for year in sorted(comparison_df["year"].unique()):
    gdp_countries = set(top10_per_year[top10_per_year["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    overlap = gdp_countries.intersection(score_countries)
    print(f"{year} → Overlap ({len(overlap)} countries): {overlap}")



import matplotlib.pyplot as plt

# Loop through each year
for year in sorted(top10_per_year["year"].unique()):
    # Get top 10 GDP and total_score countries for this year
    gdp_countries = set(top10_per_year[top10_per_year["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    overlap = gdp_countries & score_countries
    gdp_only = gdp_countries - score_countries
    score_only = score_countries - gdp_countries
    
    # Prepare data for plotting
    plot_data = []
    for country in overlap:
        plot_data.append((country, "Overlap"))
    for country in gdp_only:
        plot_data.append((country, "GDP Only"))
    for country in score_only:
        plot_data.append((country, "Total_Score Only"))
    
    # Convert to DataFrame
    df_plot = pd.DataFrame(plot_data, columns=["country", "category"])
    
    # Plot
    plt.figure(figsize=(8, 6))
    colors = {"Overlap": "green", "GDP Only": "skyblue", "Total_Score Only": "orange"}
    plt.barh(df_plot["country"], [1] * len(df_plot), color=df_plot["category"].map(colors))
    
    plt.title(f"Top 10 Overlap Between GDP and Total Score ({year})")
    plt.xlabel("Presence in Top 10")
    plt.ylabel("Country")
    plt.gca().invert_yaxis()
    legend_handles = [plt.Rectangle((0,0),1,1,color=colors[k]) for k in colors]
    plt.legend(legend_handles, colors.keys(), title="Category")
    plt.tight_layout()
    plt.show()






# Load dataset
edu_df = pd.read_csv("data/gov_exp_on_edu.csv")


edu_df.info()


edu_df.head(5)


edu_df = edu_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).infer_objects(copy=False)


edu_df.isna().sum()


# 1. Replace ".." placeholders with NaN
edu_df = edu_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
edu_df = edu_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in edu_df.columns if "[YR" in col]
edu_df[year_cols] = edu_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = edu_df[year_cols].isna().sum()
print(missing_summary)





edu_df[edu_df.isna().any(axis=1)]


# Step 2: Keep only education expenditure rows
edu_filtered = edu_df[
    edu_df["Series Name"] == "Government expenditure on education, total (% of GDP)"
].copy()

# Step 3: Drop unneeded columns
edu_filtered = edu_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
edu_filtered = edu_filtered.loc[:, ~edu_filtered.columns.str.contains("^Unnamed")]

# Step 4: Reshape from wide to long format
edu_long = edu_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_exp_edu"
)

# Extract just the year number from "2017 [YR2017]"
edu_long["year"] = edu_long["year"].str.extract(r"(\d{4})")

# Rename country column
edu_long = edu_long.rename(columns={"Country Name": "country"})

# Convert to numeric types
edu_long["year"] = pd.to_numeric(edu_long["year"], errors="coerce")
edu_long["gov_exp_edu"] = pd.to_numeric(edu_long["gov_exp_edu"], errors="coerce")

# Drop missing values
edu_long = edu_long.dropna(subset=["year", "gov_exp_edu"])

# Convert year to int
edu_long["year"] = edu_long["year"].astype(int)

# Filter only 2017–2024
edu_long = edu_long[edu_long["year"].between(2017, 2023)]

print(edu_long.dtypes)
print(edu_long.head(10))





# Load dataset
gov_df = pd.read_csv("data/Gov_effect_score .csv")


gov_df = gov_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).infer_objects(copy=False)


gov_df.isna().sum()


# 1. Replace ".." placeholders with NaN
gov_df = gov_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
gov_df = gov_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in gov_df.columns if "[YR" in col]
gov_df[year_cols] = gov_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = gov_df[year_cols].isna().sum()
print(missing_summary)


gov_df[gov_df.isna().any(axis=1)]


# Step 1: Keep only Government Effectiveness rows
gov_eff_filtered = gov_df[
    gov_df["Series Name"] == "Government Effectiveness: Percentile Rank"
].copy()

# Step 2: Drop unneeded columns
gov_eff_filtered = gov_eff_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
gov_eff_filtered = gov_eff_filtered.loc[:, ~gov_eff_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
gov_eff_long = gov_eff_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_effectiveness"
)

# Extract year number from "2017 [YR2017]"
gov_eff_long["year"] = gov_eff_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
gov_eff_long = gov_eff_long.rename(columns={"Country Name": "country"})

# Convert to numeric
gov_eff_long["year"] = pd.to_numeric(gov_eff_long["year"], errors="coerce")
gov_eff_long["gov_effectiveness"] = pd.to_numeric(gov_eff_long["gov_effectiveness"], errors="coerce")

# Drop missing values
gov_eff_long = gov_eff_long.dropna(subset=["year", "gov_effectiveness"])

# Ensure year is integer
gov_eff_long["year"] = gov_eff_long["year"].astype(int)

# Filter for years 2017–2024
gov_eff_long = gov_eff_long[gov_eff_long["year"].between(2017, 2023)]

print(gov_eff_long.dtypes)
print(gov_eff_long.head(10))





# Load dataset
res_df = pd.read_csv("data/res_dev.csv")  


res_df = res_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).infer_objects(copy=False)


res_df.isna().sum()


# 1. Replace ".." placeholders with NaN
res_df = res_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
res_df = res_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in res_df.columns if "[YR" in col]
res_df[year_cols] = res_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = res_df[year_cols].isna().sum()
print(missing_summary)


res_df.info()


# Step 1: Keep only R&D expenditure rows
rd_exp_filtered = res_df[
    res_df["Series Name"] == "Research and development expenditure (% of GDP)"
].copy()

# Step 2: Drop unneeded columns
rd_exp_filtered = rd_exp_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
rd_exp_filtered = rd_exp_filtered.loc[:, ~rd_exp_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
rd_exp_long = rd_exp_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="rd_exp_gdp"
)

# Extract year number from "2017 [YR2017]"
rd_exp_long["year"] = rd_exp_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
rd_exp_long = rd_exp_long.rename(columns={"Country Name": "country"})

# Convert to numeric
rd_exp_long["year"] = pd.to_numeric(rd_exp_long["year"], errors="coerce")
rd_exp_long["rd_exp_gdp"] = pd.to_numeric(rd_exp_long["rd_exp_gdp"], errors="coerce")

# Drop missing values
rd_exp_long = rd_exp_long.dropna(subset=["year", "rd_exp_gdp"])

# Ensure year is integer
rd_exp_long["year"] = rd_exp_long["year"].astype(int)

# Filter for years 2017–2023
rd_exp_long = rd_exp_long[rd_exp_long["year"].between(2017, 2023)]

print(rd_exp_long.dtypes)
print(rd_exp_long.head(10))


rd_exp_long.info()























# Merge GDP with ranking stats
merged_df = pd.merge(gdp_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df.head(10)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Clean numeric conversion
def to_numeric(series):
    return pd.to_numeric(series, errors="coerce")

# Ensure numeric
merged_df['total_score'] = to_numeric(merged_df['total_score'])
merged_df['gdp_per_capita'] = to_numeric(merged_df['gdp_per_capita'])

# Drop missing rows
corr_df = merged_df.dropna(subset=['total_score', 'gdp_per_capita'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gdp_per_capita'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gdp_per_capita'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gdp_per_capita'])

print("=== Correlation with GDP per Capita ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")





# Merge GDP with ranking stats
merged_df_edu = pd.merge(edu_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_edu.head(10)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Ensure numeric
merged_df_edu['total_score'] = pd.to_numeric(merged_df_edu['total_score'], errors='coerce')
merged_df_edu['gov_exp_edu'] = pd.to_numeric(merged_df_edu['gov_exp_edu'], errors='coerce')

# Drop missing rows
corr_df = merged_df_edu.dropna(subset=['total_score', 'gov_exp_edu'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_exp_edu'])

print("=== Correlation with Government Expenditure on Education ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")





# Merge Gov effectivness with ranking stats
merged_df_gov = pd.merge(gov_eff_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_gov.head(5)


import pandas as pd
from scipy.stats import pearsonr, spearmanr, kendalltau

# Ensure numeric
merged_df_gov['total_score'] = pd.to_numeric(merged_df_gov['total_score'], errors='coerce')
merged_df_gov['gov_effectiveness'] = pd.to_numeric(merged_df_gov['gov_effectiveness'], errors='coerce')

# Drop missing rows
corr_df = merged_df_gov.dropna(subset=['total_score', 'gov_effectiveness'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_effectiveness'])

print("=== Correlation with Government Effectiveness ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")





from scipy.stats import pearsonr, spearmanr, kendalltau
import pandas as pd

# Assuming:
# rd_exp_long → cleaned dataset with ['country', 'year', 'rd_exp_gdp']
# merged_df   → main dataset with ['country', 'year', 'total_score']

# --- Merge R&D data into main dataset ---
merged_rd = pd.merge(
    merged_df,
    rd_exp_long,
    on=["country", "year"],
    how="inner"
)

# Ensure numeric types
merged_rd['total_score'] = pd.to_numeric(merged_rd['total_score'], errors='coerce')
merged_rd['rd_exp_gdp'] = pd.to_numeric(merged_rd['rd_exp_gdp'], errors='coerce')

# Drop NaNs
corr_df = merged_rd.dropna(subset=['total_score', 'rd_exp_gdp'])

# --- Pearson ---
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Spearman ---
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Kendall ---
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Print ---
print("=== Correlation with R&D Expenditure (% of GDP) ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")








import pandas as pd
import statsmodels.api as sm

# Merge your datasets (ensure GDP and total_score match in country-year)
merged_df = total_score_df.merge(gdp_long, on=["country", "year"], how="inner")

# Independent and dependent variables
X = merged_df["gdp_per_capita"]
y = merged_df["total_score"]

# Add constant for intercept
X = sm.add_constant(X)

# Fit model
model = sm.OLS(y, X).fit()

# Summary
print(model.summary())








# Forecasting by moving average and linear forecast

# ---- 1) Time-index helpers ----
def yearly_series(group, value_col="total_score"):
    """Regular annual PeriodIndex series with freq=Y-DEC. Interpolates gaps."""
    g = group.sort_values("year")
    s = g.set_index(pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC"))[value_col]
    s = s.asfreq("Y-DEC")
    if s.isna().any():
        s = s.interpolate(limit_direction="both")
    return s
    
def future_index(s, steps):
    """Future yearly PeriodIndex aligned to s.frequency."""
    return pd.period_range(s.index[-1] + 1, periods=steps, freq=s.index.freq)

# ---- 2) Moving Average forecast ----
def moving_average_forecast(df, window=3, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        if len(s) == 0:
            # empty guard
            idx = pd.period_range(2000, periods=steps, freq="Y-DEC")
            results[c] = pd.Series([np.nan]*steps, index=idx)
            continue
        w = max(1, min(window, len(s)))
        ma_last = s.rolling(w).mean().iloc[-1]
        idx = future_index(s, steps)
        results[c] = pd.Series([ma_last]*steps, index=idx)
    return results

# ---- 3) Linear Trend Extrapolation (no sklearn; pure NumPy) ----
def linear_trend_forecast(df, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        g = g.sort_values("year")
        years = g["year"].astype(int).values
        y = g["total_score"].values

        if len(y) < 2:
            # fallback to last observed value if too short
            last = y[-1] if len(y) else np.nan
            last_year = years[-1] if len(years) else 2023
            fut_years = np.arange(last_year+1, last_year+1+steps)
            idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
            results[c] = pd.Series([last]*steps, index=idx)
            continue

        # center years for numerical stability
        base = years.min()
        x = (years - base).astype(float)
        # fit y = a*x + b
        a, b = np.polyfit(x, y, 1)

        last_year = years[-1]
        fut_years = np.arange(last_year+1, last_year+1+steps)
        x_future = (fut_years - base).astype(float)
        preds = a * x_future + b
        idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
        results[c] = pd.Series(preds, index=idx)
    return results

# ---- 4) Run both & build comparison table ----
STEPS = 2  # forecast next 2 years beyond each country's last observed year
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)

rows = []
for c in top10_countries:
    # Use linear trend index for naming (both methods share same future years count)
    idx = lin_preds[c].index
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]  = ma_preds[c].iloc[i]  if len(ma_preds[c])  > i else np.nan
        row[f"LIN_{y}"] = lin_preds[c].iloc[i] if len(lin_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)


comparison_df


# Forecasting using ARIMA

def arima_forecast(df, steps=2, order=(1,1,0)):
    """
    ARIMA forecast for total_score per country.
    order=(p,d,q):
        p - AR order
        d - differencing order
        q - MA order
    """
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)  # from your helper — gives annual PeriodIndex, interpolated

        if s.isna().all() or len(s) < (order[0] + order[2] + 1):
            # fallback: repeat last value
            last_val = s.iloc[-1] if len(s) else np.nan
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)
            continue

        try:
            # Fit ARIMA model
            model = ARIMA(s, order=order)
            fitted = model.fit()

            # Forecast
            forecast = fitted.forecast(steps=steps)
            idx = future_index(s, steps)
            results[c] = pd.Series(forecast.values, index=idx)
        except Exception as e:
            # Fallback on error
            last_val = s.iloc[-1]
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)
    return results


STEPS = 2
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)
arima_preds = arima_forecast(top10_df, steps=STEPS, order=(1,1,0))


rows = []
for c in top10_countries:
    idx = lin_preds[c].index  # consistent years
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]    = ma_preds[c].iloc[i]    if len(ma_preds[c]) > i else np.nan
        row[f"LIN_{y}"]   = lin_preds[c].iloc[i]   if len(lin_preds[c]) > i else np.nan
        row[f"ARIMA_{y}"] = arima_preds[c].iloc[i] if len(arima_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)


comparison_df








df_rank_2023 = pd.read_csv("data/2023 QS World University Rankings.csv")


df_rank_2023


df_rank_2024 = pd.read_csv("data/2024 QS World University Rankings 1.1 (For qs.com).csv")


df_rank_2024


df_rank_2023.shape





# Investigate columns from both data
columns1 = df_rank.columns # columns from data 2017-2022 
columns2 = df_rank_2023.columns # columns from data 2023 

print("columns from data 2017-2022:", columns1)
print("columns from data 2023 :", columns2)





# copy selected columns from Data 2023
df_temp2023 = df_rank_2023[['institution','Rank','score scaled','location']].copy()
# add year column
df_temp2023['year'] = 2023
# Check result
df_temp2023.info()


df_temp2023.columns = ['university','rank_display','score','country', 'year']
# Check result
df_temp2023.info()


# Convert rank_display to numeric 
df_temp2023['rank_display'] = pd.to_numeric(df_temp2023['rank_display'], errors='coerce')

# Filter top 300 for each year
df_top300_2023 = df_temp2023[df_temp2023['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300_2023['score'].isna().sum()
total_count = len(df_top300_2023)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")


# Start with a copy to avoid accidental index mismatch
df_clean_2023 = df_top300_2023.copy()

# Strip whitespace and title-case country names
df_clean_2023["country"] = df_clean_2023["country"].str.strip().str.title()

# Replace common variations
country_replacements = {
    "Usa": "United States",
    "U.s.a.": "United States",
    "United States Of America": "United States",
    "Uk": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean_2023["country"] = df_clean_2023["country"].replace(country_replacements)


def tidy_university_dataset(df_clean_2023: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregates by [year, country] and computes:
      - total_score (sum of 'score' across universities)
      - num_universities (unique 'university' count)
    Returns one row per (year, country).
    """
    agg_df = (
        df_clean_2023.groupby(["year", "country"], as_index=False)
                .agg(total_score=("score", "sum"),
                     num_universities=("university", "nunique"))
    )
    return agg_df

# Build aggregated dataset
agg_df_yr_country = tidy_university_dataset(df_clean_2023)

# Actuals for 2023 (one row per country)
actuals_2023 = (
    agg_df_yr_country.loc[agg_df_yr_country["year"] == 2023, ["country", "total_score"]]
    .rename(columns={"total_score": "actual_2023"})
)

# Keep only top-10 countries you forecasted for
actuals_2023 = actuals_2023[actuals_2023["country"].isin(top10_countries)].reset_index(drop=True)


actuals_2023


# One row per (year, country)
assert agg_df_yr_country.duplicated(["year", "country"]).sum() == 0

# All top10 countries appear in actuals (unless missing in 2023)
print(set(top10_countries) - set(actuals_2023["country"]))


comparison_df["country"] = comparison_df["country"].astype(str).str.strip()
actuals_2023["country"]  = actuals_2023["country"].astype(str).str.strip()


if "actual_2023" in comparison_df.columns:
    comparison_df = comparison_df.drop(columns=["actual_2023"])

comparison_df = comparison_df.merge(actuals_2023, on="country", how="left")


comparison_df


# Drop redundant columns
comparison_df = comparison_df.drop(columns=["actual_2023_x", "actual_2023_y"], errors="ignore")


comparison_df


for model in ["MA", "LIN", "ARIMA"]:
    comparison_df[f"{model}_abs_error_2023"] = (comparison_df[f"{model}_2023"] - comparison_df["actual_2023"]).abs()
    comparison_df[f"{model}_pct_error_2023"] = (
        (comparison_df[f"{model}_2023"] - comparison_df["actual_2023"]).abs() / comparison_df["actual_2023"] * 100
    )


comparison_df


avg_errors = comparison_df[
    [col for col in comparison_df.columns if "pct_error_2023" in col]
].mean().sort_values()

print(avg_errors)





from sklearn.linear_model import LinearRegression
import pandas as pd

# --- 1) Pick Top-10 countries by avg total_score (2017–2022) ---
top10_countries = (
    train_df.groupby("country")["total_score"]
            .mean()
            .sort_values(ascending=False)
            .head(10)
            .index
            .tolist()
)
print("Top 10 countries by avg total_score:", top10_countries)

# --- 2) Get 2023 GDP for Top-10, carry-forward 2022 GDP if missing ---
pred_top10_2023 = pred_df[(pred_df["year"] == 2023) & (pred_df["country"].isin(top10_countries))][["country", "year", "gdp_per_capita"]].copy()

missing_top10 = sorted(set(top10_countries) - set(pred_top10_2023["country"].unique()))
if missing_top10:
    gdp_2022 = (
        train_df[(train_df["year"] == 2022) & (train_df["country"].isin(missing_top10))]
        [["country", "gdp_per_capita"]]
        .copy()
    )
    if not gdp_2022.empty:
        gdp_2022["year"] = 2023
        pred_top10_2023 = pd.concat([pred_top10_2023, gdp_2022], ignore_index=True)

# --- 3) Filter training data for Top-10 only ---
train_top10 = train_df[train_df["country"].isin(top10_countries)].copy()

# --- 4) Fit linear regression using only GDP ---
X_train_gdp = train_top10[["gdp_per_capita"]]
y_train = train_top10["total_score"]

model_gdp = LinearRegression()
model_gdp.fit(X_train_gdp, y_train)

# --- 5) Predict 2023 total_score ---
pred_top10_2023["pred_total_score_gdp"] = model_gdp.predict(pred_top10_2023[["gdp_per_capita"]])

# --- 6) Sort predictions ---
pred_top10_2023 = pred_top10_2023.sort_values("pred_total_score_gdp", ascending=False).reset_index(drop=True)

# Show results
print("\nPredictions for 2023 (GDP only):")
print(pred_top10_2023)



















