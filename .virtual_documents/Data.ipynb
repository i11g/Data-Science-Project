import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re















































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


df_rank


# Display basic information
df_rank.info()


df_rank.describe().T


df_rank.head()


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()

print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank} years")


df_rank.dtypes


print("Missing Values in Ranking Dataset:")
print(df_rank.isnull().sum())


# Convert rank_display to numeric 
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# Optional: check missing by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


df_top300.to_csv("top300_universities.csv", index=False)


df_top300.iloc[763]


df_top300.shape





df_top300.info()


# Keep only the columns needed
df_clean = df_top300[["university", "year", "score", "country"]].copy()


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

# Replace common variations
country_replacements = {
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",
    "UK": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean["country"] = df_clean["country"].replace(country_replacements)


df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


df_clean = df_clean.dropna(subset=["score"])


df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


df_clean.info()


import pandas as pd

def tidy_university_dataset(df_clean: pd.DataFrame) -> pd.DataFrame:
    """
    Aggregates by [year, country] and computes:
      - total_score (sum of 'score')
      - num_universities (unique 'university' count)
    Then drops raw columns (if present) from the returned aggregated frame.
    """
    # Aggregate
    agg_df = (
        df_clean.groupby(["year", "country"], as_index=False)
                .agg(total_score=("score", "sum"),
                     num_universities=("university", "nunique"))
    )

    # Drop the originals if you no longer need them (no-op here, but kept for clarity)
    agg_df = agg_df.drop(columns=["university", "score"], errors="ignore")

    return agg_df


import matplotlib.pyplot as plt

def plot_totals_and_counts_per_country_per_year(
    agg_data: pd.DataFrame,
    top_n: int = 10,
    top_by: str = "total_score",   # or "num_universities"
    figsize_per_row: tuple = (10, 2.8)
):
    """
    Makes two figures stacked by years:
      1) Total score per country
      2) Number of universities per country
    Uses the same top-N selection per year based on `top_by`.
    """
    years = sorted(agg_data["year"].unique())
    n_years = len(years)

    def top_per_year(df_year, by_col):
        return df_year.sort_values(by_col, ascending=False).head(top_n)

    # -------- Figure 1: Total Score --------
    fig1, axes1 = plt.subplots(
        n_years, 1,
        figsize=(figsize_per_row[0], max(figsize_per_row[1] * n_years, 2.5)),
        squeeze=False
    )
    for i, yr in enumerate(years):
        sub = agg_data[agg_data["year"] == yr].copy()
        top_set = top_per_year(sub, top_by)["country"]
        sub = sub[sub["country"].isin(top_set)].sort_values("total_score", ascending=False)

        ax = axes1[i, 0]
        ax.bar(sub["country"], sub["total_score"])
        ax.set_title(f"Total Score by Country â€” {yr}")
        ax.set_ylabel("Total Score")

        # Rotate + align tick labels (no 'ha' inside tick_params!)
        ax.tick_params(axis='x', labelrotation=45)
        plt.setp(ax.get_xticklabels(), ha='right')

    fig1.tight_layout()

    # -------- Figure 2: University Count --------
    fig2, axes2 = plt.subplots(
        n_years, 1,
        figsize=(figsize_per_row[0], max(figsize_per_row[1] * n_years, 2.5)),
        squeeze=False
    )
    for i, yr in enumerate(years):
        sub = agg_data[agg_data["year"] == yr].copy()
        top_set = top_per_year(sub, top_by)["country"]
        sub = sub[sub["country"].isin(top_set)].sort_values("num_universities", ascending=False)

        ax = axes2[i, 0]
        ax.bar(sub["country"], sub["num_universities"])
        ax.set_title(f"University Count by Country â€” {yr}")
        ax.set_ylabel("# Universities")

        ax.tick_params(axis='x', labelrotation=45)
        plt.setp(ax.get_xticklabels(), ha='right')

    fig2.tight_layout()
    return fig1, fig2


agg_data = tidy_university_dataset(df_clean)
fig1, fig2 = plot_totals_and_counts_per_country_per_year(
    agg_data, top_n=10, top_by="total_score"
)
plt.show()


agg_data.info()





# Find top 10 countries by total_score
top10_countries = (
    agg_data.groupby("country")["total_score"]
    .mean()
    .nlargest(10)
    .index
)
# Filter dataset
top10_df = agg_data[agg_data["country"].isin(top10_countries)].copy()
top10_df = top10_df.sort_values(["country", "year"])


from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing  

# ---- 1) Time-index helpers ----
def yearly_series(group, value_col="total_score"):
    """Regular annual PeriodIndex series with freq=Y-DEC. Interpolates gaps."""
    g = group.sort_values("year")
    s = g.set_index(pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC"))[value_col]
    s = s.asfreq("Y-DEC")
    if s.isna().any():
        s = s.interpolate(limit_direction="both")
    return s

def future_index(s, steps):
    """Future yearly PeriodIndex aligned to s.frequency."""
    return pd.period_range(s.index[-1] + 1, periods=steps, freq=s.index.freq)

# ---- 2) Moving Average forecast ----
def moving_average_forecast(df, window=3, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        if len(s) == 0:
            # empty guard
            idx = pd.period_range(2000, periods=steps, freq="Y-DEC")
            results[c] = pd.Series([np.nan]*steps, index=idx)
            continue
        w = max(1, min(window, len(s)))
        ma_last = s.rolling(w).mean().iloc[-1]
        idx = future_index(s, steps)
        results[c] = pd.Series([ma_last]*steps, index=idx)
    return results

# ---- 3) Linear Trend Extrapolation (no sklearn; pure NumPy) ----
def linear_trend_forecast(df, steps=2):
    results = {}
    for c, g in df.groupby("country"):
        g = g.sort_values("year")
        years = g["year"].astype(int).values
        y = g["total_score"].values

        if len(y) < 2:
            # fallback to last observed value if too short
            last = y[-1] if len(y) else np.nan
            last_year = years[-1] if len(years) else 2023
            fut_years = np.arange(last_year+1, last_year+1+steps)
            idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
            results[c] = pd.Series([last]*steps, index=idx)
            continue

        # center years for numerical stability
        base = years.min()
        x = (years - base).astype(float)
        # fit y = a*x + b
        a, b = np.polyfit(x, y, 1)

        last_year = years[-1]
        fut_years = np.arange(last_year+1, last_year+1+steps)
        x_future = (fut_years - base).astype(float)
        preds = a * x_future + b
        idx = pd.PeriodIndex(fut_years, freq="Y-DEC")
        results[c] = pd.Series(preds, index=idx)
    return results

# ---- 4) Run both & build comparison table ----
STEPS = 2  # forecast next 2 years beyond each country's last observed year
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)

rows = []
for c in top10_countries:
    # Use linear trend index for naming (both methods share same future years count)
    idx = lin_preds[c].index
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]  = ma_preds[c].iloc[i]  if len(ma_preds[c])  > i else np.nan
        row[f"LIN_{y}"] = lin_preds[c].iloc[i] if len(lin_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)
comparison_df


from statsmodels.tsa.arima.model import ARIMA

def arima_forecast(df, steps=2, order=(1,1,0)):
    """
    ARIMA forecast for total_score per country.
    order=(p,d,q):
        p - AR order
        d - differencing order
        q - MA order
    """
    results = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)  # from your helper â€” gives annual PeriodIndex, interpolated

        if s.isna().all() or len(s) < (order[0] + order[2] + 1):
            # fallback: repeat last value
            last_val = s.iloc[-1] if len(s) else np.nan
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)
            continue

        try:
            # Fit ARIMA model
            model = ARIMA(s, order=order)
            fitted = model.fit()

            # Forecast
            forecast = fitted.forecast(steps=steps)
            idx = future_index(s, steps)
            results[c] = pd.Series(forecast.values, index=idx)
        except Exception as e:
            # Fallback on error
            last_val = s.iloc[-1]
            idx = future_index(s, steps)
            results[c] = pd.Series([last_val] * steps, index=idx)

    return results


STEPS = 2
ma_preds  = moving_average_forecast(top10_df, window=3, steps=STEPS)
lin_preds = linear_trend_forecast(top10_df, steps=STEPS)
arima_preds = arima_forecast(top10_df, steps=STEPS, order=(1,1,0))


rows = []
for c in top10_countries:
    idx = lin_preds[c].index  # consistent years
    row = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        row[f"MA_{y}"]    = ma_preds[c].iloc[i]    if len(ma_preds[c]) > i else np.nan
        row[f"LIN_{y}"]   = lin_preds[c].iloc[i]   if len(lin_preds[c]) > i else np.nan
        row[f"ARIMA_{y}"] = arima_preds[c].iloc[i] if len(arima_preds[c]) > i else np.nan
    rows.append(row)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)
comparison_df








from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing

# Moving Average
def moving_average_forecast(df, window=3):
    results = {}
    for country, group in df.groupby("country"):
        ma = group["total_score"].rolling(window).mean().iloc[-1]
        preds = [ma] * 2  # same prediction for next 2 years
        results[country] = preds
    return results

# Simple Exponential Smoothing
def exponential_smoothing_forecast(df, forecast_periods=2):
    results = {}
    for country, group in df.groupby("country"):
        model = SimpleExpSmoothing(group["total_score"]).fit(smoothing_level=0.2, optimized=False)
        preds = model.forecast(forecast_periods)
        results[country] = preds.values
    return results

# Holt-Winters (trend only for yearly data)
def holt_winters_forecast(df, forecast_periods=2):
    results = {}
    for country, group in df.groupby("country"):
        model = ExponentialSmoothing(group["total_score"], trend='add', seasonal=None).fit()
        preds = model.forecast(forecast_periods)
        results[country] = preds.values
    return results


# ========= 1) Time-index helpers (Y-DEC) =========
def yearly_series(group, value_col="total_score"):
    """Return a regular annual PeriodIndex series with freq=Y-DEC."""
    g = group.sort_values("year")
    s = g.set_index(pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC"))[value_col]
    s = s.asfreq("Y-DEC")  # ensure regularity; introduces NaN if years missing
    # If there are internal NaNs (missing years), simple forward/back fill
    if s.isna().any():
        s = s.interpolate(limit_direction="both")
    return s

def future_index(s, steps):
    """Future yearly PeriodIndex aligned to the series freq."""
    return pd.period_range(s.index[-1] + 1, periods=steps, freq=s.index.freq)

# ========= 2) Models =========
def moving_average_forecast(df, window=3, steps=2):
    out = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        if len(s) == 0:
            out[c] = pd.Series([np.nan]*steps, index=pd.period_range(2000, periods=steps, freq="Y-DEC"))
            continue
        w = min(window, max(1, len(s)))
        ma_last = s.rolling(w).mean().iloc[-1] if len(s) >= 1 else np.nan
        idx = future_index(s, steps)
        out[c] = pd.Series([ma_last]*steps, index=idx)
    return out

def ses_forecast(df, steps=2, alpha=None):
    out = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        idx = future_index(s, steps) if len(s) else pd.period_range(2000, periods=steps, freq="Y-DEC")
        if len(s) < 2:
            last = s.iloc[-1] if len(s) else np.nan
            out[c] = pd.Series([last]*steps, index=idx)
            continue
        model = SimpleExpSmoothing(s, initialization_method="estimated")
        fit = model.fit(smoothing_level=alpha, optimized=(alpha is None))
        out[c] = pd.Series(fit.forecast(steps), index=idx)
    return out

def holt_winters_forecast(df, steps=2, trend='add'):
    out = {}
    for c, g in df.groupby("country"):
        s = yearly_series(g)
        idx = future_index(s, steps) if len(s) else pd.period_range(2000, periods=steps, freq="Y-DEC")
        if len(s) < 3:
            # Fallback to SES for very short series
            if len(s) == 0:
                out[c] = pd.Series([np.nan]*steps, index=idx)
                continue
            fit = SimpleExpSmoothing(s, initialization_method="estimated").fit()
            out[c] = pd.Series(fit.forecast(steps), index=idx)
            continue
        fit = ExponentialSmoothing(s, trend=trend, seasonal=None, initialization_method="estimated").fit()
        out[c] = pd.Series(fit.forecast(steps), index=idx)
    return out

# ========= 3) Run forecasts (next 2 years beyond each countryâ€™s last year) =========
STEPS = 2  # e.g., forecast 2024 & 2025 if last year is 2023
ma_preds = moving_average_forecast(top10_df, window=3, steps=STEPS)
es_preds = ses_forecast(top10_df, steps=STEPS)                  # Simple Exponential Smoothing
hw_preds = holt_winters_forecast(top10_df, steps=STEPS)         # Holt-Winters (trend only)

# ========= 4) Comparison table =========
rows = []
for c in top10_countries:
    # Use HW index to name columns (all share same future years for that country)
    idx = hw_preds[c].index
    entry = {"country": c}
    for i in range(STEPS):
        y = idx[i].year
        entry[f"MA_{y}"]  = ma_preds[c].iloc[i] if len(ma_preds[c]) > i else np.nan
        entry[f"SES_{y}"] = es_preds[c].iloc[i] if len(es_preds[c]) > i else np.nan
        entry[f"HW_{y}"]  = hw_preds[c].iloc[i] if len(hw_preds[c]) > i else np.nan
    rows.append(entry)

comparison_df = pd.DataFrame(rows).sort_values("country").reset_index(drop=True)
comparison_df


def plot_country(country):
    g = top10_df[top10_df["country"] == country].sort_values("year")
    hist_idx = pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC")

    plt.figure(figsize=(9,5))
    plt.plot(hist_idx.to_timestamp(), g["total_score"], marker="o", label="Historical")
    for name, pred in [("Moving Avg", ma_preds[country]), ("SES", es_preds[country]), 
                       ("Holt-Winters", hw_preds[country])]:
        plt.plot(pred.index.to_timestamp(), pred.values, marker="o", label=name)
    plt.title(f"Total Score Forecast â€” {country}")
    plt.xlabel("Year"); plt.ylabel("Total Score"); plt.legend(); plt.tight_layout()
    plt.show()

# Example:
# plot_country(top10_countries[0])











df_rank_2023 = pd.read_csv("data/2023 QS World University Rankings.csv")


df_rank_2023


df_rank_2024 = pd.read_csv("data/2024 QS World University Rankings 1.1 (For qs.com).csv")


df_rank_2024








df_gdp = pd.read_csv("data/gdp.csv")


#gdp_df = pd.read_csv("GDP.csv", skiprows=4)

# Preview the structure of the cleaned GDP dataframe
#gdp_df.head()


df_gdp


df_gdp.info()


df_gdp.describe()


df_gdp.head(5)


df_gdp.dtypes





# Step 2: Keep only GDP per capita (current US$)
gdp_filtered = df_gdp[df_gdp["Series Name"] == "GDP per capita (current US$)"].copy()

# Step 3: Drop unneeded columns
gdp_filtered = gdp_filtered.drop(columns=["Country Code", "Series Code", "Series Name"], errors="ignore")
gdp_filtered = gdp_filtered.loc[:, ~gdp_filtered.columns.str.contains("^Unnamed")]

# Step 4: Reshape from wide to long format
gdp_long = gdp_filtered.melt(id_vars=["Country Name"], var_name="year", value_name="gdp_per_capita")

# Clean year strings like "2017 [YR2017]" -> "2017"
gdp_long["year"] = gdp_long["year"].str.extract(r"(\d{4})")

# Continue with steps
gdp_long = gdp_long.rename(columns={"Country Name": "country"})
gdp_long["year"] = pd.to_numeric(gdp_long["year"], errors="coerce")
gdp_long = gdp_long.dropna(subset=["year", "gdp_per_capita"])
gdp_long["year"] = gdp_long["year"].astype(int)

# Filter for 2017â€“2024
gdp_long = gdp_long[gdp_long["year"].between(2017, 2024)]

gdp_long.head(20)








# Merge GDP with ranking stats
merged_df = pd.merge(gdp_long, agg_data, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df.head(20)


merged_df.info()


# Filter for the country
country = "United States"
country_df = merged_df[merged_df["country"] == country]

# Create figure and first axis (GDP)
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot GDP on left y-axis
ax1.set_title(f"GDP vs Rankings - {country}")
ax1.set_xlabel("Year")
ax1.set_ylabel("GDP per Capita (USD)", color="tab:blue")
ax1.plot(country_df["year"], country_df["gdp_per_capita"], label="GDP per Capita", color="tab:blue", linewidth=2)
ax1.tick_params(axis="y", labelcolor="tab:blue")

# Create second y-axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot average score and university count on right y-axis
ax2.set_ylabel("Score / University Count", color="tab:red")
ax2.plot(country_df["year"], country_df["total_score"], label="Total Score", color="tab:red", linestyle="--", linewidth=2)
ax2.plot(country_df["year"], country_df["num_universities"], label="# of Universities", color="tab:green", 
         linestyle=":", linewidth=2)
ax2.tick_params(axis="y", labelcolor="tab:red")

# Combine legends from both axes
lines_1, labels_1 = ax1.get_legend_handles_labels()
lines_2, labels_2 = ax2.get_legend_handles_labels()
ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc="upper left")

# Grid and layout
ax1.grid(True)
plt.tight_layout()
plt.show()


# Compute correlation matrix
correlation = merged_df[["gdp_per_capita", "total_score", "num_universities"]].corr()

# Display correlation table
print(correlation)





# Load your dataset
edu_df = pd.read_csv("world-education-data.csv")  

# Keep only the relevant columns
spending_df = edu_df[["country", "year", "gov_exp_pct_gdp"]].copy()

# Drop missing values (optional: you can choose to keep them too)
spending_df = spending_df.dropna(subset=["gov_exp_pct_gdp"])

# Convert year to integer
spending_df["year"] = spending_df["year"].astype(int)

# Optional: Filter for years of interest
spending_df = spending_df[spending_df["year"].between(2017, 2024)]

# View results
spending_df.head(20)


merged = pd.merge(merged_df, spending_df, on=["country", "year"], how="left")


merged[["gov_exp_pct_gdp", "score", "gdp_per_capita"]].corr()


print(merged_df.columns.tolist())


edu_df = pd.read_csv("world-education-data.csv")  

# Keep only the relevant columns
edu_features_df = edu_df[[
    "country", "year",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]].copy()



merged_df = pd.merge(merged_df, edu_features_df, on=["country", "year"], how="left")


# Select only the relevant columns
columns_of_interest = [
    "score",
    "gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Filter the dataset
edu_corr = merged_df[columns_of_interest].dropna()

# Compute correlation
corr_matrix = edu_corr.corr()
print(corr_matrix)

# Display as heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix: Education Indicators vs Score")
plt.tight_layout()
plt.show()


from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import pandas as pd

# Step 1: Define the features and target
features = [
    "gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Drop missing values
model_data = merged_df.dropna(subset=features + ["score"])

# X = predictors, y = target
X = model_data[features]
y = model_data["score"]


# Fit the model
reg = LinearRegression()
reg.fit(X, y)

# Predict
y_pred = reg.predict(X)

# R-squared
r2 = r2_score(y, y_pred)

# Coefficients
print("ðŸ“Š Multiple Linear Regression Results:")
for feature, coef in zip(features, reg.coef_):
    print(f"{feature}: {coef:.4f}")
    
print(f"\nIntercept: {reg.intercept_:.2f}")
print(f"RÂ² (explained variance): {r2:.4f}")


import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
plt.scatter(y, y_pred, alpha=0.6)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel("Actual Score")
plt.ylabel("Predicted Score")
plt.title("Actual vs Predicted University Score")
plt.grid(True)
plt.tight_layout()
plt.show()



import numpy as np

# Add log-transformed GDP column
merged_df["log_gdp_per_capita"] = np.log(merged_df["gdp_per_capita"])

# Optional: confirm no -inf or NaN values
merged_df = merged_df.replace([np.inf, -np.inf], np.nan)


from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Define new feature list
log_features = [
    "log_gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Drop missing
log_model_data = merged_df.dropna(subset=log_features + ["score"])
X_log = log_model_data[log_features]
y_log = log_model_data["score"]

# Fit the model
log_model = LinearRegression()
log_model.fit(X_log, y_log)
y_log_pred = log_model.predict(X_log)

# Print results
print("ðŸ“Š Multiple Regression with log(GDP):")
for f, coef in zip(log_features, log_model.coef_):
    print(f"{f}: {coef:.4f}")

print(f"\nIntercept: {log_model.intercept_:.2f}")
print(f"RÂ² (explained variance): {r2_score(y_log, y_log_pred):.4f}")


from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Step 1: Get GDP per capita time series for United States (2017â€“2023)
country = "United States"
gdp_series = merged_df[(merged_df["country"] == country) & (merged_df["year"] <= 2023)]
gdp_series = gdp_series.set_index("year")["gdp_per_capita"]

# Step 2: Fit ARIMA model (try (1,1,1) as a default)
model = ARIMA(gdp_series, order=(1, 1, 1))
model_fit = model.fit()

# Step 3: Forecast GDP for 2024
gdp_forecast_2024 = model_fit.forecast(steps=1)
forecasted_gdp = gdp_forecast_2024.iloc[0]

print(f"ðŸ“ˆ Forecasted GDP per capita for {country} in 2024: ${forecasted_gdp:,.2f}")


features = ['log_gdp_per_capita', 'gov_exp_pct_gdp', 'lit_rate_adult_pct',
            'school_enrol_primary_pct', 'school_enrol_secondary_pct', 'school_enrol_tertiary_pct']


merged_df[(merged_df["country"] == "United States") & (merged_df["year"] == 2024)]



import numpy as np
import pandas as pd

# Step 1: Define the features used in your log-GDP regression model
features = [
    "log_gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Step 2: Create a minimal input row for 2024
# Replace these values with averages or reasonable estimates
input_row_2024 = pd.DataFrame([{
    "log_gdp_per_capita": np.log(forecasted_gdp),
    "gov_exp_pct_gdp": 5.0,                  # average: ~5% of GDP
    "lit_rate_adult_pct": 99.0,              # high literacy for US
    "school_enrol_primary_pct": 102.0,       # slightly over 100% (common due to repeaters)
    "school_enrol_secondary_pct": 95.0,
    "school_enrol_tertiary_pct": 70.0
}])

# Step 3: Predict score
predicted_score_2024 = log_model.predict(input_row_2024)[0]
print(f"ðŸŽ“ Predicted university score for United States in 2024 (GDP-based): {predicted_score_2024:.2f}")



# Find top 10 countries by average total_score
top10_countries = (
    merged_df.groupby("country")["total_score"]
    .mean()
    .nlargest(10)
    .index
)

# Filter dataset
top10_df = merged_df[merged_df["country"].isin(top10_countries)].copy()
top10_df = top10_df.sort_values(["country", "year"])


from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing

# Moving Average
def moving_average_forecast(df, window=3):
    results = {}
    for country, group in df.groupby("country"):
        ma = group["total_score"].rolling(window).mean().iloc[-1]
        preds = [ma] * 2  # same prediction for next 2 years
        results[country] = preds
    return results

# Simple Exponential Smoothing
def exponential_smoothing_forecast(df, forecast_periods=2):
    results = {}
    for country, group in df.groupby("country"):
        model = SimpleExpSmoothing(group["total_score"]).fit(smoothing_level=0.2, optimized=False)
        preds = model.forecast(forecast_periods)
        results[country] = preds.values
    return results

# Holt-Winters (trend only for yearly data)
def holt_winters_forecast(df, forecast_periods=2):
    results = {}
    for country, group in df.groupby("country"):
        model = ExponentialSmoothing(group["total_score"], trend='add', seasonal=None).fit()
        preds = model.forecast(forecast_periods)
        results[country] = preds.values
    return results









def plot_country(country):
    g = top10_df[top10_df["country"] == country].sort_values("year")
    hist_idx = pd.PeriodIndex(g["year"].astype(int), freq="Y-DEC")

    plt.figure(figsize=(9,5))
    plt.plot(hist_idx.to_timestamp(), g["total_score"], marker="o", label="Historical")
    for name, pred in [("Moving Avg", ma_preds[country]), ("SES", es_preds[country]), ("Holt-Winters", hw_preds[country])]:
        plt.plot(pred.index.to_timestamp(), pred.values, marker="o", label=name)
    plt.title(f"Total Score Forecast â€” {country}")
    plt.xlabel("Year"); plt.ylabel("Total Score"); plt.legend(); plt.tight_layout()
    plt.show()

# Example:
# plot_country(top10_countries[1])


plot_country(top10_countries[1])


comparison = []

for country in top10_countries:
    comparison.append({
        "country": country,
        "linear_2023": linear_preds[country][0],
        "linear_2024": linear_preds[country][1],
        "ma_2023": ma_preds[country][0],
        "ma_2024": ma_preds[country][1],
        "es_2034": es_preds[country][0],
        "es_2024": es_preds[country][1],
        "hw_2023": hw_preds[country][0],
        "hw_2024": hw_preds[country][1],
    })

comparison_df = pd.DataFrame(comparison)
print(comparison_df)



























