import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from scipy.stats import pearsonr, spearmanr, kendalltau
import seaborn as sns
pd.set_option('future.no_silent_downcasting', True)
























































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


# Display basic information
## df_rank.info()


# df_rank.describe().T


df_rank.dtypes


df_rank.head(5)


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()
print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank}years")


print(df_rank.isnull().sum())


# Convert rank_display to numeric 
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 universities for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# check missing score by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


# save the new dataset in tables folder
df_top300.to_csv("tables/top300_universities.csv", index=False)


# check what is the shape of the table
df_top300.shape





# Keep only the columns needed 
df_clean = df_top300[["university", "year", "score", "country"]].copy()


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

# Replace common variations
country_replacements = {
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",
    "UK": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean["country"] = df_clean["country"].replace(country_replacements)


# Drop duplicates
df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


# Since the missing values are 0.39 % we will drop them
df_clean = df_clean.dropna(subset=["score"])


# For sanity convert year and score to int and float 
df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


df_clean.info()


# Make the total_score_df, in which total_score = sum of all university scores per country-year
total_score_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(total_score=("score", "sum"))
)

print(total_score_df.head())


# Boxplot for total_score to visualize outliers
plt.figure(figsize=(8, 4))
plt.boxplot(total_score_df["total_score"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightblue"))

plt.title("Boxplot of Total Score (All Years)")
plt.xlabel("Total Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/qs_total_score_trends.png", dpi=300, bbox_inches='tight')
plt.show()


# Make num_univ_df in which num_universities = count of unique universities per country-year
num_univ_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(num_universities=("university", "nunique"))
)

print(num_univ_df.head())


# Boxplot for num_universities
plt.figure(figsize=(8, 4))
plt.boxplot(num_univ_df["num_universities"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightgreen"))
plt.title("Boxplot of Number of Universities (All Years)")
plt.xlabel("Number of Universities")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/num_universities_trends.png", dpi=300, bbox_inches='tight')
plt.show()


# Merge total_score_df and num_universities_df to new agg_df
agg_df = total_score_df.merge(num_univ_df, on=["year", "country"], how="left")

# Make avg_score_df in which avg_score = total_score / num_universities
agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

print(agg_df.head())


# Boxplot for avg_score
plt.figure(figsize=(8, 4))
plt.boxplot(agg_df["avg_score"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightcoral"))
plt.title("Boxplot of Average Score (All Years)")
plt.xlabel("Average Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/avearge_scores_trends.png", dpi=300, bbox_inches='tight')
plt.show()








# ---------- Build aggregated dataset ----------
def build_agg_from_raw(df_clean: pd.DataFrame) -> pd.DataFrame:
    """
    Expects columns: ['university','year','score','country'].
    Returns agg_df with: ['year','country','total_score','num_universities','avg_score'].
    """
    required_cols = {"university", "year", "score", "country"}
    missing = required_cols - set(df_clean.columns)
    if missing:
        raise KeyError(f"Missing columns: {sorted(missing)}")

    # Drop rows with NaNs in essential columns
    df = df_clean.dropna(subset=["country", "year", "score", "university"]).copy()
    df["year"] = df["year"].astype(int)

    # One groupby-agg for both metrics
    agg_df = (
        df.groupby(["year", "country"], as_index=False)
          .agg(
              total_score=("score", "sum"),
              num_universities=("university", "nunique")
          )
    )

    # Average score per university
    agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

    return agg_df

agg_df.to_csv("tables/agg_university_metrics.csv", index=False)

# ---------- 2) Helper to get top-K per year ----------
def top_k_per_year(df_clean: pd.DataFrame, value_col: str, k: int = 10) -> pd.DataFrame:
    need = {"year", "country", value_col}
    if not need.issubset(df.columns):
        raise KeyError(f"DataFrame must contain: {sorted(need)}")
    return (
        df.sort_values(["year", value_col], ascending=[True, False], kind="mergesort")
          .groupby("year", group_keys=False)
          .head(k)
          .reset_index(drop=True)
    )

# ---------- 3) Plotters (one per metric) ----------
def plot_top10_total_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("total_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["total_score"])
        for i, v in enumerate(d["total_score"]):
            plt.text(v, i, f"{v:.1f}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Total Score ({y})")
        plt.xlabel("Total Score")
        plt.tight_layout()
        plt.show()


def plot_top10_avg_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("avg_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["avg_score"])
        # label with both avg and number of universities
        for i, (v, n) in enumerate(zip(d["avg_score"], d["num_universities"])):
            plt.text(v, i, f"{v:.1f}  |  {n} univ", va="center", ha="left")
        plt.title(f"Top {k} Countries by Average Score ({y})")
        plt.xlabel("Average Score per University")
        plt.tight_layout()
        plt.show()


def plot_top10_num_universities_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("num_universities", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["num_universities"])
        for i, v in enumerate(d["num_universities"]):
            plt.text(v, i, f"{v}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Number of Universities ({y})")
        plt.xlabel("Number of Universities")
        plt.tight_layout()
        plt.show()


# ---------- 4) Example usage ----------
# raw df name assumed to be `df` with cols: university, year, score, country
# agg_df = build_agg_from_raw(df)

# Tables (if you need them):
# top10_total = top_k_per_year(agg_df[["year","country","total_score"]], "total_score", k=10)
# top10_avg   = top_k_per_year(agg_df[["year","country","avg_score","num_universities"]], 
        #"avg_score", k=10)
# top10_count = top_k_per_year(agg_df[["year","country","num_universities"]], 
        #"num_universities", k=10)

# Plots:
# plot_top10_total_score_per_year(agg_df, k=10)
# plot_top10_avg_score_per_year(agg_df, k=10)
# plot_top10_num_universities_per_year(agg_df, k=10)


plot_top10_total_score_per_year(agg_df, k=10)
plot_top10_avg_score_per_year(agg_df, k=10)
plot_top10_num_universities_per_year(agg_df, k=10)


def plot_score_trends(agg_df: pd.DataFrame, metric: str = "total_score", top_n: int = 5):
    """
    Plots time-series trends for the top N countries based on the last available year.
    
    Parameters
    ----------
    agg_df : pd.DataFrame
        Must contain ['year','country', metric].
    metric : str
        One of ['total_score', 'avg_score', 'num_universities'].
    top_n : int
        Number of countries to plot (selected from the last year).
    """
    if metric not in agg_df.columns:
        raise KeyError(f"Metric '{metric}' not found in DataFrame.")

    # --- Determine top N countries based on last year ---
    last_year = agg_df["year"].max()
    top_countries = (
        agg_df[agg_df["year"] == last_year]
        .nlargest(top_n, metric)["country"]
        .tolist()
    )

    # --- Filter for those countries only ---
    plot_df = agg_df[agg_df["country"].isin(top_countries)].copy()

    # --- Plot ---
    plt.figure(figsize=(10, 6))
    sns.lineplot(data=plot_df, x="year", y=metric, hue="country", marker="o")

    plt.title(f"{metric.replace('_', ' ').title()} Trends for Top {top_n} Countries")
    plt.xlabel("Year")
    plt.ylabel(metric.replace("_", " ").title())
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend(title="Country", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()

# Example usage:
# plot_score_trends(agg_df, metric="total_score", top_n=5)


plot_score_trends(agg_df, metric="total_score", top_n=10)
# plot_score_trends(agg_df, metric="avg_score", top_n=5)











# Load datasets
df_gdp = pd.read_csv("data/gdp.csv")
edu_df = pd.read_csv("data/gov_exp_on_edu.csv")
gov_df = pd.read_csv("data/gov_effect_score .csv")
res_df = pd.read_csv("data/res_dev.csv")


# Get basic information
df_gdp.info()
edu_df.info()
gov_df.info()
res_df.info()


import re
import numpy as np
import pandas as pd

def check_missing_values(df: pd.DataFrame, dataset_name: str = "Dataset",
                         year_min: int = 2017, year_max: int = 2022):
    """
    Cleans '..' placeholders to NaN, checks missing values ONLY for years in [year_min, year_max],
    and returns cleaned DataFrame + summary.
    """
    df_clean = (
        df.copy()
          .replace(r"^\s*\.\.\s*$", np.nan, regex=True)
          .infer_objects(copy=False)
    )

    # Detect year columns
    def _extract_year(col):
        m = re.match(r"^\s*((19|20)\d{2})", str(col))
        return int(m.group(1)) if m else None

    year_cols = [c for c in df_clean.columns
                 if (y := _extract_year(c)) is not None and year_min <= y <= year_max]

    if not year_cols:
        print(f"[{dataset_name}] No year columns found in {year_min}–{year_max} range.")
        return df_clean, pd.DataFrame()

    # Missing summary only for relevant years
    missing_summary = df_clean[year_cols].isna().sum().to_frame(name="missing_count")
    missing_summary["missing_pct"] = (missing_summary["missing_count"] / len(df_clean)) * 100

    print(f"=== Missing Values Summary ({year_min}–{year_max}): {dataset_name} ===")
    print(missing_summary)

    return df_clean, missing_summary


gdp_clean, gdp_missing = check_missing_values(df_gdp, "GDP per Capita")


edu_exp_clean, edu_exp_missing = check_missing_values(edu_df, "Gov. Expenditure on Education")


rd_exp_clean, rd_exp_missing = check_missing_values(res_df, "R&D Expenditure")


gov_clean, gov_missing = check_missing_values(gov_df, "Gov Effectivness")



YEAR_COL_RE = re.compile(r"^\s*(19|20)\d{2}\s*\[YR(19|20)\d{2}\]\s*$")

def _extract_year(col: str) -> int | None:
    m = re.match(r"^\s*((19|20)\d{2})", str(col))
    return int(m.group(1)) if m else None

def impute_economic_timeseries_axis1(
    df: pd.DataFrame,
    dataset_name: str = "Dataset",
    year_min: int | None = None,
    year_max: int | None = None,
    use_year_median: bool = True,
    show_debug: bool = False,
):
    """
    World-Bank-style wide table imputation (columns like '2017 [YR2017]').

    Steps (only within the specified window of years):
      1) Per-row linear interpolation across years (axis=1)
      2) Then per-row forward/backward fill (axis=1)
      3) (optional) Fill remaining gaps with per-year global median

    Returns
    -------
    df_filled : pd.DataFrame
    fill_report : pd.DataFrame  (counts per step)
    """

    # 0) Copy & clean placeholders (and silence the future downcasting warning)
    df_clean = (
        df.copy()
          .replace(r"^\s*\.\.\s*$", np.nan, regex=True)
          .infer_objects(copy=False)
    )

    # 1) Detect year columns and sort them by actual year
    all_year_cols = []
    for c in df_clean.columns:
        y = _extract_year(str(c))
        if y is not None:
            all_year_cols.append((c, y))

    if not all_year_cols:
        # No year columns detected → return as-is
        report = pd.DataFrame({
            "dataset": [dataset_name],
            "metric": ["no_year_columns_detected"],
            "count": [0]
        })
        return df_clean, report

    # sort by numeric year
    all_year_cols.sort(key=lambda t: t[1])
    year_cols = [c for c, _ in all_year_cols]
    years = [y for _, y in all_year_cols]

    # 2) Restrict to window (e.g., 2017–2022)
    if (year_min is not None) or (year_max is not None):
        mask = [(year_min is None or y >= year_min) and (year_max is None or y <= year_max) 
                for y in years]
        window_cols = [c for c, keep in zip(year_cols, mask) if keep]
    else:
        window_cols = year_cols

    if show_debug:
        print(f"[{dataset_name}] Detected year columns:", year_cols[:5], 
              "... (total:", len(year_cols), ")")
        print(f"[{dataset_name}] Window cols ({year_min}-{year_max}):", window_cols)

    # If window has no columns, nothing to impute: return original + small report
    if len(window_cols) == 0:
        report = pd.DataFrame({
            "dataset": [dataset_name],
            "metric": ["empty_window_no_imputation"],
            "count": [0]
        })
        return df_clean, report

    # Ensure numeric for year columns
    for c in year_cols:
        df_clean[c] = pd.to_numeric(df_clean[c], errors="coerce")

    # Track baseline NA counts (only inside window)
    baseline_na = df_clean[window_cols].isna().sum().sum()

    # 3) Step 1: per-row linear interpolation across the window
    before = df_clean[window_cols].isna()
    df_step1 = df_clean.copy()
    df_step1[window_cols] = df_step1[window_cols].interpolate(axis=1, limit_direction="both")
    after = df_step1[window_cols].isna()
    filled_interp = (before.values & ~after.values).sum()

    # 4) Step 2: per-row ffill/bfill across the window
    before = df_step1[window_cols].isna()
    df_step2 = df_step1.copy()
    df_step2[window_cols] = df_step2[window_cols].ffill(axis=1).bfill(axis=1)
    after = df_step2[window_cols].isna()
    filled_ffill = (before.values & ~after.values).sum()

    df_final = df_step2

    # 5) Step 3: per-year global median (optional)
    filled_median = 0
    if use_year_median:
        # compute medians from current table per column
        col_medians = df_final[window_cols].median(axis=0, skipna=True)
        # fill remaining NaNs with same-column median
        before = df_final[window_cols].isna()
        df_final[window_cols] = df_final[window_cols].apply(
            lambda s: s.fillna(col_medians[s.name])
        )
        after = df_final[window_cols].isna()
        filled_median = (before.values & ~after.values).sum()

    remaining_missing = int(df_final[window_cols].isna().sum().sum())

    # 6) Build a concise report
    report = pd.DataFrame({
        "dataset": [dataset_name]*5,
        "metric": ["baseline_missing", "filled_by_interpolation", "filled_by_ffill_bfill",
                   "filled_by_year_median", "remaining_missing_after_all"],
        "count": [int(baseline_na), int(filled_interp), int(filled_ffill), 
                  int(filled_median), int(remaining_missing)]
    })

    return df_final, report


gdp_filled, gdp_report = impute_economic_timeseries_axis1(
    df_gdp, "GDP per Capita", year_min=2017, year_max=2022
)

edu_exp_filled, edu_exp_report = impute_economic_timeseries_axis1(
    edu_df, "Gov. Expenditure on Education", year_min=2017, year_max=2022
)

rd_filled, rd_report = impute_economic_timeseries_axis1(
    res_df, "R&D Expenditure", year_min=2017, year_max=2022
)

gov_eff_filled, gov_eff_report = impute_economic_timeseries_axis1(
    gov_df, "Gov Effectiveness", year_min=2017, year_max=2022
)
print(gdp_report)


edu_exp_filled, edu_exp_report = impute_economic_timeseries_axis1(
    edu_df, "Gov. Expenditure on Education", year_min=2017, year_max=2022
)
print(edu_exp_report)





rd_filled, rd_report = impute_economic_timeseries_axis1(
    res_df, "R&D Expenditure", year_min=2017, year_max=2022
)
print(rd_report)


gov_eff_filled, gov_eff_report = impute_economic_timeseries_axis1(
    gov_df, "Gov Effectiveness", year_min=2017, year_max=2022
)
print(gov_eff_report)





# First, replace ".." with NaN if you haven't already
import numpy as np
df_gdp = df_gdp.replace(r"^\s*\.\.\s*$", np.nan, regex=True)

# Convert numeric year columns to float
year_cols = [col for col in df_gdp.columns if "[YR" in col]
df_gdp[year_cols] = df_gdp[year_cols].astype(float)

# Interpolate row-wise across years
df_gdp[year_cols] = df_gdp[year_cols].interpolate(axis=1, limit_direction="both")


df_gdp.isna().sum()


df_gdp[df_gdp.isna().any(axis=1)]


# Keep only GDP per capita (current US$)
gdp_filtered = df_gdp[df_gdp["Series Name"] == "GDP per capita (current US$)"].copy()

# Drop unneeded columns
gdp_filtered = gdp_filtered.drop(columns=["Country Code", "Series Code", "Series Name"], errors="ignore")
gdp_filtered = gdp_filtered.loc[:, ~gdp_filtered.columns.str.contains("^Unnamed")]

# Reshape from wide to long format
gdp_long = gdp_filtered.melt(id_vars=["Country Name"], var_name="year", value_name="gdp_per_capita")

# Clean year strings like "2017 [YR2017]" -> "2017"
gdp_long["year"] = gdp_long["year"].str.extract(r"(\d{4})")

# Continue with steps
gdp_long = gdp_long.rename(columns={"Country Name": "country"})
gdp_long["year"] = pd.to_numeric(gdp_long["year"], errors="coerce")
gdp_long = gdp_long.dropna(subset=["year", "gdp_per_capita"])
gdp_long["year"] = gdp_long["year"].astype(int)

# Filter for 2017–2024
gdp_long = gdp_long[gdp_long["year"].between(2017, 2024)]

gdp_long.head(10)





top10_per_year_gdp = (
    gdp_long
    .sort_values(["year", "gdp_per_capita"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_gdp)


# If you want to save to CSV
top10_per_year_gdp.to_csv("top10_gdp_per_year.csv", index=False)

# Loop through each year and make a separate plot
for year, group in top10_per_year_gdp.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gdp_per_capita", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gdp_per_capita"], color="skyblue")
    plt.xlabel("GDP per Capita (US$)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by GDP per Capita in {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()





# Load dataset
edu_df = pd.read_csv("data/gov_exp_on_edu.csv")


edu_df.info()


edu_df.head(5)


edu_df = (
    edu_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True)
           .infer_objects(copy=False)
)


edu_df.isna().sum()


# 1. Replace ".." placeholders with NaN
edu_df = edu_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
edu_df = edu_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in edu_df.columns if "[YR" in col]
edu_df[year_cols] = edu_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = edu_df[year_cols].isna().sum()
print(missing_summary)





edu_df[edu_df.isna().any(axis=1)]


# Step 2: Keep only education expenditure rows
edu_filtered = edu_df[
    edu_df["Series Name"] == "Government expenditure on education, total (% of GDP)"
].copy()

# Step 3: Drop unneeded columns
edu_filtered = edu_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
edu_filtered = edu_filtered.loc[:, ~edu_filtered.columns.str.contains("^Unnamed")]

# Step 4: Reshape from wide to long format
edu_long = edu_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_exp_edu"
)

# Extract just the year number from "2017 [YR2017]"
edu_long["year"] = edu_long["year"].str.extract(r"(\d{4})")

# Rename country column
edu_long = edu_long.rename(columns={"Country Name": "country"})

# Convert to numeric types
edu_long["year"] = pd.to_numeric(edu_long["year"], errors="coerce")
edu_long["gov_exp_edu"] = pd.to_numeric(edu_long["gov_exp_edu"], errors="coerce")

# Drop missing values
edu_long = edu_long.dropna(subset=["year", "gov_exp_edu"])

# Convert year to int
edu_long["year"] = edu_long["year"].astype(int)

# Filter only 2017–2024
edu_long = edu_long[edu_long["year"].between(2017, 2022)]

print(edu_long.dtypes)
print(edu_long.head(10))


top10_per_year_edu = (
    edu_long
    .sort_values(["year", "gov_exp_edu"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_edu)


# Save to CSV
top10_per_year_edu.to_csv("top10_edu_per_year.csv", index=False)

# Loop through each year and make a separate plot
for year, group in top10_per_year_edu.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gov_exp_edu", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gov_exp_edu"], color="skyblue")
    plt.xlabel("gov_exp_edu(US$)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by GDP per Capita in {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()





# Assuming your total_score data is in df_total_score with columns: country, year, total_score
top10_total_score = (
    total_score_df
    .sort_values(["year", "total_score"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True))


# Add an indicator column so we can identify source
top10_per_year_edu["source"] = "gov_edu"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_per_year_edu, top10_total_score], ignore_index=True)


for year in sorted(comparison_df["year"].unique()):
    edu_countries = set(top10_per_year_edu[top10_per_year_edu["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    overlap = edu_countries.intersection(score_countries)
    print(f"{year} → Overlap ({len(overlap)} countries): {overlap}")





# Load dataset
gov_df = pd.read_csv("data/Gov_effect_score .csv")


gov_df = gov_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).infer_objects(copy=False)


gov_df.isna().sum()


# 1. Replace ".." placeholders with NaN
gov_df = gov_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
gov_df = gov_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in gov_df.columns if "[YR" in col]
gov_df[year_cols] = gov_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = gov_df[year_cols].isna().sum()
print(missing_summary)


gov_df[gov_df.isna().any(axis=1)]


# Step 1: Keep only Government Effectiveness rows
gov_eff_filtered = gov_df[
    gov_df["Series Name"] == "Government Effectiveness: Percentile Rank"
].copy()

# Step 2: Drop unneeded columns
gov_eff_filtered = gov_eff_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
gov_eff_filtered = gov_eff_filtered.loc[:, ~gov_eff_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
gov_eff_long = gov_eff_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="gov_effectiveness"
)

# Extract year number from "2017 [YR2017]"
gov_eff_long["year"] = gov_eff_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
gov_eff_long = gov_eff_long.rename(columns={"Country Name": "country"})

# Convert to numeric
gov_eff_long["year"] = pd.to_numeric(gov_eff_long["year"], errors="coerce")
gov_eff_long["gov_effectiveness"] = pd.to_numeric(gov_eff_long["gov_effectiveness"], errors="coerce")

# Drop missing values
gov_eff_long = gov_eff_long.dropna(subset=["year", "gov_effectiveness"])

# Ensure year is integer
gov_eff_long["year"] = gov_eff_long["year"].astype(int)

# Filter for years 2017–2024
gov_eff_long = gov_eff_long[gov_eff_long["year"].between(2017, 2023)]

print(gov_eff_long.dtypes)
print(gov_eff_long.head(10))


top10_per_year_gov_eff = (
    gov_eff_long
    .sort_values(["year", "gov_effectiveness"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_gov_eff)


# Add an indicator column so we can identify source
top10_per_year_gov_eff["source"] = "gov_effectiveness"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_per_year_gov_eff, top10_total_score], ignore_index=True)





# Load dataset
res_df = pd.read_csv("data/res_dev.csv")  


res_df = res_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).infer_objects(copy=False)


res_df.isna().sum()


# 1. Replace ".." placeholders with NaN
res_df = res_df.replace(r"^\s*\.\.\s*$", np.nan, regex=True).convert_dtypes()

# 2. Drop the years with excessive missing values
cols_to_drop = ["1990 [YR1990]", "2024 [YR2024]"]
res_df = res_df.drop(columns=cols_to_drop, errors="ignore")

# 3. Interpolate missing values for the remaining year columns
year_cols = [col for col in res_df.columns if "[YR" in col]
res_df[year_cols] = res_df[year_cols].astype(float).interpolate(
    axis=1, method="linear", limit_direction="both"
)
# 4. (Optional) Check remaining missing values
missing_summary = res_df[year_cols].isna().sum()
print(missing_summary)


res_df.info()


# Step 1: Keep only R&D expenditure rows
rd_exp_filtered = res_df[
    res_df["Series Name"] == "Research and development expenditure (% of GDP)"
].copy()

# Step 2: Drop unneeded columns
rd_exp_filtered = rd_exp_filtered.drop(
    columns=["Country Code", "Series Code", "Series Name"], errors="ignore"
)

# Remove unnamed columns if any
rd_exp_filtered = rd_exp_filtered.loc[:, ~rd_exp_filtered.columns.str.contains("^Unnamed")]

# Step 3: Reshape from wide to long format
rd_exp_long = rd_exp_filtered.melt(
    id_vars=["Country Name"], var_name="year", value_name="rd_exp_gdp"
)

# Extract year number from "2017 [YR2017]"
rd_exp_long["year"] = rd_exp_long["year"].str.extract(r"(\d{4})")

# Rename for consistency
rd_exp_long = rd_exp_long.rename(columns={"Country Name": "country"})

# Convert to numeric
rd_exp_long["year"] = pd.to_numeric(rd_exp_long["year"], errors="coerce")
rd_exp_long["rd_exp_gdp"] = pd.to_numeric(rd_exp_long["rd_exp_gdp"], errors="coerce")

# Drop missing values
rd_exp_long = rd_exp_long.dropna(subset=["year", "rd_exp_gdp"])

# Ensure year is integer
rd_exp_long["year"] = rd_exp_long["year"].astype(int)

# Filter for years 2017–2023
rd_exp_long = rd_exp_long[rd_exp_long["year"].between(2017, 2023)]

print(rd_exp_long.dtypes)
print(rd_exp_long.head(10))


rd_exp_long.info()


top10_per_year_res = (
    rd_exp_long
    .sort_values(["year", "rd_exp_gdp"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_res)


# Add an indicator column so we can identify source
top10_per_year_res["source"] = "rd_exp"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_per_year_res, top10_total_score], ignore_index=True)





from typing import Literal, Dict, Any
import pandas as pd

def compute_yearly_overlaps(
    top10_indicator: pd.DataFrame,
    top10_total_score: pd.DataFrame,
    indicator_name: str,
    *,
    year_col: str = "year",
    country_col: str = "country",
    baseline: Literal["score", "indicator", "union"] = "score",
    verbose: bool = True,
) -> tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Compute year-by-year overlaps between a Top-10 indicator list and the Top-10 Total Score list.

    Parameters
    ----------
    top10_indicator : DataFrame
        Must contain columns [year_col, country_col] for the indicator Top-10 per year.
    top10_total_score : DataFrame
        Must contain columns [year_col, country_col] for the Total Score Top-10 per year.
    indicator_name : str
        A label for the indicator (e.g., "GDP", "Primary", "Gov_Effectiveness", "R&D").
    year_col : str
        Name of the year column (default "year").
    country_col : str
        Name of the country column (default "country").
    baseline : {"score", "indicator", "union"}
        What to use in the denominator for the overlap percentage:
        - "score": size of Total Score list (default; matches your previous snippets)
        - "indicator": size of the indicator list
        - "union": size of the union of both lists
    verbose : bool
        If True, prints a per-year line and totals.

    Returns
    -------
    results_df : DataFrame
        Columns: [year, indicator, overlap_count, baseline_count, overlap_pct, overlap_countries]
    totals : dict
        {
          "indicator": str,
          "total_overlap_sum": int,
          "total_baseline_sum": int,
          "total_pct_sum": float,
          "unique_overlap_countries": set,
          "unique_union_countries": set,
          "unique_overlap_pct": float
        }
    """

    # Ensure required columns exist
    for df, name in [(top10_indicator, "top10_indicator"), (top10_total_score, "top10_total_score")]:
        missing = {year_col, country_col} - set(df.columns)
        if missing:
            raise ValueError(f"{name} is missing columns: {missing}")

    # Gather all years present in either dataframe
    years = sorted(set(top10_indicator[year_col].unique()).union(set(top10_total_score[year_col].unique())))

    # Accumulators
    rows = []
    total_overlap_sum = 0
    total_baseline_sum = 0
    all_overlap_countries = set()
    all_union_countries = set()

    for year in years:
        ind_countries = set(top10_indicator.loc[top10_indicator[year_col] == year, country_col])
        score_countries = set(top10_total_score.loc[top10_total_score[year_col] == year, country_col])

        overlap = ind_countries & score_countries

        if baseline == "score":
            denom = len(score_countries)
        elif baseline == "indicator":
            denom = len(ind_countries)
        elif baseline == "union":
            denom = len(ind_countries | score_countries)
        else:
            raise ValueError("baseline must be one of {'score','indicator','union'}")

        overlap_pct = (len(overlap) / denom * 100) if denom else 0.0

        # Track totals (sum-based uses same denominator choice as your previous code: size of score list)
        total_overlap_sum += len(overlap)
        total_baseline_sum += len(score_countries)  # keep consistent with your earlier snippets

        # Unique (set-based) accumulators
        all_overlap_countries |= overlap
        all_union_countries |= (ind_countries | score_countries)

        rows.append({
            "year": year,
            "indicator": indicator_name,
            "overlap_count": len(overlap),
            "baseline_count": denom,
            "overlap_pct": round(overlap_pct, 1),
            "overlap_countries": sorted(overlap),
        })

        if verbose:
            print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")

    # Totals
    total_pct_sum = (total_overlap_sum / total_baseline_sum * 100) if total_baseline_sum else 0.0
    unique_overlap_pct = (len(all_overlap_countries) / len(all_union_countries) * 100) if all_union_countries else 0.0

    if verbose:
        print("\n=== TOTAL (sum-based) ===")
        print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")
        print("=== UNIQUE (set-based across period) ===")
        print(f"Unique overlap countries: {len(all_overlap_countries)} / {len(all_union_countries)} "
              f"({unique_overlap_pct:.1f}%)")

    results_df = pd.DataFrame(rows)
    totals = {
        "indicator": indicator_name,
        "total_overlap_sum": total_overlap_sum,
        "total_baseline_sum": total_baseline_sum,
        "total_pct_sum": round(total_pct_sum, 1),
        "unique_overlap_countries": all_overlap_countries,
        "unique_union_countries": all_union_countries,
        "unique_overlap_pct": round(unique_overlap_pct, 1),
    }
    return results_df, totals


gdp_df, gdp_totals = compute_yearly_overlaps(top10_per_year_gdp, top10_total_score, 
                                             "GDP", baseline="score")
edu_df, edu_totals = compute_yearly_overlaps(top10_per_year_edu, top10_total_score, 
                                             "Education", baseline="score")
gov_df, gov_totals = compute_yearly_overlaps(top10_per_year_gov_eff, top10_total_score, 
                                             "Gov_Effectiveness", baseline="score")
res_df, res_totals = compute_yearly_overlaps(top10_per_year_res, top10_total_score, 
                                             "R&D", baseline="score")














edu_world_df = pd.read_csv("data/world-education-data.csv")


edu_world_df.dtypes


focus_cols = [
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# 1. Overview of data types & non-null counts
print("=== Data Info ===")
edu_world_df[["country", "year"] + focus_cols].info()

# 2. Basic descriptive statistics
print("\n=== Descriptive Stats ===")
print(edu_world_df[focus_cols].describe())

# 3. Missing values count & percentage
print("\n=== Missing Values ===")
missing_summary = edu_world_df[focus_cols].isna().sum().to_frame("missing_count")
missing_summary["missing_pct"] = (missing_summary["missing_count"] / len(edu_world_df)) * 100
print(missing_summary)

# 4. Check duplicates
duplicate_rows = edu_world_df.duplicated(subset=["country", "year"])
print(f"\nNumber of duplicate country-year rows: {duplicate_rows.sum()}")

# 5. Sample rows
print("\n=== Sample Data ===")
print(edu_world_df[["country", "year"] + focus_cols].sample(5))





focus_cols = [
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct",
]

# 0) Ensure numeric (coerce weird strings like '..' to NaN)
for c in focus_cols:
    edu_world_df[c] = pd.to_numeric(edu_world_df[c], errors="coerce")

# 1) Sort so interpolation makes sense over time
edu_world_df = edu_world_df.sort_values(["country", "year"])

# 2) Interpolate within each country (keeps index aligned)
edu_world_df[focus_cols] = (
    edu_world_df
      .groupby("country")[focus_cols]
      .transform(lambda g: g.interpolate(method="linear"))
)

# 3) Optional: fill any edges that interpolation can’t fill
edu_world_df[focus_cols] = (
    edu_world_df
      .groupby("country")[focus_cols]
      .transform(lambda g: g.ffill().bfill())
)

# 4) Quick check
print(edu_world_df[focus_cols].isna().sum())


# Missing counts
missing_counts = edu_world_df[focus_cols].isna().sum()

# Missing percentages
missing_pct = (missing_counts / len(edu_world_df)) * 100

# Combine into a nice DataFrame
missing_summary = pd.DataFrame({
    "missing_count": missing_counts,
    "missing_pct": missing_pct.round(2)  # Round to 2 decimal places
})

print(missing_summary)


# Duplicates by country-year
dups = edu_world_df.duplicated(["country", "year"]).sum()
print("Duplicate country-year rows:", dups)


# 2) Standardize country names (example)
edu_world_df["country"] = edu_world_df["country"].str.strip()
total_score_df["country"] = total_score_df["country"].str.strip()

# 3) Ensure numeric data for enrollment
for col in ["school_enrol_primary_pct", "school_enrol_secondary_pct", "school_enrol_tertiary_pct"]:
    edu_world_df[col] = pd.to_numeric(edu_world_df[col], errors="coerce")

# 4) Merge datasets on country & year
merged_edu_ind = pd.merge(
    total_score_df, 
    edu_world_df, 
    on=["country", "year"], 
    how="inner"
)

# Drop rows with missing values for correlation
merged_edu_ind = merged_edu_ind.dropna(subset=["total_score", "school_enrol_primary_pct",
                                               "school_enrol_secondary_pct", "school_enrol_tertiary_pct"])

print(merged_edu_ind.head())


keep_cols = [
    "country",
    "year",
    "total_score",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

merged_edu_ind_subset = merged_edu_ind[keep_cols].copy()

print(merged_edu_ind_subset.head())





# --- Top 10 Primary ---
top10_primary = (
    merged_edu_ind_subset
    .sort_values(["year", "school_enrol_primary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True))


print(top10_primary.head(10))





# --- Top 10 Secondary ---
top10_secondary = (
   merged_edu_ind_subset
    .sort_values(["year", "school_enrol_secondary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)


print(top10_secondary.head(10))





# --- Top 10 Tertiary ---
top10_tertiary = (
    merged_edu_ind_subset
    .sort_values(["year", "school_enrol_tertiary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)


print(top10_tertiary.head(10))





# We perform the overlap analysis for the three educational indicators and total_score

prim_df, prim_totals = compute_yearly_overlaps(top10_primary,
                                               top10_total_score, "Primary",  baseline="score")
sec_df,  sec_totals  = compute_yearly_overlaps(top10_secondary,
                                               top10_total_score, "Secondary", baseline="score")
ter_df,  ter_totals  = compute_yearly_overlaps(top10_tertiary,
                                               top10_total_score, "Tertiary",  baseline="score")








# --- Overlap data ---
data = {
    "Year": [2017, 2018, 2019, 2020, 2021, 2022],
    "Primary":   [10.0, 10.0, 10.0, 20.0, 10.0, 10.0],
    "Secondary": [30.0, 30.0, 20.0, 20.0, 20.0, 20.0],
    "Tertiary":  [30.0, 30.0, 30.0, 30.0, 20.0, 20.0]
}

df = pd.DataFrame(data)

# --- Plot ---
plt.figure(figsize=(10, 6))
bar_width = 0.25
x = range(len(df["Year"]))

plt.bar([p - bar_width for p in x], df["Primary"], width=bar_width, label="Primary")
plt.bar(x, df["Secondary"], width=bar_width, label="Secondary")
plt.bar([p + bar_width for p in x], df["Tertiary"], width=bar_width, label="Tertiary")

# --- Styling ---
plt.xticks(x, df["Year"])
plt.ylabel("Overlap %")
plt.xlabel("Year")
plt.title("Top-10 Overlap by Enrollment Indicator (2017–2022)")
plt.ylim(0, 40)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)

plt.tight_layout()
plt.show()














# Merge GDP with ranking stats
merged_df = pd.merge(gdp_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df.head(5)








# Merge GDP with ranking stats
merged_df_edu = pd.merge(edu_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_edu.head(5)


# Merge Gov effectivness with ranking stats
merged_df_gov = pd.merge(gov_eff_long, total_score_df, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df_gov.head(5)





merged_rd = pd.merge(merged_df, rd_exp_long, on=["country", "year"],
    how="inner"
)
# Final columns: country, year, gdp_per_capita, score, university_count
merged_rd.head(5)





def correlation_analysis(df, target_col, predictor_col, title):
    """
    Compute Pearson, Spearman, and Kendall correlations 
    between target_col and predictor_col in df.
    """
    # Ensure numeric
    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')
    df[predictor_col] = pd.to_numeric(df[predictor_col], errors='coerce')
    
    # Drop missing rows
    corr_df = df.dropna(subset=[target_col, predictor_col])
    
    # Calculate correlations
    pearson_corr, pearson_p = pearsonr(corr_df[target_col], corr_df[predictor_col])
    spearman_corr, spearman_p = spearmanr(corr_df[target_col], corr_df[predictor_col])
    kendall_corr, kendall_p = kendalltau(corr_df[target_col], corr_df[predictor_col])
    
    # Print results
    print(f"=== Correlation with {title} ===")
    print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
    print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
    print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")
    print()

# Example usage:
correlation_analysis(merged_df, 'total_score', 'gdp_per_capita', 'GDP per Capita')
correlation_analysis(merged_df_edu, 'total_score', 'gov_exp_edu', 
                    'Government Expenditure on Education')
correlation_analysis(merged_df_gov, 'total_score', 'gov_effectiveness', 
                    'Government Effectiveness')
correlation_analysis(merged_rd, 'total_score', 'rd_exp_gdp', 'R&D Expenditure (% of GDP)')





indicators = ["school_enrol_primary_pct", "school_enrol_secondary_pct", "school_enrol_tertiary_pct"]

results = []
for ind in indicators:
    pearson_r, pearson_p = pearsonr(merged_edu_ind[ind], merged_edu_ind["total_score"])
    spearman_r, spearman_p = spearmanr(merged_edu_ind[ind], merged_edu_ind["total_score"])
    kendall_r, kendall_p = kendalltau(merged_edu_ind[ind], merged_edu_ind["total_score"])
    
    results.append({
        "Indicator": ind,
        "Pearson_r": pearson_r, "Pearson_p": pearson_p,
        "Spearman_r": spearman_r, "Spearman_p": spearman_p,
        "Kendall_tau": kendall_r, "Kendall_p": kendall_p
    })

corr_df = pd.DataFrame(results)
print(corr_df)




































