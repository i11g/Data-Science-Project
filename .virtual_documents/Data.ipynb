import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re















































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


df_rank


# Display basic information 
df_rank.info()


df_rank.describe().T


df_rank.head(5)


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()

print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank} years")


df_rank.dtypes


print("Missing Values in Ranking Dataset:")
print(df_rank.isnull().sum())

print("\nMissing Values in GDP Dataset:")
print(df_gdp.isnull().sum())


# Convert rank_display to numeric if it's not already
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# Optional: check missing by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


df_top300.to_csv("top300_universities.csv", index=False)


df_top300.iloc[763]


df_top300.shape





df_top300.info()


# Keep only the columns needed
df_clean = df_top300[["university", "year", "score", "country"]].copy()


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

# Replace common variations
country_replacements = {
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",
    "UK": "United Kingdom",
    "Russia": "Russian Federation",
}
df_clean["country"] = df_clean["country"].replace(country_replacements)


df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


df_clean = df_clean.dropna(subset=["score"])


df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


# 1. Average score per country per year
avg_score = (
    df_clean.groupby(["country", "year"], as_index=False)["score"]
      .mean()
      .rename(columns={"score": "avg_score"})
)

# 2. Number of ranked universities per country per year
num_universities = (
    df_clean.groupby(["country", "year"], as_index=False)["university"]
      .nunique()
      .rename(columns={"university": "num_universities"})
)

# Optional: Merge into one DataFrame
summary = pd.merge(avg_score, num_universities, on=["country", "year"])

# Sort by year and average score (optional)
summary = summary.sort_values(["year", "avg_score"], ascending=[True, False])

# Display result
print(summary)


def plot_totals_and_counts_per_country_per_year(df_clean, top_n=10):
    """
    Plots two figures (stacked by years):
      1) Total score per country (sum of 'score') for each year (top_n countries)
      2) Number of ranked universities per country for each year (top_n countries)
    Uses only matplotlib.
    """
    # --- Aggregate ---
    agg = (
        df_clean.groupby(["year", "country"])
                .agg(total_score=("score", "sum"),
                     num_universities=("university", "nunique"))
                .reset_index()
    )

    years = sorted(agg["year"].unique())
    n_years = len(years)

    # --- Figure A: Total score per country per year (top_n) ---
    fig1, axes1 = plt.subplots(n_years, 1, figsize=(11, 4 * n_years), constrained_layout=True)
    if n_years == 1:
        axes1 = [axes1]

    for ax, yr in zip(axes1, years):
        data_y = agg[agg["year"] == yr].sort_values("total_score", ascending=False).head(top_n)
        ax.barh(data_y["country"], data_y["total_score"])
        ax.invert_yaxis()  # highest at top
        ax.set_title(f"Total Score per Country â€” Top {top_n} (Year {yr})")
        ax.set_xlabel("Total Score")
        ax.set_ylabel("Country")

        # Optional value labels
        for i, v in enumerate(data_y["total_score"]):
            ax.text(v, i, f" {v:.1f}", va="center")

    # --- Figure B: Number of universities per country per year (top_n) ---
    fig2, axes2 = plt.subplots(n_years, 1, figsize=(11, 4 * n_years), constrained_layout=True)
    if n_years == 1:
        axes2 = [axes2]

    for ax, yr in zip(axes2, years):
        data_y = agg[agg["year"] == yr].sort_values("num_universities", ascending=False).head(top_n)
        ax.barh(data_y["country"], data_y["num_universities"])
        ax.invert_yaxis()
        ax.set_title(f"Number of Ranked Universities â€” Top {top_n} (Year {yr})")
        ax.set_xlabel("Count")
        ax.set_ylabel("Country")

        for i, v in enumerate(data_y["num_universities"]):
            ax.text(v, i, f" {int(v)}", va="center")

    plt.show()


plot_totals_and_counts_per_country_per_year(df_clean)


plot_totals_and_counts_per_country_per_year(df_clean, top_n=10)








df_gdp = pd.read_csv("data/gdp.csv")


#gdp_df = pd.read_csv("GDP.csv", skiprows=4)

# Preview the structure of the cleaned GDP dataframe
#gdp_df.head()


df_gdp


df_gdp.info()


df_gdp.describe()


df_gdp.head(5)


df_gdp.dtypes





# Step 2: Keep only GDP per capita (current US$)
gdp_filtered = df_gdp[df_gdp["Series Name"] == "GDP per capita (current US$)"].copy()

# Step 3: Drop unneeded columns
gdp_filtered = gdp_filtered.drop(columns=["Country Code", "Series Code", "Series Name"], errors="ignore")
gdp_filtered = gdp_filtered.loc[:, ~gdp_filtered.columns.str.contains("^Unnamed")]

# Step 4: Reshape from wide to long format
gdp_long = gdp_filtered.melt(id_vars=["Country Name"], var_name="year", value_name="gdp_per_capita")

# Clean year strings like "2017 [YR2017]" -> "2017"
gdp_long["year"] = gdp_long["year"].str.extract(r"(\d{4})")

# Continue with steps
gdp_long = gdp_long.rename(columns={"Country Name": "country"})
gdp_long["year"] = pd.to_numeric(gdp_long["year"], errors="coerce")
gdp_long = gdp_long.dropna(subset=["year", "gdp_per_capita"])
gdp_long["year"] = gdp_long["year"].astype(int)

# Filter for 2017â€“2024
gdp_long = gdp_long[gdp_long["year"].between(2017, 2024)]

gdp_long.head(20)











# Merge average scores and university counts
rank_stats = pd.merge(avg_scores, counts_per_year, on=["country", "year"])


# Merge GDP with ranking stats
merged_df = pd.merge(gdp_long, rank_stats, on=["country", "year"])

# Final columns: country, year, gdp_per_capita, score, university_count
merged_df.head(20)


import matplotlib.pyplot as plt

# Filter for the country
country = "United States"
country_df = merged_df[merged_df["country"] == country]

# Create figure and first axis (GDP)
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot GDP on left y-axis
ax1.set_title(f"GDP vs Rankings - {country}")
ax1.set_xlabel("Year")
ax1.set_ylabel("GDP per Capita (USD)", color="tab:blue")
ax1.plot(country_df["year"], country_df["gdp_per_capita"], label="GDP per Capita", color="tab:blue", linewidth=2)
ax1.tick_params(axis="y", labelcolor="tab:blue")

# Create second y-axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot average score and university count on right y-axis
ax2.set_ylabel("Score / University Count", color="tab:red")
ax2.plot(country_df["year"], country_df["score"], label="Avg Score", color="tab:red", linestyle="--", linewidth=2)
ax2.plot(country_df["year"], country_df["university_count"], label="# of Universities", color="tab:green", linestyle=":", linewidth=2)
ax2.tick_params(axis="y", labelcolor="tab:red")

# Combine legends from both axes
lines_1, labels_1 = ax1.get_legend_handles_labels()
lines_2, labels_2 = ax2.get_legend_handles_labels()
ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc="upper left")

# Grid and layout
ax1.grid(True)
plt.tight_layout()
plt.show()


# Compute correlation matrix
correlation = merged_df[["gdp_per_capita", "score", "university_count"]].corr()

# Display correlation table
print(correlation)


# Get top 10 countries by average score
top10_score_countries = (
    merged_df.groupby("country")["score"]
    .mean()
    .nlargest(10)
    .index
    .tolist()
)

# Get top 10 countries by total number of universities
top10_universities_countries = (
    merged_df.groupby("country")["university_count"]
    .sum()
    .nlargest(10)
    .index
    .tolist()
)

# Combine both sets
top_countries = list(set(top10_score_countries + top10_universities_countries))

# Filter merged_df for those countries only
filtered_df = merged_df[merged_df["country"].isin(top_countries)]

# Compute correlation
correlation_top = filtered_df[["gdp_per_capita", "score", "university_count"]].corr()

# Display result
print("ðŸ“Š Correlation Matrix (Top Countries):")
print(correlation_top)


from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Step 1: Prepare data (we use the same top countries DataFrame)
X = filtered_df[["gdp_per_capita"]]
y = filtered_df["score"]

# Step 2: Initialize and fit the model
model = LinearRegression()
model.fit(X, y)

# Step 3: Get coefficients
slope = model.coef_[0]
intercept = model.intercept_
r_squared = model.score(X, y)

print(f"ðŸ“ˆ Linear Regression Model:")
print(f"score = {slope:.4f} * gdp_per_capita + {intercept:.2f}")
print(f"RÂ² (explained variance): {r_squared:.4f}")


# Scatterplot + regression line
plt.figure(figsize=(8, 5))
sns.scatterplot(x="gdp_per_capita", y="score", data=filtered_df, label="Data")
plt.plot(X, model.predict(X), color="red", label="Regression Line")

plt.title("GDP per Capita vs. University Score (Top Countries)")
plt.xlabel("GDP per Capita (USD)")
plt.ylabel("Average University Score")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


import pandas as pd

# Load your dataset
edu_df = pd.read_csv("world-education-data.csv")  # replace with actual filename

# Keep only the relevant columns
spending_df = edu_df[["country", "year", "gov_exp_pct_gdp"]].copy()

# Drop missing values (optional: you can choose to keep them too)
spending_df = spending_df.dropna(subset=["gov_exp_pct_gdp"])

# Convert year to integer
spending_df["year"] = spending_df["year"].astype(int)

# Optional: Filter for years of interest
spending_df = spending_df[spending_df["year"].between(2017, 2024)]

# View results
spending_df.head(20)


merged = pd.merge(merged_df, spending_df, on=["country", "year"], how="left")



merged[["gov_exp_pct_gdp", "score", "gdp_per_capita"]].corr()



print(merged_df.columns.tolist())


edu_df = pd.read_csv("world-education-data.csv")  # update with your actual filename

# Keep only the relevant columns
edu_features_df = edu_df[[
    "country", "year",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]].copy()



merged_df = pd.merge(merged_df, edu_features_df, on=["country", "year"], how="left")


# Select only the relevant columns
columns_of_interest = [
    "score",
    "gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Filter the dataset
edu_corr = merged_df[columns_of_interest].dropna()

# Compute correlation
corr_matrix = edu_corr.corr()
print(corr_matrix)

# Display as heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix: Education Indicators vs Score")
plt.tight_layout()
plt.show()


from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import pandas as pd

# Step 1: Define the features and target
features = [
    "gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Drop missing values
model_data = merged_df.dropna(subset=features + ["score"])

# X = predictors, y = target
X = model_data[features]
y = model_data["score"]


# Fit the model
reg = LinearRegression()
reg.fit(X, y)

# Predict
y_pred = reg.predict(X)

# R-squared
r2 = r2_score(y, y_pred)

# Coefficients
print("ðŸ“Š Multiple Linear Regression Results:")
for feature, coef in zip(features, reg.coef_):
    print(f"{feature}: {coef:.4f}")
    
print(f"\nIntercept: {reg.intercept_:.2f}")
print(f"RÂ² (explained variance): {r2:.4f}")


import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
plt.scatter(y, y_pred, alpha=0.6)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel("Actual Score")
plt.ylabel("Predicted Score")
plt.title("Actual vs Predicted University Score")
plt.grid(True)
plt.tight_layout()
plt.show()



import numpy as np

# Add log-transformed GDP column
merged_df["log_gdp_per_capita"] = np.log(merged_df["gdp_per_capita"])

# Optional: confirm no -inf or NaN values
merged_df = merged_df.replace([np.inf, -np.inf], np.nan)


from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Define new feature list
log_features = [
    "log_gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Drop missing
log_model_data = merged_df.dropna(subset=log_features + ["score"])
X_log = log_model_data[log_features]
y_log = log_model_data["score"]

# Fit the model
log_model = LinearRegression()
log_model.fit(X_log, y_log)
y_log_pred = log_model.predict(X_log)

# Print results
print("ðŸ“Š Multiple Regression with log(GDP):")
for f, coef in zip(log_features, log_model.coef_):
    print(f"{f}: {coef:.4f}")

print(f"\nIntercept: {log_model.intercept_:.2f}")
print(f"RÂ² (explained variance): {r2_score(y_log, y_log_pred):.4f}")


from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Step 1: Get GDP per capita time series for United States (2017â€“2023)
country = "United States"
gdp_series = merged_df[(merged_df["country"] == country) & (merged_df["year"] <= 2023)]
gdp_series = gdp_series.set_index("year")["gdp_per_capita"]

# Step 2: Fit ARIMA model (try (1,1,1) as a default)
model = ARIMA(gdp_series, order=(1, 1, 1))
model_fit = model.fit()

# Step 3: Forecast GDP for 2024
gdp_forecast_2024 = model_fit.forecast(steps=1)
forecasted_gdp = gdp_forecast_2024.iloc[0]

print(f"ðŸ“ˆ Forecasted GDP per capita for {country} in 2024: ${forecasted_gdp:,.2f}")


features = ['log_gdp_per_capita', 'gov_exp_pct_gdp', 'lit_rate_adult_pct',
            'school_enrol_primary_pct', 'school_enrol_secondary_pct', 'school_enrol_tertiary_pct']


merged_df[(merged_df["country"] == "United States") & (merged_df["year"] == 2024)]



import numpy as np
import pandas as pd

# Step 1: Define the features used in your log-GDP regression model
features = [
    "log_gdp_per_capita",
    "gov_exp_pct_gdp",
    "lit_rate_adult_pct",
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# Step 2: Create a minimal input row for 2024
# Replace these values with averages or reasonable estimates
input_row_2024 = pd.DataFrame([{
    "log_gdp_per_capita": np.log(forecasted_gdp),
    "gov_exp_pct_gdp": 5.0,                  # average: ~5% of GDP
    "lit_rate_adult_pct": 99.0,              # high literacy for US
    "school_enrol_primary_pct": 102.0,       # slightly over 100% (common due to repeaters)
    "school_enrol_secondary_pct": 95.0,
    "school_enrol_tertiary_pct": 70.0
}])

# Step 3: Predict score
predicted_score_2024 = log_model.predict(input_row_2024)[0]
print(f"ðŸŽ“ Predicted university score for United States in 2024 (GDP-based): {predicted_score_2024:.2f}")


































