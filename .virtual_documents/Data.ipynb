import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from scipy.stats import pearsonr, spearmanr, kendalltau
import seaborn as sns
import os
pd.set_option('future.no_silent_downcasting', True)
from typing import Literal, Dict, Any
























































# Load dataset
df_rank = pd.read_csv("data/world-university-rankings-2017-to-2022.csv")


# Display basic information
## df_rank.info()


# df_rank.describe().T


df_rank.dtypes


df_rank.head(5)


num_countries_rank = df_rank['country'].nunique()
num_years_rank = df_rank['year'].nunique()
print(f"Ranking Dataset: {num_countries_rank} countries, {num_years_rank}years")


print(df_rank.isnull().sum())


# Convert rank_display to numeric 
df_rank['rank_display'] = pd.to_numeric(df_rank['rank_display'], errors='coerce')

# Filter top 300 universities for each year
df_top300 = df_rank[df_rank['rank_display'] <= 300]

# Count missing scores
missing_count = df_top300['score'].isna().sum()
total_count = len(df_top300)
missing_percentage = (missing_count / total_count) * 100

print(f"Total entries in top 300: {total_count}")
print(f"Missing score entries: {missing_count}")
print(f"Missing percentage: {missing_percentage:.2f}%")

# check missing score by year
missing_by_year = df_top300.groupby('year')['score'].apply(lambda x: x.isna().sum())
print("\nMissing scores by year:")
print(missing_by_year)


# save the new dataset in tables folder
df_top300.to_csv("tables/top300_universities.csv", index=False)


# check what is the shape of the table
df_top300.shape


# Keep only the columns needed 
df_clean = df_top300[["university", "year", "score", "country"]].copy()





# =====================================
# Global country name standardization
# =====================================

country_replacements = {
    # United States
    "USA": "United States",
    "U.S.A.": "United States",
    "United States of America": "United States",

    # United Kingdom
    "UK": "United Kingdom",
    "U.K.": "United Kingdom",
    "Great Britain": "United Kingdom",
    "England": "United Kingdom",  # sometimes appears separately

    # Russia
    "Russia": "Russian Federation",
    "Russian Fed.": "Russian Federation",
    "RU": "Russian Federation",

    # South Korea
    "Korea, South": "South Korea",
    "Republic of Korea": "South Korea",
    "Korea Republic": "South Korea",

    # North Korea
    "Korea, North": "North Korea",
    "Democratic People's Republic of Korea": "North Korea",

    # Iran
    "Iran, Islamic Rep.": "Iran",
    "Islamic Republic of Iran": "Iran",

    # Egypt
    "Egypt, Arab Rep.": "Egypt",
    "Arab Republic of Egypt": "Egypt",

    # Vietnam
    "Viet Nam": "Vietnam",

    # Czech Republic
    "Czechia": "Czech Republic",

    # Others common in World Bank / QS
    "Hong Kong SAR, China": "Hong Kong",
    "Macao SAR, China": "Macau",
    "China, Hong Kong SAR": "Hong Kong",
    "China, Macao SAR": "Macau",

    "Slovak Republic": "Slovakia",
    "Syrian Arab Republic": "Syria",
    "Türkiye": "Turkey",
    "Gambia, The": "Gambia",
    "Bahamas, The": "Bahamas",
    "Yemen, Rep.": "Yemen",
    "Lao PDR": "Laos",
    "Brunei Darussalam": "Brunei",
    "Côte d'Ivoire": "Ivory Coast",
    "Congo, Dem. Rep.": "Democratic Republic of the Congo",
    "Congo, Rep.": "Republic of the Congo"
}

def standardize_country_names(df: pd.DataFrame, col: str = "country") -> pd.DataFrame:
    """
    Harmonize country names in a dataframe for consistency across datasets.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe with a country column.
    col : str
        Column name containing country names (default: 'country').
    
    Returns:
    --------
    pd.DataFrame
        Dataframe with standardized country names.
    """
    if col not in df.columns:
        raise KeyError(f"Column '{col}' not found in dataframe")

    df[col] = (
        df[col]
        .astype(str)
        .str.strip()
        .replace(country_replacements)
    )
    return df


# Strip whitespace and title-case country names
df_clean["country"] = df_clean["country"].str.strip()

df_clean = standardize_country_names(df_clean, col="country")

df_clean["country"] = df_clean["country"].replace(country_replacements)


# Drop duplicates
df_clean = df_clean.drop_duplicates(subset=["university", "year", "country"])


# Since the missing values are 0.39 % we will drop them
df_clean = df_clean.dropna(subset=["score"])


# For sanity convert year and score to int and float 
df_clean["year"] = df_clean["year"].astype(int)
df_clean["score"] = df_clean["score"].astype(float)


df_clean.info()





# Make the total_score_df, in which total_score = sum of all university scores per country-year
total_score_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(total_score=("score", "sum"))
)

print(total_score_df.head())


# Boxplot for total_score to visualize outliers
plt.figure(figsize=(8, 4))
plt.boxplot(total_score_df["total_score"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightblue"))

plt.title("Boxplot of Total Score (All Years)")
plt.xlabel("Total Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/qs_total_score_trends.png", dpi=300, bbox_inches='tight')
plt.show()


# Make num_univ_df in which num_universities = count of unique universities per country-year
num_univ_df = (
    df_clean.groupby(["year", "country"], as_index=False)
      .agg(num_universities=("university", "nunique"))
)

print(num_univ_df.head())


# Boxplot for num_universities
plt.figure(figsize=(8, 4))
plt.boxplot(num_univ_df["num_universities"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightgreen"))
plt.title("Boxplot of Number of Universities (All Years)")
plt.xlabel("Number of Universities")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/num_universities_trends.png", dpi=300, bbox_inches='tight')
plt.show()


# Merge total_score_df and num_universities_df to new agg_df
agg_df = total_score_df.merge(num_univ_df, on=["year", "country"], how="left")

# Make avg_score_df in which avg_score = total_score / num_universities
agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

print(agg_df.head())


# Boxplot for avg_score
plt.figure(figsize=(8, 4))
plt.boxplot(agg_df["avg_score"], vert=False, patch_artist=True, 
            boxprops=dict(facecolor="lightcoral"))
plt.title("Boxplot of Average Score (All Years)")
plt.xlabel("Average Score")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.savefig("plots/avearge_scores_trends.png", dpi=300, bbox_inches='tight')
plt.show()








# ---------- Build aggregated dataset ----------
def build_agg_from_raw(df_clean: pd.DataFrame) -> pd.DataFrame:
    """
    Expects columns: ['university','year','score','country'].
    Returns agg_df with: ['year','country','total_score','num_universities','avg_score'].
    """
    required_cols = {"university", "year", "score", "country"}
    missing = required_cols - set(df_clean.columns)
    if missing:
        raise KeyError(f"Missing columns: {sorted(missing)}")

    # Drop rows with NaNs in essential columns
    df = df_clean.dropna(subset=["country", "year", "score", "university"]).copy()
    df["year"] = df["year"].astype(int)

    # One groupby-agg for both metrics
    agg_df = (
        df.groupby(["year", "country"], as_index=False)
          .agg(
              total_score=("score", "sum"),
              num_universities=("university", "nunique")
          )
    )

    # Average score per university
    agg_df["avg_score"] = agg_df["total_score"] / agg_df["num_universities"]

    return agg_df

agg_df.to_csv("tables/agg_university_metrics.csv", index=False)

# ---------- 2) Helper to get top-K per year ----------
def top_k_per_year(df_clean: pd.DataFrame, value_col: str, k: int = 10) -> pd.DataFrame:
    need = {"year", "country", value_col}
    if not need.issubset(df.columns):
        raise KeyError(f"DataFrame must contain: {sorted(need)}")
    return (
        df.sort_values(["year", value_col], ascending=[True, False], kind="mergesort")
          .groupby("year", group_keys=False)
          .head(k)
          .reset_index(drop=True)
    )

# ---------- 3) Plotters (one per metric) ----------
def plot_top10_total_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("total_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["total_score"])
        for i, v in enumerate(d["total_score"]):
            plt.text(v, i, f"{v:.1f}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Total Score ({y})")
        plt.xlabel("Total Score")
        plt.tight_layout()
        plt.show()


def plot_top10_avg_score_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("avg_score", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["avg_score"])
        # label with both avg and number of universities
        for i, (v, n) in enumerate(zip(d["avg_score"], d["num_universities"])):
            plt.text(v, i, f"{v:.1f}  |  {n} univ", va="center", ha="left")
        plt.title(f"Top {k} Countries by Average Score ({y})")
        plt.xlabel("Average Score per University")
        plt.tight_layout()
        plt.show()


def plot_top10_num_universities_per_year(agg_df: pd.DataFrame, k: int = 10):
    for y in sorted(agg_df["year"].unique()):
        d = (agg_df[agg_df["year"] == y]
             .sort_values("num_universities", ascending=False)
             .head(k).iloc[::-1])
        plt.figure(figsize=(10, 6))
        plt.barh(d["country"], d["num_universities"])
        for i, v in enumerate(d["num_universities"]):
            plt.text(v, i, f"{v}", va="center", ha="left")
        plt.title(f"Top {k} Countries by Number of Universities ({y})")
        plt.xlabel("Number of Universities")
        plt.tight_layout()
        plt.show()


# ---------- 4) Example usage ----------
# raw df name assumed to be `df` with cols: university, year, score, country
# agg_df = build_agg_from_raw(df)

# Tables (if you need them):
# top10_total = top_k_per_year(agg_df[["year","country","total_score"]], "total_score", k=10)
# top10_avg   = top_k_per_year(agg_df[["year","country","avg_score","num_universities"]], 
        #"avg_score", k=10)
# top10_count = top_k_per_year(agg_df[["year","country","num_universities"]], 
        #"num_universities", k=10)

# Plots:
# plot_top10_total_score_per_year(agg_df, k=10)
# plot_top10_avg_score_per_year(agg_df, k=10)
# plot_top10_num_universities_per_year(agg_df, k=10)


plot_top10_total_score_per_year(agg_df, k=10)
plot_top10_avg_score_per_year(agg_df, k=10)
plot_top10_num_universities_per_year(agg_df, k=10)


# Time Series trends for the top 10 countires  

def plot_score_trends(agg_df: pd.DataFrame, metric: str = "total_score", top_n: int = 5):
    """
    Plots time-series trends for the top N countries based on the last available year.
    
    Parameters
    ----------
    agg_df : pd.DataFrame
        Must contain ['year','country', metric].
    metric : str
        One of ['total_score', 'avg_score', 'num_universities'].
    top_n : int
        Number of countries to plot (selected from the last year).
    """
    if metric not in agg_df.columns:
        raise KeyError(f"Metric '{metric}' not found in DataFrame.")

    # --- Determine top N countries based on last year ---
    last_year = agg_df["year"].max()
    top_countries = (
        agg_df[agg_df["year"] == last_year]
        .nlargest(top_n, metric)["country"]
        .tolist()
    )

    # --- Filter for those countries only ---
    plot_df = agg_df[agg_df["country"].isin(top_countries)].copy()

    # --- Plot ---
    plt.figure(figsize=(10, 6))
    sns.lineplot(data=plot_df, x="year", y=metric, hue="country", marker="o")

    plt.title(f"{metric.replace('_', ' ').title()} Trends for Top {top_n} Countries")
    plt.xlabel("Year")
    plt.ylabel(metric.replace("_", " ").title())
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend(title="Country", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()


plot_score_trends(agg_df, metric="total_score", top_n=10)








# Load datasets
gdp_df = pd.read_csv("data/gdp.csv")
edu_df = pd.read_csv("data/gov_exp_on_edu.csv")
gov_df = pd.read_csv("data/gov_effect_score .csv")
res_df = pd.read_csv("data/res_dev.csv")


gdp_df


edu_df


# Get basic information
gdp_df.info()
edu_df.info()
gov_df.info()
res_df.info()





def clean_worldbank_df(df, country_col="Country Name", last_country="Zimbabwe"):
    """
    Cleans a World Bank dataset by:
    - Dropping fully empty rows
    - Keeping only rows from the start until the last country (e.g., Zimbabwe)
    - Resetting the index
    
    Parameters
    ----------
    df : pd.DataFrame
        The dataset to clean
    country_col : str
        Name of the column containing country names
    last_country : str
        The last valid country in the dataset (default: 'Zimbabwe')
    
    Returns
    -------
    pd.DataFrame
        Cleaned dataset
    """
    # Remove completely empty rows
    df = df.dropna(how="all")
    
    # Keep only up to last_country
    last_country_idx = df[df[country_col] == last_country].index
    if not last_country_idx.empty:
        df = df.loc[:last_country_idx[0]]
    
    # Reset index
    df = df.reset_index(drop=True)
    
    return df

# Apply to all datasets
gdp_df = clean_worldbank_df(gdp_df)
edu_df = clean_worldbank_df(edu_df)
gov_df = clean_worldbank_df(gov_df)
res_df = clean_worldbank_df(res_df)

# Quick check
print(gdp_df.tail(3))





def check_missing_values(df: pd.DataFrame, dataset_name: str = "Dataset",
                         year_min: int = 2017, year_max: int = 2022):
    """
    Cleans '..' placeholders to NaN, checks missing values ONLY for years in [year_min, year_max],
    and returns cleaned DataFrame + summary.
    """
    df_clean = (
        df.copy()
          .replace(r"^\s*\.\.\s*$", np.nan, regex=True)
          .infer_objects(copy=False)
    )

    # Detect year columns
    def _extract_year(col):
        m = re.match(r"^\s*((19|20)\d{2})", str(col))
        return int(m.group(1)) if m else None

    year_cols = [c for c in df_clean.columns
                 if (y := _extract_year(c)) is not None and year_min <= y <= year_max]

    if not year_cols:
        print(f"[{dataset_name}] No year columns found in {year_min}–{year_max} range.")
        return df_clean, pd.DataFrame()

    # Missing summary only for relevant years
    missing_summary = df_clean[year_cols].isna().sum().to_frame(name="missing_count")
    missing_summary["missing_pct"] = (missing_summary["missing_count"] / len(df_clean)) * 100

    print(f"=== Missing Values Summary ({year_min}–{year_max}): {dataset_name} ===")
    print(missing_summary)

    return df_clean, missing_summary


df_clean_gdp, missing_summary_gdp = check_missing_values(gdp_df)


df_clean_edu, missing_summary_edu = check_missing_values(edu_df)


df_clean_gov, missing_summary_gov = check_missing_values(gov_df)


df_clean_res, missing_summary_res = check_missing_values(res_df)






def _yr_cols(df: pd.DataFrame, year_min: int = 2017, year_max: int = 2022) -> list[str]:
    yrs = []
    for c in df.columns:
        m = re.search(r'(19|20)\d{2}', str(c))
        if m:
            y = int(m.group(0))
            if year_min <= y <= year_max:
                yrs.append((c, y))
    yrs.sort(key=lambda t: t[1])
    return [c for c, _ in yrs]

def clean_interp_ffill_bfill_2017_2022(
    df: pd.DataFrame,
    year_min: int = 2017,
    year_max: int = 2022,
    clean_dots: bool = True
):
    year_cols = _yr_cols(df, year_min, year_max)
    if not year_cols:
        raise ValueError(f"No {year_min}–{year_max} year-like columns found.")

    out = df.copy()

    if clean_dots:
        out[year_cols] = out[year_cols].replace(r'^\s*\.\.\s*$', np.nan, regex=True)
    out[year_cols] = out[year_cols].apply(pd.to_numeric, errors="coerce")

    before = out[year_cols].isna().sum()
    total_before = int(before.sum())
    rows_all_na_before = int(out[year_cols].isna().all(axis=1).sum())

    out[year_cols] = out[year_cols].interpolate(axis=1, method="linear", limit_direction="both")
    out[year_cols] = out[year_cols].ffill(axis=1).bfill(axis=1)

    after = out[year_cols].isna().sum()
    total_after = int(after.sum())
    rows_all_na_after = int(out[year_cols].isna().all(axis=1).sum())

    per_year_report = pd.DataFrame({
        "missing_before": before,
        "missing_after": after,
        "reduced": before - after
    })
    overall = {
        "total_nans_before": total_before,
        "total_nans_after": total_after,
        "total_reduction": total_before - total_after,
        "rows_all_nan_before": rows_all_na_before,
        "rows_all_nan_after": rows_all_na_after,
    }
    return out, per_year_report, overall


def sanity_check_wb_data(
    df: pd.DataFrame,
    year_min: int = 2017,
    year_max: int = 2022,
    expected_range: tuple[float, float] | None = None
):
    """
    Run basic sanity checks on a cleaned World Bank-style dataset.
    
    Checks performed:
    1. Year columns exist, are sorted, and no duplicates.
    2. Missing values did not increase after cleaning.
    3. Rows with all-NaN remain unchanged.
    4. Optional: values lie within expected bounds.
    5. Warn if high proportion of missing values remain.
    """

    year_cols = _yr_cols(df, year_min, year_max)
    if not year_cols:
        raise ValueError(f"No {year_min}–{year_max} year-like columns found.")

    # --- 1. Structure ---
    years = [int(re.search(r'(19|20)\d{2}', c).group(0)) for c in year_cols]
    if sorted(years) != years:
        print("[WARN] Year columns are not sorted.")
    if len(set(year_cols)) != len(year_cols):
        print("[WARN] Duplicate year columns detected.")

    # --- 2. Missingness before vs after (requires original NaN info) ---
    # Here we just compute current missingness:
    missing_now = df[year_cols].isna().sum().sum()
    rows_all_na = int(df[year_cols].isna().all(axis=1).sum())
    print(f"[INFO] Total missing values now: {missing_now}")
    print(f"[INFO] Rows with all NaNs: {rows_all_na}")

    # --- 3. Value range ---
    if expected_range is not None:
        lo, hi = expected_range
        vmin, vmax = df[year_cols].min().min(), df[year_cols].max().max()
        if vmin < lo or vmax > hi:
            print(f"[WARN] Values outside expected range {expected_range}: "
                  f"min={vmin}, max={vmax}")

    # --- 4. Proportion of missing ---
    total_cells = len(df) * len(year_cols)
    missing_pct = (missing_now / total_cells) * 100
    if missing_pct > 20:
        print(f"[WARN] {missing_pct:.1f}% of values still missing after cleaning.")

    print("[OK] Sanity check completed.")


# After cleaning step
df_clean, per_year, overall = clean_interp_ffill_bfill_2017_2022(gov_df)

# Run sanity checks
sanity_check_wb_data(df_clean, expected_range=(0, 100))


datasets = {
    "gdp": gdp_df,        # World Bank GDP (wide)
    "gov_eff": gov_df,    # Government Effectiveness (wide)
    "edu_spend": edu_df,  # Gov. Expenditure on Education (wide)
    "rd": res_df,         # R&D Expenditure (wide)
}

cleaned = {}
per_year_reports = {}
overall_rows = []

for name, raw in datasets.items():
    df_clean, per_year_report, overall = clean_interp_ffill_bfill_2017_2022(raw)
    cleaned[name] = df_clean
    per_year_reports[name] = per_year_report
    overall_rows.append({"dataset": name, **overall})

overall_summary_df = pd.DataFrame(overall_rows)

print("=== Overall missing summary (before/after) ===")
print(overall_summary_df)








# Countries to exclude per dataset 

os.makedirs("excluded", exist_ok=True)

excluded_results = {}

for name, df_clean in cleaned.items():
    year_cols = list(per_year_reports[name].index)  # same year columns used in reporting

    excluded_countries = (
        df_clean.loc[df_clean[year_cols].isna().all(axis=1), "Country Name"]
        .dropna()
        .unique()
    )
    excluded_results[name] = excluded_countries

    print(f"\n=== Countries to exclude: {name.upper()} ===")
    for c in excluded_countries:
        print(" -", c)
    print(f"Total excluded: {len(excluded_countries)}")

    # Save one file per dataset (optional, comment out if not needed)
    pd.DataFrame({"Country Name": excluded_countries}).to_csv(
        f"excluded/excluded_{name}.csv", index=False
    )

# (Optional) combined long table for appendix
combined_excluded = pd.concat(
    [pd.DataFrame({"dataset": name, "country_name": countries})
     for name, countries in excluded_results.items()],
    ignore_index=True
)
combined_excluded.to_csv("excluded/excluded_all_datasets.csv", index=False)
print("\nSaved combined excluded list to excluded/excluded_all_datasets.csv")





def drop_all_nan_rows(df_clean: pd.DataFrame, year_min=2017, year_max=2022):
    yc = _yr_cols(df_clean, year_min, year_max)
    return df_clean.loc[~df_clean[yc].isna().all(axis=1)].reset_index(drop=True)

cleaned_no_allnan = {k: drop_all_nan_rows(v) for k, v in cleaned.items()}


def wb_to_long_simple_nointerp(
    df: pd.DataFrame,
    *,
    value_name: str,                 # e.g. "gov_effectiveness_pctile"
    series_code: str | None = None,  # e.g. "GE.PER.RNK"
    series_name: str | None = None,  # e.g. "Government Effectiveness: Percentile Rank"
    year_min: int = 2017,
    year_max: int = 2022,
) -> pd.DataFrame:
    """
    Wide -> long for 2017–2022 with optional indicator filter.
    Assumes data already cleaned (NaNs handled, numeric).
    """
    # Likely column names
    name_col = ("Series Name" if "Series Name" in df.columns else
                "Indicator Name" if "Indicator Name" in df.columns else None)
    code_col = ("Series Code" if "Series Code" in df.columns else
                "Indicator Code" if "Indicator Code" in df.columns else None)
    country_col = ("Country Name" if "Country Name" in df.columns else
                   "country" if "country" in df.columns else None)
    if country_col is None:
        raise KeyError("Need a country column (expected 'Country Name' or 'country').")

    d = df.copy()

    # Optional exact-match filters (case-insensitive)
    if series_code and code_col:
        d = d[d[code_col].astype(str).str.strip().str.casefold() == series_code.strip().casefold()]
    if series_name and name_col:
        d = d[d[name_col].astype(str).str.strip().str.casefold() == series_name.strip().casefold()]

    # Drop meta + Unnamed
    d = d.drop(columns=[c for c in ["Country Code", "Series Code", "Series Name",
                                    "Indicator Code", "Indicator Name"] if c in d.columns],
               errors="ignore")
    d = d.loc[:, ~d.columns.str.match(r"^Unnamed")]

    # Year columns
    year_cols = _yr_cols(d, year_min, year_max)
    if not year_cols:
        raise ValueError(f"No year columns detected in {year_min}-{year_max}.")

    # Wide -> long
    long_df = d.melt(
        id_vars=[country_col],
        value_vars=year_cols,
        var_name="year_raw",
        value_name=value_name
    )
    # Extract 4-digit year
    long_df["year"] = pd.to_numeric(
        long_df["year_raw"].astype(str).str.extract(r"(\d{4})")[0],
        errors="coerce"
    ).astype("Int64")

    long_df = long_df.drop(columns=["year_raw"]).dropna(subset=["year"])
    long_df["year"] = long_df["year"].astype(int)
    long_df = long_df[long_df["year"].between(year_min, year_max)]

    # Standardize country column name
    if country_col != "country":
        long_df = long_df.rename(columns={country_col: "country"})

    # Since upstream cleaning already handled NaNs, we can keep rows as-is,
    # but if you prefer to drop any remaining NaNs in this value:
    long_df = long_df.dropna(subset=[value_name]).reset_index(drop=True)

    return long_df


def sanity_check_long(
    df: pd.DataFrame,
    value_col: str,
    year_min: int = 2017,
    year_max: int = 2022,
    expected_range: tuple[float, float] | None = None
) -> dict:
    """
    Sanity checks for long-format World Bank data.
    Expects columns: ['country','year', value_col]
    """

    summary = {"rows": len(df)}

    # 1) Year checks
    if not pd.api.types.is_integer_dtype(df["year"]):
        print("[WARN] Year column is not integer dtype.")
    if not df["year"].between(year_min, year_max).all():
        print("[WARN] Some years fall outside expected range.")
    summary["year_min"] = df["year"].min()
    summary["year_max"] = df["year"].max()

    # 2) Duplicates
    dup_count = df.duplicated(subset=["country", "year"]).sum()
    if dup_count > 0:
        print(f"[WARN] {dup_count} duplicate (country, year) rows detected.")
    summary["duplicates"] = int(dup_count)

    # 3) Coerce value column to numeric (tracks non-numeric)
    s_raw = df[value_col]
    s_num = pd.to_numeric(s_raw, errors="coerce")
    non_numeric = s_raw.notna() & s_num.isna()
    nn_count = int(non_numeric.sum())
    if nn_count > 0:
        print(f"[WARN] {nn_count} non-numeric entries in '{value_col}' were coerced to NaN.")

    n_missing = int(s_num.isna().sum())
    if n_missing > 0:
        print(f"[INFO] {n_missing} missing (or non-numeric) values in '{value_col}'.")
    summary["missing_values"] = n_missing

    # 4) Range check (only if we have any numeric values)
    if expected_range is not None and s_num.notna().any():
        lo, hi = expected_range
        vmin, vmax = float(s_num.min()), float(s_num.max())
        if (vmin < lo) or (vmax > hi):
            print(f"[WARN] Values outside expected range {expected_range}: min={vmin}, max={vmax}")
        summary["value_min"] = vmin
        summary["value_max"] = vmax

    print("[OK] Long-format sanity check completed.")
    return summary



# Step 1: Reshape your raw World Bank dataframe
gov_long = wb_to_long_simple_nointerp(
    gov_df,                                # raw dataframe
    value_name="gov_effectiveness_pctile", # column name for values
    series_code="GE.PER.RNK"               # filter for indicator (optional)
)

# Step 2: Run sanity check on reshaped data
summary = sanity_check_long(
    gov_long, 
    value_col="gov_effectiveness_pctile", 
    year_min=2017, 
    year_max=2022, 
    expected_range=(0, 100)  # optional: if you know the expected scale
)

print(summary)


print("datasets keys:", list(datasets.keys()))
print("cleaned keys :", list(cleaned.keys()))


gdp_pc_long = wb_to_long_simple_nointerp(
    cleaned["gdp"],
    value_name="gdp_per_capita",
    series_code="NY.GDP.PCAP.CD"
)
print(gdp_pc_long.head())
 
gov_long = wb_to_long_simple_nointerp(
    cleaned["gov_eff"],
    value_name="gov_effectiveness",
    series_code="GE.PER.RNK"
)
print(gov_long.head())

rd_long = wb_to_long_simple_nointerp(
    cleaned["rd"],
    value_name="rd_exp_gdp",
    series_code="GB.XPD.RSDV.GD.ZS"
)
print(rd_long.head())

edu_long = wb_to_long_simple_nointerp(
    cleaned["edu_spend"],
    value_name="gov_edu_exp",
    series_code="SE.XPD.TOTL.GD.ZS"
)
print(edu_long.head())


edu_long = standardize_country_names(edu_long, col="country")


rd_long = standardize_country_names(rd_long, col="country")


gov_long = standardize_country_names(gov_long, col="country")


gdp_pc_long = standardize_country_names(gdp_pc_long, col="country")








top10_per_year_gdp = (
    gdp_pc_long
    .sort_values(["year", "gdp_per_capita"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)
top10_per_year_gdp.to_csv("tables/top10_gdp_per_year.csv", index = False)
print(top10_per_year_gdp)


# We loop through each year and make a separate plot
for year, group in top10_per_year_gdp.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gdp_per_capita", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gdp_per_capita"], color="skyblue")
    plt.xlabel("GDP per Capita (US$)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by GDP per Capita in {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()





top10_per_year_edu = (
    edu_long
    .sort_values(["year", "gov_edu_exp"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_edu)


# Save to CSV
top10_per_year_edu.to_csv("top10_edu_per_year.csv", index=False)

# Loop through each year and make a separate plot
for year, group in top10_per_year_edu.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gov_edu_exp", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gov_edu_exp"], color="skyblue")
    plt.xlabel("gov_edu_exp")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by Gov_Edu_Exp {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()





top10_per_year_gov = (
    gov_long
    .sort_values(["year", "gov_effectiveness"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_gov)


# We loop through each year and make a separate plot
for year, group in top10_per_year_gov.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("gov_effectiveness", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["gov_effectiveness"], color="skyblue")
    plt.xlabel("Gov Effectiveness (%)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by R%S {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()





top10_per_year_res = (
    rd_long
    .sort_values(["year", "rd_exp_gdp"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)
)

print(top10_per_year_res)


# We loop through each year and make a separate plot
for year, group in top10_per_year_res.groupby("year"):
    plt.figure(figsize=(10, 6))
    group_sorted = group.sort_values("rd_exp_gdp", ascending=False)
    plt.barh(group_sorted["country"], group_sorted["rd_exp_gdp"], color="skyblue")
    plt.xlabel("R&S (% GDPUS$)")
    plt.ylabel("Country")
    plt.title(f"Top 10 Countries by R%S {year}")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()








# Sort by year, then total_score descending
top10_total_score_df = (
    total_score_df
    .sort_values(["year", "total_score"], ascending=[True, False])
    .groupby("year")
    .head(10)  # top 10 per year
    .reset_index(drop=True)
)

print(top10_total_score_df)





def compute_yearly_overlaps(
    top10_indicator: pd.DataFrame,
    top10_total_score: pd.DataFrame,
    indicator_name: str,
    *,
    year_col: str = "year",
    country_col: str = "country",
    baseline: Literal["score", "indicator", "union"] = "score",
    verbose: bool = True,
) -> tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Compute year-by-year overlaps between a Top-10 indicator list and the Top-10 Total Score list.

    Parameters
    ----------
    top10_indicator : DataFrame
        Must contain columns [year_col, country_col] for the indicator Top-10 per year.
    top10_total_score : DataFrame
        Must contain columns [year_col, country_col] for the Total Score Top-10 per year.
    indicator_name : str
        A label for the indicator (e.g., "GDP", "Primary", "Gov_Effectiveness", "R&D").
    year_col : str
        Name of the year column (default "year").
    country_col : str
        Name of the country column (default "country").
    baseline : {"score", "indicator", "union"}
        What to use in the denominator for the overlap percentage:
        - "score": size of Total Score list (default; matches your previous snippets)
        - "indicator": size of the indicator list
        - "union": size of the union of both lists
    verbose : bool
        If True, prints a per-year line and totals.

    Returns
    -------
    results_df : DataFrame
        Columns: [year, indicator, overlap_count, baseline_count, overlap_pct, overlap_countries]
    totals : dict
        {
          "indicator": str,
          "total_overlap_sum": int,
          "total_baseline_sum": int,
          "total_pct_sum": float,
          "unique_overlap_countries": set,
          "unique_union_countries": set,
          "unique_overlap_pct": float
        }
    """

    # Ensure required columns exist
    for df, name in [(top10_indicator, "top10_indicator"), (top10_total_score, "top10_total_score")]:
        missing = {year_col, country_col} - set(df.columns)
        if missing:
            raise ValueError(f"{name} is missing columns: {missing}")

    # Gather all years present in either dataframe
    years = sorted(set(top10_indicator[year_col].unique()).union(set(top10_total_score[year_col].unique())))

    # Accumulators
    rows = []
    total_overlap_sum = 0
    total_baseline_sum = 0
    all_overlap_countries = set()
    all_union_countries = set()

    for year in years:
        ind_countries = set(top10_indicator.loc[top10_indicator[year_col] == year, country_col])
        score_countries = set(top10_total_score.loc[top10_total_score[year_col] == year, country_col])

        overlap = ind_countries & score_countries

        if baseline == "score":
            denom = len(score_countries)
        elif baseline == "indicator":
            denom = len(ind_countries)
        elif baseline == "union":
            denom = len(ind_countries | score_countries)
        else:
            raise ValueError("baseline must be one of {'score','indicator','union'}")

        overlap_pct = (len(overlap) / denom * 100) if denom else 0.0

        # Track totals (sum-based uses same denominator choice as your previous code: size of score list)
        total_overlap_sum += len(overlap)
        total_baseline_sum += len(score_countries)  # keep consistent with your earlier snippets

        # Unique (set-based) accumulators
        all_overlap_countries |= overlap
        all_union_countries |= (ind_countries | score_countries)

        rows.append({
            "year": year,
            "indicator": indicator_name,
            "overlap_count": len(overlap),
            "baseline_count": denom,
            "overlap_pct": round(overlap_pct, 1),
            "overlap_countries": sorted(overlap),
        })

        if verbose:
            print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")

    # Totals
    total_pct_sum = (total_overlap_sum / total_baseline_sum * 100) if total_baseline_sum else 0.0
    unique_overlap_pct = (len(all_overlap_countries) / len(all_union_countries) * 100) if all_union_countries else 0.0

    if verbose:
        print("\n=== TOTAL (sum-based) ===")
        print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")
        print("=== UNIQUE (set-based across period) ===")
        print(f"Unique overlap countries: {len(all_overlap_countries)} / {len(all_union_countries)} "
              f"({unique_overlap_pct:.1f}%)")

    results_df = pd.DataFrame(rows)
    totals = {
        "indicator": indicator_name,
        "total_overlap_sum": total_overlap_sum,
        "total_baseline_sum": total_baseline_sum,
        "total_pct_sum": round(total_pct_sum, 1),
        "unique_overlap_countries": all_overlap_countries,
        "unique_union_countries": all_union_countries,
        "unique_overlap_pct": round(unique_overlap_pct, 1),
    }
    return results_df, totals


def quick_overlap_check(
    top10_indicator: pd.DataFrame,
    top10_total_score: pd.DataFrame,
    *,
    year_col: str = "year",
    country_col: str = "country",
) -> pd.DataFrame:
    """
    Minimal cross-check of overlaps under three baselines: score, indicator, union.
    Returns a tidy per-year DataFrame with counts and percentages.
    """
    years = sorted(
        set(top10_indicator[year_col].unique()).union(
            set(top10_total_score[year_col].unique())
        )
    )

    rows = []
    for y in years:
        ind = set(top10_indicator.loc[top10_indicator[year_col] == y, country_col].astype(str))
        sco = set(top10_total_score.loc[top10_total_score[year_col] == y, country_col].astype(str))
        overlap = ind & sco
        union = ind | sco

        n_ind, n_sco = len(ind), len(sco)
        n_ovl, n_uni = len(overlap), len(union)

        pct_score = (n_ovl / n_sco * 100) if n_sco else 0.0
        pct_indicator = (n_ovl / n_ind * 100) if n_ind else 0.0
        pct_union = (n_ovl / n_uni * 100) if n_uni else 0.0

        rows.append({
            "year": y,
            "n_indicator": n_ind,
            "n_score": n_sco,
            "n_overlap": n_ovl,
            "pct_score": round(pct_score, 1),
            "pct_indicator": round(pct_indicator, 1),
            "pct_union": round(pct_union, 1),
            "overlap_countries": sorted(overlap),
        })

    return pd.DataFrame(rows)


check_df = quick_overlap_check(top10_per_year_gdp, top10_total_score_df)
print(check_df)


gdp_df, gdp_totals = compute_yearly_overlaps(top10_per_year_gdp, top10_total_score_df, 
                                             "GDP", baseline="score")
edu_df, edu_totals = compute_yearly_overlaps(top10_per_year_edu, top10_total_score_df, 
                                             "Education", baseline="score")
gov_df, gov_totals = compute_yearly_overlaps(top10_per_year_gov, top10_total_score_df, 
                                             "Gov_Effectiveness", baseline="score")
res_df, res_totals = compute_yearly_overlaps(top10_per_year_res, top10_total_score_df, 
                                             "R&D", baseline="score")














edu_world_df = pd.read_csv("data/world-education-data.csv")


edu_world_df.dtypes


focus_cols = [
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct"
]

# 1. Overview of data types & non-null counts
print("=== Data Info ===")
edu_world_df[["country", "year"] + focus_cols].info()

# 2. Basic descriptive statistics
print("\n=== Descriptive Stats ===")
print(edu_world_df[focus_cols].describe())

# 3. Missing values count & percentage
print("\n=== Missing Values ===")
missing_summary = edu_world_df[focus_cols].isna().sum().to_frame("missing_count")
missing_summary["missing_pct"] = (missing_summary["missing_count"] / len(edu_world_df)) * 100
print(missing_summary)

# 4. Check duplicates
duplicate_rows = edu_world_df.duplicated(subset=["country", "year"])
print(f"\nNumber of duplicate country-year rows: {duplicate_rows.sum()}")

# 5. Sample rows
print("\n=== Sample Data ===")
print(edu_world_df[["country", "year"] + focus_cols].sample(5))





focus_cols = [
    "school_enrol_primary_pct",
    "school_enrol_secondary_pct",
    "school_enrol_tertiary_pct",
]

# 0) Ensure numeric (coerce weird strings like '..' to NaN)
for c in focus_cols:
    edu_world_df[c] = pd.to_numeric(edu_world_df[c], errors="coerce")

# 1) Sort so interpolation makes sense over time
edu_world_df = edu_world_df.sort_values(["country", "year"])

# 2) Interpolate within each country (keeps index aligned)
edu_world_df[focus_cols] = (
    edu_world_df
      .groupby("country")[focus_cols]
      .transform(lambda g: g.interpolate(method="linear"))
)

# 3) Optional: fill any edges that interpolation can’t fill
edu_world_df[focus_cols] = (
    edu_world_df
      .groupby("country")[focus_cols]
      .transform(lambda g: g.ffill().bfill())
)

# 4) Quick check
print(edu_world_df[focus_cols].isna().sum())


# Missing counts
missing_counts = edu_world_df[focus_cols].isna().sum()

# Missing percentages
missing_pct = (missing_counts / len(edu_world_df)) * 100

# Combine into a nice DataFrame
missing_summary = pd.DataFrame({
    "missing_count": missing_counts,
    "missing_pct": missing_pct.round(2)  # Round to 2 decimal places
})

print(missing_summary)


edu_world_df = standardize_country_names(edu_world_df, col="country")


# Duplicates by country-year
dups = edu_world_df.duplicated(["country", "year"]).sum()
print("Duplicate country-year rows:", dups)





def build_edu_merge(total_score_df: pd.DataFrame, edu_world_df: pd.DataFrame) -> pd.DataFrame:
    """
    Merge QS total_score (country, year) with enrollment percentages (primary/secondary/tertiary).
    - Trims country names
    - Optionally standardizes common variants
    - Ensures numeric types
    - Drops rows with required NaNs
    Returns a tidy DataFrame ready for correlation analysis.
    """
    required_ts = {"country", "year", "total_score"}
    required_edu = {
        "country", "year",
        "school_enrol_primary_pct",
        "school_enrol_secondary_pct",
        "school_enrol_tertiary_pct",
    }
    missing_ts = required_ts - set(total_score_df.columns)
    missing_edu = required_edu - set(edu_world_df.columns)
    if missing_ts:
        raise KeyError(f"total_score_df missing columns: {sorted(missing_ts)}")
    if missing_edu:
        raise KeyError(f"edu_world_df missing columns: {sorted(missing_edu)}")

    # --- 1) Standardize country names
    for df in (total_score_df, edu_world_df):
        df["country"] = df["country"].astype(str).str.strip()

    # --- 2) Types & cleaning
    total_score_df = total_score_df.copy()
    edu_world_df = edu_world_df.copy()

    # Ensure year is int (drop rows where it's missing or non-numeric)
    for df in (total_score_df, edu_world_df):
        df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")

    # Enrollment to numeric
    enrol_cols = [
        "school_enrol_primary_pct",
        "school_enrol_secondary_pct",
        "school_enrol_tertiary_pct",
    ]
    for c in enrol_cols:
        edu_world_df[c] = pd.to_numeric(edu_world_df[c], errors="coerce")

    # Optional: de-duplicate on (country, year) by keeping the latest/first
    total_score_df = (total_score_df
                      .sort_values(["country", "year"])
                      .drop_duplicates(subset=["country", "year"], keep="last"))
    edu_world_df = (edu_world_df
                    .sort_values(["country", "year"])
                    .drop_duplicates(subset=["country", "year"], keep="last"))

    # --- 3) Merge
    merged = pd.merge(
        total_score_df[["country", "year", "total_score"]],
        edu_world_df[["country", "year"] + enrol_cols],
        on=["country", "year"],
        how="inner",
        validate="one_to_one"  # change to "many_to_one" if needed
    )

    # --- 4) Drop rows with missing required values
    before = len(merged)
    merged = merged.dropna(subset=["total_score"] + enrol_cols)
    after = len(merged)

    # --- 5) Keep only relevant columns (already the case, but explicit)
    merged = merged[["country", "year", "total_score"] + enrol_cols].copy()

    # Quick sanity print
    print(f"Merged rows (before dropna → after): {before} → {after}")
    print(merged.head())

    return merged


merged_edu_ind = build_edu_merge(total_score_df, edu_world_df)

# keep only the relevant subset
merged_edu_ind_subset = merged_edu_ind[[
    "country", "year", "total_score",
    "school_enrol_primary_pct", 
    "school_enrol_secondary_pct", 
    "school_enrol_tertiary_pct"
]].copy()








# --- Top 10 Primary (only country, year, and primary enrollment) ---
top10_primary = (
    merged_edu_ind_subset
    .sort_values(["year", "school_enrol_primary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)[["country", "year", "school_enrol_primary_pct"]]
)

print(top10_primary.head())


# --- Top 10 Secondary ---
top10_secondary = (
   merged_edu_ind_subset
    .sort_values(["year", "school_enrol_secondary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True)[["country", "year", "school_enrol_secondary_pct"]]
)
print(top10_secondary.head())








import matplotlib.pyplot as plt

def get_top10_per_year(df, value_col):
    """
    Return top-10 countries per year for the given value column.
    Expects columns: ['country','year', value_col]
    """
    return (
        df
        .sort_values(["year", value_col], ascending=[True, False])
        .groupby("year")
        .head(10)
        .reset_index(drop=True)
    )

def plot_top10_per_year(top10_df, value_col, label, units="(% gross)"):
    """
    Horizontal bar charts of top-10 per year for a given indicator.
    """
    for year, group in top10_df.groupby("year"):
        plt.figure(figsize=(10, 6))
        group_sorted = group.sort_values(value_col, ascending=False)
        plt.barh(group_sorted["country"], group_sorted[value_col], color="skyblue")
        plt.xlabel(f"{label} {units}")
        plt.ylabel("Country")
        plt.title(f"Top 10 Countries by {label} in {int(year)}")
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

# --- Build top-10s ---
top10_primary   = get_top10_per_year(merged_edu_ind_subset, "school_enrol_primary_pct")
top10_secondary = get_top10_per_year(merged_edu_ind_subset, "school_enrol_secondary_pct")
top10_tertiary  = get_top10_per_year(merged_edu_ind_subset, "school_enrol_tertiary_pct")

# --- Plot them (same look as your GDP charts) ---
plot_top10_per_year(top10_primary,   "school_enrol_primary_pct",   "Primary School Enrollment")
plot_top10_per_year(top10_secondary, "school_enrol_secondary_pct", "Secondary School Enrollment")
plot_top10_per_year(top10_tertiary,  "school_enrol_tertiary_pct",  "Tertiary School Enrollment")


# --- Top 10 Tertiary ---
top10_tertiary = (
    merged_edu_ind_subset
    .sort_values(["year", "school_enrol_tertiary_pct"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True) [["country", "year", "school_enrol_tertiary_pct"]]
)
print(top10_tertiary.head(10))





# We perform the overlap analysis for the three educational indicators and total_score

prim_df, prim_totals = compute_yearly_overlaps(top10_primary,
                                               top10_total_score_df, "Primary",  baseline="score")
sec_df,  sec_totals  = compute_yearly_overlaps(top10_secondary,
                                               top10_total_score_df, "Secondary", baseline="score")
ter_df,  ter_totals  = compute_yearly_overlaps(top10_tertiary,
                                               top10_total_score_df, "Tertiary",  baseline="score")











# --- Overlap data ---
data = {
    "Year": [2017, 2018, 2019, 2020, 2021, 2022],
    "Primary":   [10.0, 10.0, 10.0, 20.0, 10.0, 10.0],
    "Secondary": [30.0, 30.0, 20.0, 20.0, 20.0, 20.0],
    "Tertiary":  [30.0, 30.0, 30.0, 30.0, 20.0, 20.0]
}

df = pd.DataFrame(data)

# --- Plot ---
plt.figure(figsize=(10, 6))
bar_width = 0.25
x = range(len(df["Year"]))

plt.bar([p - bar_width for p in x], df["Primary"], width=bar_width, label="Primary")
plt.bar(x, df["Secondary"], width=bar_width, label="Secondary")
plt.bar([p + bar_width for p in x], df["Tertiary"], width=bar_width, label="Tertiary")

# --- Styling ---
plt.xticks(x, df["Year"])
plt.ylabel("Overlap %")
plt.xlabel("Year")
plt.title("Top-10 Overlap by Enrollment Indicator (2017–2022)")
plt.ylim(0, 40)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)

plt.tight_layout()
plt.show()











# Expected inputs:
# total_score_df: [country, year, total_score, num_universities, avg_score]
# gdp_pc_long:    [country, year, gdp_per_capita]
# gov_long:       [country, year, gov_effectiveness]
# rd_long:        [country, year, rd_exp_gdp]
# edu_long:       [country, year, gov_edu_exp]

merged_all = (
    total_score_df
    .merge(gdp_pc_long, on=["country","year"], how="left")
    .merge(gov_long,    on=["country","year"], how="left")
    .merge(rd_long,     on=["country","year"], how="left")
    .merge(edu_long,    on=["country","year"], how="left")
)

print(merged_all.head())


# 1) Select columns and make a clean frame
focus_cols = [
    "total_score",      # target
    "gdp_per_capita",
    "gov_effectiveness",
    "rd_exp_gdp",
    "gov_edu_exp"
]
corr_df = merged_all[focus_cols].dropna()

# 2) Full correlation matrices (for heatmaps)
pearson_mat  = corr_df.corr(method="pearson")
spearman_mat = corr_df.corr(method="spearman")
kendall_mat  = corr_df.corr(method="kendall")

# 3) Target-wise correlations (coef + p-value) in one tidy table
def correlate_to_target(df: pd.DataFrame, target: str, features: list[str]) -> pd.DataFrame:
    rows = []
    for feat in features:
        sub = df[[target, feat]].dropna()
        x, y = sub[target].to_numpy(), sub[feat].to_numpy()

        pr, pp = pearsonr(x, y)
        sr, sp = spearmanr(x, y)          # rank-based, monotonic
        tr, tp = kendalltau(x, y)         # concordance of pairs

        rows.append({
            "feature": feat,
            "pearson_r": pr,   "pearson_p": pp,
            "spearman_rho": sr,"spearman_p": sp,
            "kendall_tau": tr, "kendall_p": tp,
            "n": len(sub)
        })
    out = pd.DataFrame(rows)
    return out.sort_values(by="pearson_r", ascending=False).reset_index(drop=True)

target = "total_score"
features = [c for c in focus_cols if c != target]
corr_table = correlate_to_target(corr_df, target, features)

# 4) Pretty print a compact summary
title = "QS Total Score vs. National Indicators"
print(f"=== Correlation with {title} ===")
for _, r in corr_table.iterrows():
    print(
        f"{r['feature']}: "
        f"Pearson r={r['pearson_r']:.4f} (p={r['pearson_p']:.3g}), "
        f"Spearman ρ={r['spearman_rho']:.4f} (p={r['spearman_p']:.3g}), "
        f"Kendall τ={r['kendall_tau']:.4f} (p={r['kendall_p']:.3g}); n={int(r['n'])}"
    )

# 5) Heatmaps (optional)
plt.figure(figsize=(7,5))
sns.heatmap(pearson_mat, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Pearson Correlation Heatmap"); plt.tight_layout(); plt.show()

plt.figure(figsize=(7,5))
sns.heatmap(spearman_mat, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Spearman Correlation Heatmap"); plt.tight_layout(); plt.show()

plt.figure(figsize=(7,5))
sns.heatmap(kendall_mat, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Kendall Correlation Heatmap"); plt.tight_layout(); plt.show()






indicators = ["school_enrol_primary_pct", "school_enrol_secondary_pct", "school_enrol_tertiary_pct"]

results = []
for ind in indicators:
    pearson_r, pearson_p = pearsonr(merged_edu_ind[ind], merged_edu_ind["total_score"])
    spearman_r, spearman_p = spearmanr(merged_edu_ind[ind], merged_edu_ind["total_score"])
    kendall_r, kendall_p = kendalltau(merged_edu_ind[ind], merged_edu_ind["total_score"])
    
    results.append({
        "Indicator": ind,
        "Pearson_r": pearson_r, "Pearson_p": pearson_p,
        "Spearman_r": spearman_r, "Spearman_p": spearman_p,
        "Kendall_tau": kendall_r, "Kendall_p": kendall_p
    })

corr_df = pd.DataFrame(results)
print(corr_df)


focus_cols = ["total_score"] + indicators
corr_data = merged_edu_ind[focus_cols].dropna()

# Pearson, Spearman, Kendall matrices
pearson_mat  = corr_data.corr(method="pearson")
spearman_mat = corr_data.corr(method="spearman")
kendall_mat  = corr_data.corr(method="kendall")


plt.figure(figsize=(6,5))
sns.heatmap(pearson_mat, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Pearson Correlation Heatmap — Education Indicators")
plt.show()

plt.figure(figsize=(6,5))
sns.heatmap(spearman_mat, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Spearman Correlation Heatmap — Education Indicators")
plt.show()

plt.figure(figsize=(6,5))
sns.heatmap(kendall_mat, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Kendall Correlation Heatmap — Education Indicators")
plt.show()



























