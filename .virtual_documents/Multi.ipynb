


# Assuming:
# rd_exp_long → cleaned dataset with ['country', 'year', 'rd_exp_gdp']
# merged_df   → main dataset with ['country', 'year', 'total_score']

# --- Merge R&D data into main dataset ---
merged_rd = pd.merge(
    merged_df,
    rd_exp_long,
    on=["country", "year"],
    how="inner"
)

# Ensure numeric types
merged_rd['total_score'] = pd.to_numeric(merged_rd['total_score'], errors='coerce')
merged_rd['rd_exp_gdp'] = pd.to_numeric(merged_rd['rd_exp_gdp'], errors='coerce')

# Drop NaNs
corr_df = merged_rd.dropna(subset=['total_score', 'rd_exp_gdp'])

# --- Pearson ---
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Spearman ---
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Kendall ---
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['rd_exp_gdp'])

# --- Print ---
print("=== Correlation with R&D Expenditure (% of GDP) ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Ensure numeric
merged_df_edu['total_score'] = pd.to_numeric(merged_df_edu['total_score'], errors='coerce')
merged_df_edu['gov_exp_edu'] = pd.to_numeric(merged_df_edu['gov_exp_edu'], errors='coerce')

# Drop missing rows
corr_df = merged_df_edu.dropna(subset=['total_score', 'gov_exp_edu'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_exp_edu'])

print("=== Correlation with Government Expenditure on Education ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Ensure numeric
merged_df_edu['total_score'] = pd.to_numeric(merged_df_edu['total_score'], errors='coerce')
merged_df_edu['gov_exp_edu'] = pd.to_numeric(merged_df_edu['gov_exp_edu'], errors='coerce')

# Drop missing rows
corr_df = merged_df_edu.dropna(subset=['total_score', 'gov_exp_edu'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_exp_edu'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_exp_edu'])

print("=== Correlation with Government Expenditure on Education ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Ensure numeric
merged_df_gov['total_score'] = pd.to_numeric(merged_df_gov['total_score'], errors='coerce')
merged_df_gov['gov_effectiveness'] = pd.to_numeric(merged_df_gov['gov_effectiveness'], errors='coerce')

# Drop missing rows
corr_df = merged_df_gov.dropna(subset=['total_score', 'gov_effectiveness'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_effectiveness'])

print("=== Correlation with Government Effectiveness ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


# Ensure numeric
merged_df_gov['total_score'] = pd.to_numeric(merged_df_gov['total_score'], errors='coerce')
merged_df_gov['gov_effectiveness'] = pd.to_numeric(merged_df_gov['gov_effectiveness'], errors='coerce')

# Drop missing rows
corr_df = merged_df_gov.dropna(subset=['total_score', 'gov_effectiveness'])

# Pearson
pearson_corr, pearson_p = pearsonr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Spearman
spearman_corr, spearman_p = spearmanr(corr_df['total_score'], corr_df['gov_effectiveness'])

# Kendall
kendall_corr, kendall_p = kendalltau(corr_df['total_score'], corr_df['gov_effectiveness'])

print("=== Correlation with Government Effectiveness ===")
print(f"Pearson:  r = {pearson_corr:.4f}, p = {pearson_p:.4g}")
print(f"Spearman: rho = {spearman_corr:.4f}, p = {spearman_p:.4g}")
print(f"Kendall:  tau = {kendall_corr:.4f}, p = {kendall_p:.4g}")


top10_secondary["source"] = "primary"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_secondary, top10_total_score], ignore_index=True)


top10_secondary["source"] = "primary"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_secondary, top10_total_score], ignore_index=True)


top10_secondary["source"] = "primary"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_secondary, top10_total_score], ignore_index=True)


# Initialize totals
total_overlap_sum = 0
total_baseline_sum = 0

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    secondary_countries = set(top10_secondary[top10_secondary["year"] == year]["country"])
    score_countries     = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Calculate overlap ---
    overlap = secondary_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)  
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (secondary_countries | score_countries)

# --- Sum-based total ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")



top10_tertiary["source"] = "primary"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_tertiary, top10_total_score], ignore_index=True)


total_overlap_sum = 0        # sum of overlaps across years
total_baseline_sum = 0       # sum of denominator sizes across years (here: score_countries)

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    tertiary_countries = set(top10_tertiary[top10_tertiary["year"] == year]["country"])
    score_countries    = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Yearly overlap ---
    overlap = tertiary_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)  # change to len(tertiary_countries) if you prefer that baseline
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (tertiary_countries | score_countries)

# --- Sum-based total (e.g., '16 overlaps over 70 countries') ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")



total_overlap_sum = 0        # sum of overlaps across years
total_baseline_sum = 0       # sum of denominator sizes across years (here: score_countries)

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    tertiary_countries = set(top10_tertiary[top10_tertiary["year"] == year]["country"])
    score_countries    = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Yearly overlap ---
    overlap = tertiary_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)  # change to len(tertiary_countries) if you prefer that baseline
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (tertiary_countries | score_countries)

# --- Sum-based total (e.g., '16 overlaps over 70 countries') ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")



# Add an indicator column so we can identify source
top10_per_year_gdp["source"] = "GDP"
top10_total_score["source"] = "Total_Score"

# Merge both lists
comparison_df = pd.concat([top10_per_year_gdp, top10_total_score], ignore_index=True)


# Assuming your total_score data is in df_total_score with columns: country, year, total_score
top10_total_score = (
    total_score_df
    .sort_values(["year", "total_score"], ascending=[True, False])
    .groupby("year")
    .head(10)
    .reset_index(drop=True))


# Initialize totals
total_overlap_sum = 0
total_baseline_sum = 0

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    gdp_countries   = set(top10_per_year_gdp[top10_per_year_gdp["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Calculate overlap ---
    overlap = gdp_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)      
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (gdp_countries | score_countries)

# --- Sum-based total ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")


# Initialize totals
total_overlap_sum = 0
total_baseline_sum = 0

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    gov_countries   = set(top10_per_year_gov_eff[top10_per_year_gov_eff["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Calculate overlap ---
    overlap = gov_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)      
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (gov_countries | score_countries)

# --- Sum-based total ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")



# Initialize totals
total_overlap_sum = 0
total_baseline_sum = 0

# For the unique-country summary across the whole period:
all_overlap_countries = set()
all_union_countries = set()

for year in sorted(comparison_df["year"].unique()):
    # --- Select countries for this year ---
    res_countries   = set(top10_per_year_res[top10_per_year_res["year"] == year]["country"])
    score_countries = set(top10_total_score[top10_total_score["year"] == year]["country"])
    
    # --- Calculate overlap ---
    overlap = res_countries & score_countries
    
    # --- Yearly percentage (vs total_score list size) ---
    overlap_pct = (len(overlap) / len(score_countries) * 100) if len(score_countries) else 0.0
    
    # --- Print per-year ---
    print(f"{year} → Overlap: {len(overlap)} countries ({overlap_pct:.1f}%) → {sorted(overlap)}")
    
    # --- Accumulate for sum-based total ---
    total_overlap_sum  += len(overlap)
    total_baseline_sum += len(score_countries)      
    
    # --- Accumulate for unique-country total ---
    all_overlap_countries |= overlap
    all_union_countries   |= (res_countries | score_countries)

# --- Sum-based total ---
if total_baseline_sum > 0:
    total_pct_sum = total_overlap_sum / total_baseline_sum * 100
else:
    total_pct_sum = 0.0

print("\n=== TOTAL (sum-based) ===")
print(f"Overlaps: {total_overlap_sum} over baseline {total_baseline_sum} → {total_pct_sum:.1f}%")


import pandas as pd
import statsmodels.formula.api as smf
from sklearn.preprocessing import StandardScaler

# --- 1) Standardize predictors ---
X_vars = ["gdp_per_capita", "gov_effectiveness", "gov_exp_edu", "rd_exp_gdp"]

scaler = StandardScaler()
analytic_std = analytic.copy()
analytic_std[X_vars] = scaler.fit_transform(analytic_std[X_vars])

# --- 2) Models ---

# Model 1: No FE
model_no_fe = smf.ols(
    "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp",
    data=analytic_std
).fit()

# Model 2: Year FE only
model_year_fe = smf.ols(
    "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(year)",
    data=analytic_std
).fit()

# Model 3: Country FE only
model_country_fe = smf.ols(
    "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country)",
    data=analytic_std
).fit()

# Model 4: Both Country & Year FE
model_both_fe = smf.ols(
    "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country) + C(year)",
    data=analytic_std
).fit()

# --- 3) R² comparison ---
print("\n=== Multiple Regression (OLS, standardized predictors) ===")
print(f"No FE:           R² = {model_no_fe.rsquared:.3f}")
print(f"Year FE only:    R² = {model_year_fe.rsquared:.3f}")
print(f"Country FE only: R² = {model_country_fe.rsquared:.3f}")
print(f"Both FE:         R² = {model_both_fe.rsquared:.3f}")

# --- 4) Coefficients (standardized, easier to compare) ---
print("\nStandardized Coefficients (No FE):")
print(model_no_fe.params[X_vars].sort_values(ascending=False))

print("\nStandardized Coefficients (Year FE):")
print(model_year_fe.params[X_vars].sort_values(ascending=False))

print("\nStandardized Coefficients (Country FE):")
print(model_country_fe.params[X_vars].sort_values(ascending=False))

print("\nStandardized Coefficients (Both FE):")
print(model_both_fe.params[X_vars].sort_values(ascending=False))

# --- 5) Optionally save summaries ---
with open("ols_no_fe_std.txt", "w") as f: f.write(model_no_fe.summary().as_text())
with open("ols_year_fe_std.txt", "w") as f: f.write(model_year_fe.summary().as_text())
with open("ols_country_fe_std.txt", "w") as f: f.write(model_country_fe.summary().as_text())
with open("ols_both_fe_std.txt", "w") as f: f.write(model_both_fe.summary().as_text())


# --- Model 1: No Fixed Effects ---
formula_no_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp"
ols_no_fe = smf.ols(formula=formula_no_fe, data=analytic).fit()

# --- Model 2: Year Fixed Effects only ---
formula_year_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(year)"
ols_year_fe = smf.ols(formula=formula_year_fe, data=analytic).fit()

# --- Model 3: Country Fixed Effects only ---
formula_country_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country)"
ols_country_fe = smf.ols(formula=formula_country_fe, data=analytic).fit()

# --- Model 4: Both Country & Year Fixed Effects ---
formula_both_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country) + C(year)"
ols_both_fe = smf.ols(formula=formula_both_fe, data=analytic).fit()

# --- Print R² comparison ---
print("\n=== Fixed Effects Comparison ===")
print(f"No FE:           R² = {ols_no_fe.rsquared:.3f}")
print(f"Year FE only:    R² = {ols_year_fe.rsquared:.3f}")
print(f"Country FE only: R² = {ols_country_fe.rsquared:.3f}")
print(f"Both FE:         R² = {ols_both_fe.rsquared:.3f}")

# --- Optional: check key coefficients for Year FE model ---
print("\nYear FE model coefficients:")
print(ols_year_fe.params[["gdp_per_capita","gov_effectiveness","gov_exp_edu","rd_exp_gdp"]])

# --- Optional: save summaries to text files ---
with open("ols_no_fe.txt", "w") as f: f.write(ols_no_fe.summary().as_text())
with open("ols_year_fe.txt", "w") as f: f.write(ols_year_fe.summary().as_text())
with open("ols_country_fe.txt", "w") as f: f.write(ols_country_fe.summary().as_text())
with open("ols_both_fe.txt", "w") as f: f.write(ols_both_fe.summary().as_text())



# --- Model 1: No Fixed Effects ---
formula_no_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp"
ols_no_fe = smf.ols(formula=formula_no_fe, data=analytic).fit()

# --- Model 2: Year Fixed Effects only ---
formula_year_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(year)"
ols_year_fe = smf.ols(formula=formula_year_fe, data=analytic).fit()

# --- Model 3: Country Fixed Effects only ---
formula_country_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country)"
ols_country_fe = smf.ols(formula=formula_country_fe, data=analytic).fit()

# --- Model 4: Both Country & Year Fixed Effects ---
formula_both_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country) + C(year)"
ols_both_fe = smf.ols(formula=formula_both_fe, data=analytic).fit()

# --- Print R² comparison ---
print("\n=== Fixed Effects Comparison ===")
print(f"No FE:           R² = {ols_no_fe.rsquared:.3f}")
print(f"Year FE only:    R² = {ols_year_fe.rsquared:.3f}")
print(f"Country FE only: R² = {ols_country_fe.rsquared:.3f}")
print(f"Both FE:         R² = {ols_both_fe.rsquared:.3f}")

# --- Optional: check key coefficients for Year FE model ---
print("\nYear FE model coefficients:")
print(ols_year_fe.params[["gdp_per_capita","gov_effectiveness","gov_exp_edu","rd_exp_gdp"]])

# --- Optional: save summaries to text files ---
with open("ols_no_fe.txt", "w") as f: f.write(ols_no_fe.summary().as_text())
with open("ols_year_fe.txt", "w") as f: f.write(ols_year_fe.summary().as_text())
with open("ols_country_fe.txt", "w") as f: f.write(ols_country_fe.summary().as_text())
with open("ols_both_fe.txt", "w") as f: f.write(ols_both_fe.summary().as_text())



# === 0) Imports
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error

import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor

# -------------------------------------------------------------------
# === 1) Normalize column names & fix typos
def normalize_cols(df):
    df = df.copy()
    df.columns = (
        df.columns
          .str.strip()
          .str.lower()
          .str.replace(r"\s+", "_", regex=True)
    )
    rename_map = {
        "ountry": "country",
        "total score": "total_score",
        "gdp_percapita": "gdp_per_capita",
        "gov.effectivnes": "gov_effectiveness",
        "gov.effectiveness": "gov_effectiveness",
        "gov_exp_on_education": "gov_exp_edu",
        "reseacrh_and_development": "rd_exp_gdp",
        "research_and_development": "rd_exp_gdp",
        "rd_exp_%gdp": "rd_exp_gdp",
        "rd_exp_gdp(%)": "rd_exp_gdp",
    }
    df = df.rename(columns=rename_map)
    return df

merged_df      = normalize_cols(merged_df)
merged_df_edu  = normalize_cols(merged_df_edu)
merged_df_gov  = normalize_cols(merged_df_gov)
merged_rd      = normalize_cols(merged_rd)

base = merged_df[["country","year","gdp_per_capita","total_score"]].copy()
edu  = merged_df_edu[["country","year","gov_exp_edu","total_score"]].copy()
gov  = merged_df_gov[["country","year","gov_effectiveness","total_score"]].copy()
rd   = merged_rd[["country","year","rd_exp_gdp","total_score"]].copy()

# -------------------------------------------------------------------
# === 2) Deduplicate
def dedupe(df, cols_keep):
    return (
        df.groupby(["country","year"], as_index=False)
          .agg({c:"mean" for c in cols_keep if c not in ["country","year"]})
    )

base = dedupe(base, base.columns)
edu  = dedupe(edu,  edu.columns)
gov  = dedupe(gov,  gov.columns)
rd   = dedupe(rd,   rd.columns)

# -------------------------------------------------------------------
# === 3) Merge into single analytic dataset
analytic = (
    base
      .merge(edu[["country","year","gov_exp_edu"]], on=["country","year"], how="left")
      .merge(gov[["country","year","gov_effectiveness"]], on=["country","year"], how="left")
      .merge(rd[["country","year","rd_exp_gdp"]], on=["country","year"], how="left")
)

analytic = analytic.query("year >= 2017 and year <= 2022")
analytic = analytic.dropna(subset=["total_score","gdp_per_capita","gov_exp_edu","gov_effectiveness","rd_exp_gdp"]).copy()

print("Rows:", len(analytic))
print(analytic.head())

analytic.to_csv("analytic_multivariate_dataset.csv", index=False)

# -------------------------------------------------------------------
# === 4) VIF
X_vars = ["gdp_per_capita","gov_effectiveness","gov_exp_edu","rd_exp_gdp"]
X_scaled = StandardScaler().fit_transform(analytic[X_vars])
vif = pd.DataFrame({
    "feature": X_vars,
    "VIF": [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
})
print("\nVariance Inflation Factors (VIF):\n", vif.sort_values("VIF", ascending=False))

# -------------------------------------------------------------------
# === 5) OLS (no fixed effects)
X = sm.add_constant(analytic[X_vars])
y = analytic["total_score"].values
ols_plain = sm.OLS(y, X).fit()
print("\n=== OLS (no fixed effects) ===")
print(ols_plain.summary())

# -------------------------------------------------------------------
# === 6) OLS with fixed effects
formula_fe = "total_score ~ gdp_per_capita + gov_effectiveness + gov_exp_edu + rd_exp_gdp + C(country) + C(year)"
ols_fe = smf.ols(formula=formula_fe, data=analytic).fit()
print("\n=== OLS with Country & Year Fixed Effects ===")
print(ols_fe.summary())

# -------------------------------------------------------------------
# === 7) Standardized coefficients
scaler_X = StandardScaler()
scaler_y = StandardScaler()
Xs = scaler_X.fit_transform(analytic[X_vars])
ys = scaler_y.fit_transform(analytic[["total_score"]]).ravel()
ols_std = sm.OLS(ys, sm.add_constant(Xs)).fit()
beta_coefs = pd.Series(ols_std.params[1:], index=X_vars, name="std_beta")
print("\nStandardized Betas (plain OLS):\n", beta_coefs.sort_values(ascending=False))

# -------------------------------------------------------------------
# === 8) Ridge & Lasso
alphas = np.logspace(-3, 3, 25)
ridge = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", RidgeCV(alphas=alphas, cv=5))
])
lasso = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", LassoCV(alphas=alphas, cv=5, max_iter=10000))
])
X_mat = analytic[X_vars].values
y_vec = analytic["total_score"].values

ridge.fit(X_mat, y_vec)
lasso.fit(X_mat, y_vec)

ridge_coefs = pd.Series(ridge.named_steps["model"].coef_, index=X_vars, name="ridge_coef")
lasso_coefs = pd.Series(lasso.named_steps["model"].coef_, index=X_vars, name="lasso_coef")

print("\nRidge best alpha:", ridge.named_steps["model"].alpha_)
print("Ridge coefficients:\n", ridge_coefs.sort_values(ascending=False))
print("\nLasso best alpha:", lasso.named_steps["model"].alpha_)
print("Lasso coefficients:\n", lasso_coefs.sort_values(ascending=False))

# -------------------------------------------------------------------
# === 9) Metrics with updated RMSE
def metrics(y_true, y_pred, label):
    print(f"\n[{label}] R2={r2_score(y_true,y_pred):.3f}  "
          f"MAE={mean_absolute_error(y_true,y_pred):.2f}  "
          f"RMSE={root_mean_squared_error(y_true,y_pred):.2f}")

metrics(y_vec, ols_plain.predict(X), "OLS no-FE")
metrics(y_vec, ols_fe.fittedvalues, "OLS with FE")
metrics(y_vec, ridge.predict(X_mat), "Ridge")
metrics(y_vec, lasso.predict(X_mat), "Lasso")

# -------------------------------------------------------------------
# === 10) Save OLS plain coefficient table
coef_table = pd.DataFrame({
    "variable": ["const"] + X_vars,
    "coef": ols_plain.params,
    "std_err": ols_plain.bse,
    "t": ols_plain.tvalues,
    "pval": ols_plain.pvalues
})
coef_table.to_csv("ols_plain_coef_table.csv", index=False)
print("\nSaved: ols_plain_coef_table.csv")

